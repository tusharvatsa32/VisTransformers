{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Edit Metadata",
    "colab": {
      "name": "ViT_pretrained_B_16_imagenet1k.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tusharvatsa32/VisTransformers/blob/main/Code/ViT_pretrained_B_16_imagenet1k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDT1Xwv9RZXb"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEujzAIePqnn",
        "outputId": "00acf6a4-66e2-46b6-c0ec-8409899032ea"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  1 00:34:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    48W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je-f_fI7aVgr"
      },
      "source": [
        "!pip install -q einops"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq-e52uORIVg"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch import einsum\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV-E0IFHRITw",
        "outputId": "6142021e-059e-4bcc-f580-5095e31c7af7"
      },
      "source": [
        "print(f\"Torch: {torch.__version__}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch: 1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSO-6jwwlYzO"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(11785)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS5Jp9SBRgQr"
      },
      "source": [
        "device = 'cuda'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWry7mRoT5iT"
      },
      "source": [
        "!pip install -q pytorch_pretrained_vit"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5EizoWMFu51",
        "outputId": "40687ffb-f2e3-414e-9e88-2c56d87fa670"
      },
      "source": [
        "from pytorch_pretrained_vit import ViT\n",
        "model = ViT('B_16_imagenet1k', pretrained=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1taJs7lFu2c"
      },
      "source": [
        "model\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 100)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMqVK7kBUO8l",
        "outputId": "dfca553a-06c7-4359-f45e-d467e10b5e30"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  (positional_embedding): PositionalEmbedding1D()\n",
              "  (transformer): Transformer(\n",
              "    (blocks): ModuleList(\n",
              "      (0): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=768, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_0631Qvbi_3"
      },
      "source": [
        "img_size = ((384, 384)) \n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "    transforms.RandomHorizontalFlip(p=0.3),\n",
        "    transforms.RandomVerticalFlip(p=0.3),\n",
        "    transforms.RandomRotation(10, interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
        "    transforms.RandomCrop(img_size, fill=0),\n",
        "    transforms.RandomAffine(10, scale=(0.8, 1.2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "transforms_val = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEIyZaYXRkZF"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTsnUVjBbOKG",
        "outputId": "ddbb600d-dcbf-4c0f-c791-2133df7e5844"
      },
      "source": [
        "train_data = torchvision.datasets.CIFAR100(train=True,download=True,root= \"./cifar100/train_data\", transform=transforms_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=8,\n",
        "                                          shuffle=True, num_workers=8)\n",
        "\n",
        "valid_data = torchvision.datasets.CIFAR100(train=False,download=True,root= \"./cifar100/test_data\", transform=transforms_val)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=4,\n",
        "                                         shuffle=False, num_workers=8)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiIcaG1zSZLA",
        "outputId": "31061400-4afe-44da-e455-20e930c0fee5"
      },
      "source": [
        "print(len(train_data), len(train_loader))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLLq3UCRSZIo",
        "outputId": "abde82b3-9b97-4d06-f9a9-3f8c80436052"
      },
      "source": [
        "print(len(valid_data), len(valid_loader))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VCgoZBvovkx"
      },
      "source": [
        "### Visual Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u5YZG1eozIv"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Go1icggSnrB"
      },
      "source": [
        "numEpochs = 100\n",
        "in_features = 3 # RGB channels\n",
        "\n",
        "learningRate = 0.01\n",
        "weightDecay = 2e-5\n",
        "\n",
        "num_classes = len(train_data.classes)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9, nesterov=True)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.4, patience=5, threshold=0.002, verbose=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsF6UcuGc981"
      },
      "source": [
        "my_acc = []\n",
        "my_loss = []"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KDBDOcxeU1eR",
        "outputId": "1790cb41-fad9-4173-e926-5a8b4a964c13"
      },
      "source": [
        "# Train!\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        del(outputs)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(train_data)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(valid_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(valid_data)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_d3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 4.7801\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 4.8242\n",
            "Epoch: 0\tBatch: 201\tAvg-Loss: 4.7395\n",
            "Epoch: 0\tBatch: 301\tAvg-Loss: 4.6986\n",
            "Epoch: 0\tBatch: 401\tAvg-Loss: 4.6760\n",
            "Epoch: 0\tBatch: 501\tAvg-Loss: 4.6526\n",
            "Epoch: 0\tBatch: 601\tAvg-Loss: 4.6294\n",
            "Epoch: 0\tBatch: 701\tAvg-Loss: 4.6027\n",
            "Epoch: 0\tBatch: 801\tAvg-Loss: 4.5882\n",
            "Epoch: 0\tBatch: 901\tAvg-Loss: 4.5682\n",
            "Epoch: 0\tBatch: 1001\tAvg-Loss: 4.5518\n",
            "Epoch: 0\tBatch: 1101\tAvg-Loss: 4.5312\n",
            "Epoch: 0\tBatch: 1201\tAvg-Loss: 4.5126\n",
            "Epoch: 0\tBatch: 1301\tAvg-Loss: 4.4888\n",
            "Epoch: 0\tBatch: 1401\tAvg-Loss: 4.4649\n",
            "Epoch: 0\tBatch: 1501\tAvg-Loss: 4.4448\n",
            "Epoch: 0\tBatch: 1601\tAvg-Loss: 4.4232\n",
            "Epoch: 0\tBatch: 1701\tAvg-Loss: 4.4015\n",
            "Epoch: 0\tBatch: 1801\tAvg-Loss: 4.3775\n",
            "Epoch: 0\tBatch: 1901\tAvg-Loss: 4.3585\n",
            "Epoch: 0\tBatch: 2001\tAvg-Loss: 4.3397\n",
            "Epoch: 0\tBatch: 2101\tAvg-Loss: 4.3159\n",
            "Epoch: 0\tBatch: 2201\tAvg-Loss: 4.2974\n",
            "Epoch: 0\tBatch: 2301\tAvg-Loss: 4.2774\n",
            "Epoch: 0\tBatch: 2401\tAvg-Loss: 4.2569\n",
            "Epoch: 0\tBatch: 2501\tAvg-Loss: 4.2397\n",
            "Epoch: 0\tBatch: 2601\tAvg-Loss: 4.2237\n",
            "Epoch: 0\tBatch: 2701\tAvg-Loss: 4.2085\n",
            "Epoch: 0\tBatch: 2801\tAvg-Loss: 4.1923\n",
            "Epoch: 0\tBatch: 2901\tAvg-Loss: 4.1768\n",
            "Epoch: 0\tBatch: 3001\tAvg-Loss: 4.1616\n",
            "Epoch: 0\tBatch: 3101\tAvg-Loss: 4.1451\n",
            "Epoch: 0\tBatch: 3201\tAvg-Loss: 4.1282\n",
            "Epoch: 0\tBatch: 3301\tAvg-Loss: 4.1152\n",
            "Epoch: 0\tBatch: 3401\tAvg-Loss: 4.0962\n",
            "Epoch: 0\tBatch: 3501\tAvg-Loss: 4.0818\n",
            "Epoch: 0\tBatch: 3601\tAvg-Loss: 4.0672\n",
            "Epoch: 0\tBatch: 3701\tAvg-Loss: 4.0517\n",
            "Epoch: 0\tBatch: 3801\tAvg-Loss: 4.0372\n",
            "Epoch: 0\tBatch: 3901\tAvg-Loss: 4.0237\n",
            "Epoch: 0\tBatch: 4001\tAvg-Loss: 4.0089\n",
            "Epoch: 0\tBatch: 4101\tAvg-Loss: 3.9950\n",
            "Epoch: 0\tBatch: 4201\tAvg-Loss: 3.9814\n",
            "Epoch: 0\tBatch: 4301\tAvg-Loss: 3.9665\n",
            "Epoch: 0\tBatch: 4401\tAvg-Loss: 3.9500\n",
            "Epoch: 0\tBatch: 4501\tAvg-Loss: 3.9333\n",
            "Epoch: 0\tBatch: 4601\tAvg-Loss: 3.9177\n",
            "Epoch: 0\tBatch: 4701\tAvg-Loss: 3.9036\n",
            "Epoch: 0\tBatch: 4801\tAvg-Loss: 3.8897\n",
            "Epoch: 0\tBatch: 4901\tAvg-Loss: 3.8771\n",
            "Epoch: 0\tBatch: 5001\tAvg-Loss: 3.8630\n",
            "Epoch: 0\tBatch: 5101\tAvg-Loss: 3.8463\n",
            "Epoch: 0\tBatch: 5201\tAvg-Loss: 3.8333\n",
            "Epoch: 0\tBatch: 5301\tAvg-Loss: 3.8183\n",
            "Epoch: 0\tBatch: 5401\tAvg-Loss: 3.8048\n",
            "Epoch: 0\tBatch: 5501\tAvg-Loss: 3.7913\n",
            "Epoch: 0\tBatch: 5601\tAvg-Loss: 3.7770\n",
            "Epoch: 0\tBatch: 5701\tAvg-Loss: 3.7598\n",
            "Epoch: 0\tBatch: 5801\tAvg-Loss: 3.7468\n",
            "Epoch: 0\tBatch: 5901\tAvg-Loss: 3.7311\n",
            "Epoch: 0\tBatch: 6001\tAvg-Loss: 3.7157\n",
            "Epoch: 0\tBatch: 6101\tAvg-Loss: 3.7003\n",
            "Epoch: 0\tBatch: 6201\tAvg-Loss: 3.6837\n",
            "Epoch: 0\t Training Accuracy: 13.8260\t Validation Accuracy: 32.0200\t Avg-Loss: 3.6755\n",
            "Epoch: 1\tBatch: 1\tAvg-Loss: 2.5010\n",
            "Epoch: 1\tBatch: 101\tAvg-Loss: 2.7908\n",
            "Epoch: 1\tBatch: 201\tAvg-Loss: 2.6779\n",
            "Epoch: 1\tBatch: 301\tAvg-Loss: 2.6359\n",
            "Epoch: 1\tBatch: 401\tAvg-Loss: 2.5923\n",
            "Epoch: 1\tBatch: 501\tAvg-Loss: 2.5592\n",
            "Epoch: 1\tBatch: 601\tAvg-Loss: 2.5120\n",
            "Epoch: 1\tBatch: 701\tAvg-Loss: 2.4974\n",
            "Epoch: 1\tBatch: 801\tAvg-Loss: 2.4733\n",
            "Epoch: 1\tBatch: 901\tAvg-Loss: 2.4490\n",
            "Epoch: 1\tBatch: 1001\tAvg-Loss: 2.4250\n",
            "Epoch: 1\tBatch: 1101\tAvg-Loss: 2.3956\n",
            "Epoch: 1\tBatch: 1201\tAvg-Loss: 2.3775\n",
            "Epoch: 1\tBatch: 1301\tAvg-Loss: 2.3667\n",
            "Epoch: 1\tBatch: 1401\tAvg-Loss: 2.3483\n",
            "Epoch: 1\tBatch: 1501\tAvg-Loss: 2.3276\n",
            "Epoch: 1\tBatch: 1601\tAvg-Loss: 2.3115\n",
            "Epoch: 1\tBatch: 1701\tAvg-Loss: 2.3041\n",
            "Epoch: 1\tBatch: 1801\tAvg-Loss: 2.2871\n",
            "Epoch: 1\tBatch: 1901\tAvg-Loss: 2.2714\n",
            "Epoch: 1\tBatch: 2001\tAvg-Loss: 2.2599\n",
            "Epoch: 1\tBatch: 2101\tAvg-Loss: 2.2424\n",
            "Epoch: 1\tBatch: 2201\tAvg-Loss: 2.2310\n",
            "Epoch: 1\tBatch: 2301\tAvg-Loss: 2.2184\n",
            "Epoch: 1\tBatch: 2401\tAvg-Loss: 2.2071\n",
            "Epoch: 1\tBatch: 2501\tAvg-Loss: 2.1923\n",
            "Epoch: 1\tBatch: 2601\tAvg-Loss: 2.1772\n",
            "Epoch: 1\tBatch: 2701\tAvg-Loss: 2.1649\n",
            "Epoch: 1\tBatch: 2801\tAvg-Loss: 2.1524\n",
            "Epoch: 1\tBatch: 2901\tAvg-Loss: 2.1394\n",
            "Epoch: 1\tBatch: 3001\tAvg-Loss: 2.1295\n",
            "Epoch: 1\tBatch: 3101\tAvg-Loss: 2.1219\n",
            "Epoch: 1\tBatch: 3201\tAvg-Loss: 2.1132\n",
            "Epoch: 1\tBatch: 3301\tAvg-Loss: 2.0995\n",
            "Epoch: 1\tBatch: 3401\tAvg-Loss: 2.0882\n",
            "Epoch: 1\tBatch: 3501\tAvg-Loss: 2.0792\n",
            "Epoch: 1\tBatch: 3601\tAvg-Loss: 2.0683\n",
            "Epoch: 1\tBatch: 3701\tAvg-Loss: 2.0656\n",
            "Epoch: 1\tBatch: 3801\tAvg-Loss: 2.0542\n",
            "Epoch: 1\tBatch: 3901\tAvg-Loss: 2.0452\n",
            "Epoch: 1\tBatch: 4001\tAvg-Loss: 2.0388\n",
            "Epoch: 1\tBatch: 4101\tAvg-Loss: 2.0307\n",
            "Epoch: 1\tBatch: 4201\tAvg-Loss: 2.0269\n",
            "Epoch: 1\tBatch: 4301\tAvg-Loss: 2.0160\n",
            "Epoch: 1\tBatch: 4401\tAvg-Loss: 2.0083\n",
            "Epoch: 1\tBatch: 4501\tAvg-Loss: 1.9981\n",
            "Epoch: 1\tBatch: 4601\tAvg-Loss: 1.9908\n",
            "Epoch: 1\tBatch: 4701\tAvg-Loss: 1.9811\n",
            "Epoch: 1\tBatch: 4801\tAvg-Loss: 1.9738\n",
            "Epoch: 1\tBatch: 4901\tAvg-Loss: 1.9655\n",
            "Epoch: 1\tBatch: 5001\tAvg-Loss: 1.9561\n",
            "Epoch: 1\tBatch: 5101\tAvg-Loss: 1.9492\n",
            "Epoch: 1\tBatch: 5201\tAvg-Loss: 1.9437\n",
            "Epoch: 1\tBatch: 5301\tAvg-Loss: 1.9375\n",
            "Epoch: 1\tBatch: 5401\tAvg-Loss: 1.9296\n",
            "Epoch: 1\tBatch: 5501\tAvg-Loss: 1.9234\n",
            "Epoch: 1\tBatch: 5601\tAvg-Loss: 1.9179\n",
            "Epoch: 1\tBatch: 5701\tAvg-Loss: 1.9112\n",
            "Epoch: 1\tBatch: 5801\tAvg-Loss: 1.9023\n",
            "Epoch: 1\tBatch: 5901\tAvg-Loss: 1.8959\n",
            "Epoch: 1\tBatch: 6001\tAvg-Loss: 1.8889\n",
            "Epoch: 1\tBatch: 6101\tAvg-Loss: 1.8825\n",
            "Epoch: 1\tBatch: 6201\tAvg-Loss: 1.8745\n",
            "Epoch: 1\t Training Accuracy: 48.9960\t Validation Accuracy: 54.0200\t Avg-Loss: 1.8722\n",
            "Epoch: 2\tBatch: 1\tAvg-Loss: 2.3724\n",
            "Epoch: 2\tBatch: 101\tAvg-Loss: 1.4402\n",
            "Epoch: 2\tBatch: 201\tAvg-Loss: 1.3924\n",
            "Epoch: 2\tBatch: 301\tAvg-Loss: 1.4447\n",
            "Epoch: 2\tBatch: 401\tAvg-Loss: 1.5129\n",
            "Epoch: 2\tBatch: 501\tAvg-Loss: 1.5098\n",
            "Epoch: 2\tBatch: 601\tAvg-Loss: 1.4798\n",
            "Epoch: 2\tBatch: 701\tAvg-Loss: 1.4534\n",
            "Epoch: 2\tBatch: 801\tAvg-Loss: 1.4362\n",
            "Epoch: 2\tBatch: 901\tAvg-Loss: 1.4275\n",
            "Epoch: 2\tBatch: 1001\tAvg-Loss: 1.4140\n",
            "Epoch: 2\tBatch: 1101\tAvg-Loss: 1.4144\n",
            "Epoch: 2\tBatch: 1201\tAvg-Loss: 1.4072\n",
            "Epoch: 2\tBatch: 1301\tAvg-Loss: 1.4085\n",
            "Epoch: 2\tBatch: 1401\tAvg-Loss: 1.4063\n",
            "Epoch: 2\tBatch: 1501\tAvg-Loss: 1.3968\n",
            "Epoch: 2\tBatch: 1601\tAvg-Loss: 1.3882\n",
            "Epoch: 2\tBatch: 1701\tAvg-Loss: 1.3852\n",
            "Epoch: 2\tBatch: 1801\tAvg-Loss: 1.3788\n",
            "Epoch: 2\tBatch: 1901\tAvg-Loss: 1.3741\n",
            "Epoch: 2\tBatch: 2001\tAvg-Loss: 1.3734\n",
            "Epoch: 2\tBatch: 2101\tAvg-Loss: 1.3644\n",
            "Epoch: 2\tBatch: 2201\tAvg-Loss: 1.3597\n",
            "Epoch: 2\tBatch: 2301\tAvg-Loss: 1.3560\n",
            "Epoch: 2\tBatch: 2401\tAvg-Loss: 1.3534\n",
            "Epoch: 2\tBatch: 2501\tAvg-Loss: 1.3520\n",
            "Epoch: 2\tBatch: 2601\tAvg-Loss: 1.3490\n",
            "Epoch: 2\tBatch: 2701\tAvg-Loss: 1.3491\n",
            "Epoch: 2\tBatch: 2801\tAvg-Loss: 1.3465\n",
            "Epoch: 2\tBatch: 2901\tAvg-Loss: 1.3472\n",
            "Epoch: 2\tBatch: 3001\tAvg-Loss: 1.3419\n",
            "Epoch: 2\tBatch: 3101\tAvg-Loss: 1.3400\n",
            "Epoch: 2\tBatch: 3201\tAvg-Loss: 1.3374\n",
            "Epoch: 2\tBatch: 3301\tAvg-Loss: 1.3384\n",
            "Epoch: 2\tBatch: 3401\tAvg-Loss: 1.3346\n",
            "Epoch: 2\tBatch: 3501\tAvg-Loss: 1.3307\n",
            "Epoch: 2\tBatch: 3601\tAvg-Loss: 1.3271\n",
            "Epoch: 2\tBatch: 3701\tAvg-Loss: 1.3244\n",
            "Epoch: 2\tBatch: 3801\tAvg-Loss: 1.3227\n",
            "Epoch: 2\tBatch: 3901\tAvg-Loss: 1.3225\n",
            "Epoch: 2\tBatch: 4001\tAvg-Loss: 1.3208\n",
            "Epoch: 2\tBatch: 4101\tAvg-Loss: 1.3153\n",
            "Epoch: 2\tBatch: 4201\tAvg-Loss: 1.3120\n",
            "Epoch: 2\tBatch: 4301\tAvg-Loss: 1.3113\n",
            "Epoch: 2\tBatch: 4401\tAvg-Loss: 1.3093\n",
            "Epoch: 2\tBatch: 4501\tAvg-Loss: 1.3073\n",
            "Epoch: 2\tBatch: 4601\tAvg-Loss: 1.3072\n",
            "Epoch: 2\tBatch: 4701\tAvg-Loss: 1.3045\n",
            "Epoch: 2\tBatch: 4801\tAvg-Loss: 1.3018\n",
            "Epoch: 2\tBatch: 4901\tAvg-Loss: 1.2988\n",
            "Epoch: 2\tBatch: 5001\tAvg-Loss: 1.2959\n",
            "Epoch: 2\tBatch: 5101\tAvg-Loss: 1.2919\n",
            "Epoch: 2\tBatch: 5201\tAvg-Loss: 1.2879\n",
            "Epoch: 2\tBatch: 5301\tAvg-Loss: 1.2894\n",
            "Epoch: 2\tBatch: 5401\tAvg-Loss: 1.2879\n",
            "Epoch: 2\tBatch: 5501\tAvg-Loss: 1.2856\n",
            "Epoch: 2\tBatch: 5601\tAvg-Loss: 1.2822\n",
            "Epoch: 2\tBatch: 5701\tAvg-Loss: 1.2805\n",
            "Epoch: 2\tBatch: 5801\tAvg-Loss: 1.2775\n",
            "Epoch: 2\tBatch: 5901\tAvg-Loss: 1.2768\n",
            "Epoch: 2\tBatch: 6001\tAvg-Loss: 1.2765\n",
            "Epoch: 2\tBatch: 6101\tAvg-Loss: 1.2745\n",
            "Epoch: 2\tBatch: 6201\tAvg-Loss: 1.2730\n",
            "Epoch: 2\t Training Accuracy: 64.1060\t Validation Accuracy: 71.7500\t Avg-Loss: 1.2723\n",
            "Epoch: 3\tBatch: 1\tAvg-Loss: 0.1571\n",
            "Epoch: 3\tBatch: 101\tAvg-Loss: 1.0283\n",
            "Epoch: 3\tBatch: 201\tAvg-Loss: 1.0244\n",
            "Epoch: 3\tBatch: 301\tAvg-Loss: 1.0217\n",
            "Epoch: 3\tBatch: 401\tAvg-Loss: 1.0110\n",
            "Epoch: 3\tBatch: 501\tAvg-Loss: 1.0201\n",
            "Epoch: 3\tBatch: 601\tAvg-Loss: 1.0175\n",
            "Epoch: 3\tBatch: 701\tAvg-Loss: 1.0096\n",
            "Epoch: 3\tBatch: 801\tAvg-Loss: 1.0239\n",
            "Epoch: 3\tBatch: 901\tAvg-Loss: 1.0206\n",
            "Epoch: 3\tBatch: 1001\tAvg-Loss: 1.0210\n",
            "Epoch: 3\tBatch: 1101\tAvg-Loss: 1.0152\n",
            "Epoch: 3\tBatch: 1201\tAvg-Loss: 1.0216\n",
            "Epoch: 3\tBatch: 1301\tAvg-Loss: 1.0167\n",
            "Epoch: 3\tBatch: 1401\tAvg-Loss: 1.0234\n",
            "Epoch: 3\tBatch: 1501\tAvg-Loss: 1.0232\n",
            "Epoch: 3\tBatch: 1601\tAvg-Loss: 1.0207\n",
            "Epoch: 3\tBatch: 1701\tAvg-Loss: 1.0221\n",
            "Epoch: 3\tBatch: 1801\tAvg-Loss: 1.0239\n",
            "Epoch: 3\tBatch: 1901\tAvg-Loss: 1.0234\n",
            "Epoch: 3\tBatch: 2001\tAvg-Loss: 1.0280\n",
            "Epoch: 3\tBatch: 2101\tAvg-Loss: 1.0277\n",
            "Epoch: 3\tBatch: 2201\tAvg-Loss: 1.0300\n",
            "Epoch: 3\tBatch: 2301\tAvg-Loss: 1.0263\n",
            "Epoch: 3\tBatch: 2401\tAvg-Loss: 1.0263\n",
            "Epoch: 3\tBatch: 2501\tAvg-Loss: 1.0263\n",
            "Epoch: 3\tBatch: 2601\tAvg-Loss: 1.0247\n",
            "Epoch: 3\tBatch: 2701\tAvg-Loss: 1.0263\n",
            "Epoch: 3\tBatch: 2801\tAvg-Loss: 1.0264\n",
            "Epoch: 3\tBatch: 2901\tAvg-Loss: 1.0231\n",
            "Epoch: 3\tBatch: 3001\tAvg-Loss: 1.0230\n",
            "Epoch: 3\tBatch: 3101\tAvg-Loss: 1.0190\n",
            "Epoch: 3\tBatch: 3201\tAvg-Loss: 1.0197\n",
            "Epoch: 3\tBatch: 3301\tAvg-Loss: 1.0221\n",
            "Epoch: 3\tBatch: 3401\tAvg-Loss: 1.0202\n",
            "Epoch: 3\tBatch: 3501\tAvg-Loss: 1.0200\n",
            "Epoch: 3\tBatch: 3601\tAvg-Loss: 1.0197\n",
            "Epoch: 3\tBatch: 3701\tAvg-Loss: 1.0211\n",
            "Epoch: 3\tBatch: 3801\tAvg-Loss: 1.0189\n",
            "Epoch: 3\tBatch: 3901\tAvg-Loss: 1.0199\n",
            "Epoch: 3\tBatch: 4001\tAvg-Loss: 1.0179\n",
            "Epoch: 3\tBatch: 4101\tAvg-Loss: 1.0192\n",
            "Epoch: 3\tBatch: 4201\tAvg-Loss: 1.0176\n",
            "Epoch: 3\tBatch: 4301\tAvg-Loss: 1.0197\n",
            "Epoch: 3\tBatch: 4401\tAvg-Loss: 1.0192\n",
            "Epoch: 3\tBatch: 4501\tAvg-Loss: 1.0189\n",
            "Epoch: 3\tBatch: 4601\tAvg-Loss: 1.0187\n",
            "Epoch: 3\tBatch: 4701\tAvg-Loss: 1.0163\n",
            "Epoch: 3\tBatch: 4801\tAvg-Loss: 1.0153\n",
            "Epoch: 3\tBatch: 4901\tAvg-Loss: 1.0148\n",
            "Epoch: 3\tBatch: 5001\tAvg-Loss: 1.0144\n",
            "Epoch: 3\tBatch: 5101\tAvg-Loss: 1.0158\n",
            "Epoch: 3\tBatch: 5201\tAvg-Loss: 1.0146\n",
            "Epoch: 3\tBatch: 5301\tAvg-Loss: 1.0142\n",
            "Epoch: 3\tBatch: 5401\tAvg-Loss: 1.0151\n",
            "Epoch: 3\tBatch: 5501\tAvg-Loss: 1.0135\n",
            "Epoch: 3\tBatch: 5601\tAvg-Loss: 1.0128\n",
            "Epoch: 3\tBatch: 5701\tAvg-Loss: 1.0137\n",
            "Epoch: 3\tBatch: 5801\tAvg-Loss: 1.0128\n",
            "Epoch: 3\tBatch: 5901\tAvg-Loss: 1.0124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a168c4a62326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                   \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                   nesterov)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# update momentum_buffers in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl7r45-FUmfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf07ae4d-b51a-4b9d-d2c0-ee148ed07616"
      },
      "source": [
        "# Train!\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        del(outputs)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(train_data)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(valid_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(valid_data)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_d3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 1.9060\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 0.7388\n",
            "Epoch: 0\tBatch: 201\tAvg-Loss: 0.7982\n",
            "Epoch: 0\tBatch: 301\tAvg-Loss: 0.8268\n",
            "Epoch: 0\tBatch: 401\tAvg-Loss: 0.8307\n",
            "Epoch: 0\tBatch: 501\tAvg-Loss: 0.8411\n",
            "Epoch: 0\tBatch: 601\tAvg-Loss: 0.8387\n",
            "Epoch: 0\tBatch: 701\tAvg-Loss: 0.8453\n",
            "Epoch: 0\tBatch: 801\tAvg-Loss: 0.8531\n",
            "Epoch: 0\tBatch: 901\tAvg-Loss: 0.8473\n",
            "Epoch: 0\tBatch: 1001\tAvg-Loss: 0.8585\n",
            "Epoch: 0\tBatch: 1101\tAvg-Loss: 0.8649\n",
            "Epoch: 0\tBatch: 1201\tAvg-Loss: 0.8633\n",
            "Epoch: 0\tBatch: 1301\tAvg-Loss: 0.8580\n",
            "Epoch: 0\tBatch: 1401\tAvg-Loss: 0.8552\n",
            "Epoch: 0\tBatch: 1501\tAvg-Loss: 0.8595\n",
            "Epoch: 0\tBatch: 1601\tAvg-Loss: 0.8601\n",
            "Epoch: 0\tBatch: 1701\tAvg-Loss: 0.8616\n",
            "Epoch: 0\tBatch: 1801\tAvg-Loss: 0.8645\n",
            "Epoch: 0\tBatch: 1901\tAvg-Loss: 0.8638\n",
            "Epoch: 0\tBatch: 2001\tAvg-Loss: 0.8638\n",
            "Epoch: 0\tBatch: 2101\tAvg-Loss: 0.8638\n",
            "Epoch: 0\tBatch: 2201\tAvg-Loss: 0.8709\n",
            "Epoch: 0\tBatch: 2301\tAvg-Loss: 0.8763\n",
            "Epoch: 0\tBatch: 2401\tAvg-Loss: 0.8750\n",
            "Epoch: 0\tBatch: 2501\tAvg-Loss: 0.8734\n",
            "Epoch: 0\tBatch: 2601\tAvg-Loss: 0.8721\n",
            "Epoch: 0\tBatch: 2701\tAvg-Loss: 0.8706\n",
            "Epoch: 0\tBatch: 2801\tAvg-Loss: 0.8723\n",
            "Epoch: 0\tBatch: 2901\tAvg-Loss: 0.8750\n",
            "Epoch: 0\tBatch: 3001\tAvg-Loss: 0.8733\n",
            "Epoch: 0\tBatch: 3101\tAvg-Loss: 0.8734\n",
            "Epoch: 0\tBatch: 3201\tAvg-Loss: 0.8715\n",
            "Epoch: 0\tBatch: 3301\tAvg-Loss: 0.8724\n",
            "Epoch: 0\tBatch: 3401\tAvg-Loss: 0.8699\n",
            "Epoch: 0\tBatch: 3501\tAvg-Loss: 0.8706\n",
            "Epoch: 0\tBatch: 3601\tAvg-Loss: 0.8725\n",
            "Epoch: 0\tBatch: 3701\tAvg-Loss: 0.8722\n",
            "Epoch: 0\tBatch: 3801\tAvg-Loss: 0.8727\n",
            "Epoch: 0\tBatch: 3901\tAvg-Loss: 0.8741\n",
            "Epoch: 0\tBatch: 4001\tAvg-Loss: 0.8723\n",
            "Epoch: 0\tBatch: 4101\tAvg-Loss: 0.8731\n",
            "Epoch: 0\tBatch: 4201\tAvg-Loss: 0.8737\n",
            "Epoch: 0\tBatch: 4301\tAvg-Loss: 0.8735\n",
            "Epoch: 0\tBatch: 4401\tAvg-Loss: 0.8738\n",
            "Epoch: 0\tBatch: 4501\tAvg-Loss: 0.8748\n",
            "Epoch: 0\tBatch: 4601\tAvg-Loss: 0.8747\n",
            "Epoch: 0\tBatch: 4701\tAvg-Loss: 0.8745\n",
            "Epoch: 0\tBatch: 4801\tAvg-Loss: 0.8747\n",
            "Epoch: 0\tBatch: 4901\tAvg-Loss: 0.8734\n",
            "Epoch: 0\tBatch: 5001\tAvg-Loss: 0.8729\n",
            "Epoch: 0\tBatch: 5101\tAvg-Loss: 0.8728\n",
            "Epoch: 0\tBatch: 5201\tAvg-Loss: 0.8739\n",
            "Epoch: 0\tBatch: 5301\tAvg-Loss: 0.8734\n",
            "Epoch: 0\tBatch: 5401\tAvg-Loss: 0.8732\n",
            "Epoch: 0\tBatch: 5501\tAvg-Loss: 0.8733\n",
            "Epoch: 0\tBatch: 5601\tAvg-Loss: 0.8713\n",
            "Epoch: 0\tBatch: 5701\tAvg-Loss: 0.8690\n",
            "Epoch: 0\tBatch: 5801\tAvg-Loss: 0.8682\n",
            "Epoch: 0\tBatch: 5901\tAvg-Loss: 0.8681\n",
            "Epoch: 0\tBatch: 6001\tAvg-Loss: 0.8683\n",
            "Epoch: 0\tBatch: 6101\tAvg-Loss: 0.8679\n",
            "Epoch: 0\tBatch: 6201\tAvg-Loss: 0.8666\n",
            "Epoch: 0\t Training Accuracy: 74.2000\t Validation Accuracy: 74.3600\t Avg-Loss: 0.8670\n",
            "Epoch: 1\tBatch: 1\tAvg-Loss: 0.2060\n",
            "Epoch: 1\tBatch: 101\tAvg-Loss: 0.7289\n",
            "Epoch: 1\tBatch: 201\tAvg-Loss: 0.7082\n",
            "Epoch: 1\tBatch: 301\tAvg-Loss: 0.7089\n",
            "Epoch: 1\tBatch: 401\tAvg-Loss: 0.6940\n",
            "Epoch: 1\tBatch: 501\tAvg-Loss: 0.6963\n",
            "Epoch: 1\tBatch: 601\tAvg-Loss: 0.6895\n",
            "Epoch: 1\tBatch: 701\tAvg-Loss: 0.6840\n",
            "Epoch: 1\tBatch: 801\tAvg-Loss: 0.6889\n",
            "Epoch: 1\tBatch: 901\tAvg-Loss: 0.6900\n",
            "Epoch: 1\tBatch: 1001\tAvg-Loss: 0.6893\n",
            "Epoch: 1\tBatch: 1101\tAvg-Loss: 0.6950\n",
            "Epoch: 1\tBatch: 1201\tAvg-Loss: 0.6982\n",
            "Epoch: 1\tBatch: 1301\tAvg-Loss: 0.6955\n",
            "Epoch: 1\tBatch: 1401\tAvg-Loss: 0.6958\n",
            "Epoch: 1\tBatch: 1501\tAvg-Loss: 0.6991\n",
            "Epoch: 1\tBatch: 1601\tAvg-Loss: 0.6970\n",
            "Epoch: 1\tBatch: 1701\tAvg-Loss: 0.7004\n",
            "Epoch: 1\tBatch: 1801\tAvg-Loss: 0.7009\n",
            "Epoch: 1\tBatch: 1901\tAvg-Loss: 0.7024\n",
            "Epoch: 1\tBatch: 2001\tAvg-Loss: 0.7054\n",
            "Epoch: 1\tBatch: 2101\tAvg-Loss: 0.7071\n",
            "Epoch: 1\tBatch: 2201\tAvg-Loss: 0.7117\n",
            "Epoch: 1\tBatch: 2301\tAvg-Loss: 0.7141\n",
            "Epoch: 1\tBatch: 2401\tAvg-Loss: 0.7158\n",
            "Epoch: 1\tBatch: 2501\tAvg-Loss: 0.7151\n",
            "Epoch: 1\tBatch: 2601\tAvg-Loss: 0.7155\n",
            "Epoch: 1\tBatch: 2701\tAvg-Loss: 0.7113\n",
            "Epoch: 1\tBatch: 2801\tAvg-Loss: 0.7126\n",
            "Epoch: 1\tBatch: 2901\tAvg-Loss: 0.7128\n",
            "Epoch: 1\tBatch: 3001\tAvg-Loss: 0.7110\n",
            "Epoch: 1\tBatch: 3101\tAvg-Loss: 0.7136\n",
            "Epoch: 1\tBatch: 3201\tAvg-Loss: 0.7149\n",
            "Epoch: 1\tBatch: 3301\tAvg-Loss: 0.7151\n",
            "Epoch: 1\tBatch: 3401\tAvg-Loss: 0.7127\n",
            "Epoch: 1\tBatch: 3501\tAvg-Loss: 0.7145\n",
            "Epoch: 1\tBatch: 3601\tAvg-Loss: 0.7149\n",
            "Epoch: 1\tBatch: 3701\tAvg-Loss: 0.7163\n",
            "Epoch: 1\tBatch: 3801\tAvg-Loss: 0.7143\n",
            "Epoch: 1\tBatch: 3901\tAvg-Loss: 0.7152\n",
            "Epoch: 1\tBatch: 4001\tAvg-Loss: 0.7176\n",
            "Epoch: 1\tBatch: 4101\tAvg-Loss: 0.7173\n",
            "Epoch: 1\tBatch: 4201\tAvg-Loss: 0.7179\n",
            "Epoch: 1\tBatch: 4301\tAvg-Loss: 0.7187\n",
            "Epoch: 1\tBatch: 4401\tAvg-Loss: 0.7200\n",
            "Epoch: 1\tBatch: 4501\tAvg-Loss: 0.7192\n",
            "Epoch: 1\tBatch: 4601\tAvg-Loss: 0.7182\n",
            "Epoch: 1\tBatch: 4701\tAvg-Loss: 0.7197\n",
            "Epoch: 1\tBatch: 4801\tAvg-Loss: 0.7204\n",
            "Epoch: 1\tBatch: 4901\tAvg-Loss: 0.7211\n",
            "Epoch: 1\tBatch: 5001\tAvg-Loss: 0.7230\n",
            "Epoch: 1\tBatch: 5101\tAvg-Loss: 0.7231\n",
            "Epoch: 1\tBatch: 5201\tAvg-Loss: 0.7229\n",
            "Epoch: 1\tBatch: 5301\tAvg-Loss: 0.7231\n",
            "Epoch: 1\tBatch: 5401\tAvg-Loss: 0.7215\n",
            "Epoch: 1\tBatch: 5501\tAvg-Loss: 0.7225\n",
            "Epoch: 1\tBatch: 5601\tAvg-Loss: 0.7236\n",
            "Epoch: 1\tBatch: 5701\tAvg-Loss: 0.7270\n",
            "Epoch: 1\tBatch: 5801\tAvg-Loss: 0.7271\n",
            "Epoch: 1\tBatch: 5901\tAvg-Loss: 0.7288\n",
            "Epoch: 1\tBatch: 6001\tAvg-Loss: 0.7297\n",
            "Epoch: 1\tBatch: 6101\tAvg-Loss: 0.7316\n",
            "Epoch: 1\tBatch: 6201\tAvg-Loss: 0.7330\n",
            "Epoch: 1\t Training Accuracy: 77.9940\t Validation Accuracy: 72.2400\t Avg-Loss: 0.7336\n",
            "Epoch: 2\tBatch: 1\tAvg-Loss: 0.2346\n",
            "Epoch: 2\tBatch: 101\tAvg-Loss: 0.6636\n",
            "Epoch: 2\tBatch: 201\tAvg-Loss: 0.6866\n",
            "Epoch: 2\tBatch: 301\tAvg-Loss: 0.6612\n",
            "Epoch: 2\tBatch: 401\tAvg-Loss: 0.6627\n",
            "Epoch: 2\tBatch: 501\tAvg-Loss: 0.6521\n",
            "Epoch: 2\tBatch: 601\tAvg-Loss: 0.6393\n",
            "Epoch: 2\tBatch: 701\tAvg-Loss: 0.6355\n",
            "Epoch: 2\tBatch: 801\tAvg-Loss: 0.6372\n",
            "Epoch: 2\tBatch: 901\tAvg-Loss: 0.6373\n",
            "Epoch: 2\tBatch: 1001\tAvg-Loss: 0.6305\n",
            "Epoch: 2\tBatch: 1101\tAvg-Loss: 0.6365\n",
            "Epoch: 2\tBatch: 1201\tAvg-Loss: 0.6321\n",
            "Epoch: 2\tBatch: 1301\tAvg-Loss: 0.6306\n",
            "Epoch: 2\tBatch: 1401\tAvg-Loss: 0.6321\n",
            "Epoch: 2\tBatch: 1501\tAvg-Loss: 0.6303\n",
            "Epoch: 2\tBatch: 1601\tAvg-Loss: 0.6296\n",
            "Epoch: 2\tBatch: 1701\tAvg-Loss: 0.6349\n",
            "Epoch: 2\tBatch: 1801\tAvg-Loss: 0.6345\n",
            "Epoch: 2\tBatch: 1901\tAvg-Loss: 0.6312\n",
            "Epoch: 2\tBatch: 2001\tAvg-Loss: 0.6325\n",
            "Epoch: 2\tBatch: 2101\tAvg-Loss: 0.6372\n",
            "Epoch: 2\tBatch: 2201\tAvg-Loss: 0.6374\n",
            "Epoch: 2\tBatch: 2301\tAvg-Loss: 0.6386\n",
            "Epoch: 2\tBatch: 2401\tAvg-Loss: 0.6367\n",
            "Epoch: 2\tBatch: 2501\tAvg-Loss: 0.6376\n",
            "Epoch: 2\tBatch: 2601\tAvg-Loss: 0.6373\n",
            "Epoch: 2\tBatch: 2701\tAvg-Loss: 0.6374\n",
            "Epoch: 2\tBatch: 2801\tAvg-Loss: 0.6393\n",
            "Epoch: 2\tBatch: 2901\tAvg-Loss: 0.6385\n",
            "Epoch: 2\tBatch: 3001\tAvg-Loss: 0.6403\n",
            "Epoch: 2\tBatch: 3101\tAvg-Loss: 0.6406\n",
            "Epoch: 2\tBatch: 3201\tAvg-Loss: 0.6447\n",
            "Epoch: 2\tBatch: 3301\tAvg-Loss: 0.6446\n",
            "Epoch: 2\tBatch: 3401\tAvg-Loss: 0.6449\n",
            "Epoch: 2\tBatch: 3501\tAvg-Loss: 0.6455\n",
            "Epoch: 2\tBatch: 3601\tAvg-Loss: 0.6442\n",
            "Epoch: 2\tBatch: 3701\tAvg-Loss: 0.6421\n",
            "Epoch: 2\tBatch: 3801\tAvg-Loss: 0.6423\n",
            "Epoch: 2\tBatch: 3901\tAvg-Loss: 0.6421\n",
            "Epoch: 2\tBatch: 4001\tAvg-Loss: 0.6423\n",
            "Epoch: 2\tBatch: 4101\tAvg-Loss: 0.6431\n",
            "Epoch: 2\tBatch: 4201\tAvg-Loss: 0.6422\n",
            "Epoch: 2\tBatch: 4301\tAvg-Loss: 0.6426\n",
            "Epoch: 2\tBatch: 4401\tAvg-Loss: 0.6421\n",
            "Epoch: 2\tBatch: 4501\tAvg-Loss: 0.6426\n",
            "Epoch: 2\tBatch: 4601\tAvg-Loss: 0.6418\n",
            "Epoch: 2\tBatch: 4701\tAvg-Loss: 0.6417\n",
            "Epoch: 2\tBatch: 4801\tAvg-Loss: 0.6404\n",
            "Epoch: 2\tBatch: 4901\tAvg-Loss: 0.6410\n",
            "Epoch: 2\tBatch: 5001\tAvg-Loss: 0.6419\n",
            "Epoch: 2\tBatch: 5101\tAvg-Loss: 0.6420\n",
            "Epoch: 2\tBatch: 5201\tAvg-Loss: 0.6414\n",
            "Epoch: 2\tBatch: 5301\tAvg-Loss: 0.6401\n",
            "Epoch: 2\tBatch: 5401\tAvg-Loss: 0.6397\n",
            "Epoch: 2\tBatch: 5501\tAvg-Loss: 0.6414\n",
            "Epoch: 2\tBatch: 5601\tAvg-Loss: 0.6418\n",
            "Epoch: 2\tBatch: 5701\tAvg-Loss: 0.6437\n",
            "Epoch: 2\tBatch: 5801\tAvg-Loss: 0.6461\n",
            "Epoch: 2\tBatch: 5901\tAvg-Loss: 0.6464\n",
            "Epoch: 2\tBatch: 6001\tAvg-Loss: 0.6474\n",
            "Epoch: 2\tBatch: 6101\tAvg-Loss: 0.6483\n",
            "Epoch: 2\tBatch: 6201\tAvg-Loss: 0.6492\n",
            "Epoch: 2\t Training Accuracy: 80.3400\t Validation Accuracy: 77.7000\t Avg-Loss: 0.6500\n",
            "Epoch: 3\tBatch: 1\tAvg-Loss: 0.7152\n",
            "Epoch: 3\tBatch: 101\tAvg-Loss: 0.5501\n",
            "Epoch: 3\tBatch: 201\tAvg-Loss: 0.5262\n",
            "Epoch: 3\tBatch: 301\tAvg-Loss: 0.5172\n",
            "Epoch: 3\tBatch: 401\tAvg-Loss: 0.5371\n",
            "Epoch: 3\tBatch: 501\tAvg-Loss: 0.5263\n",
            "Epoch: 3\tBatch: 601\tAvg-Loss: 0.5336\n",
            "Epoch: 3\tBatch: 701\tAvg-Loss: 0.5374\n",
            "Epoch: 3\tBatch: 801\tAvg-Loss: 0.5353\n",
            "Epoch: 3\tBatch: 901\tAvg-Loss: 0.5485\n",
            "Epoch: 3\tBatch: 1001\tAvg-Loss: 0.5463\n",
            "Epoch: 3\tBatch: 1101\tAvg-Loss: 0.5435\n",
            "Epoch: 3\tBatch: 1201\tAvg-Loss: 0.5377\n",
            "Epoch: 3\tBatch: 1301\tAvg-Loss: 0.5355\n",
            "Epoch: 3\tBatch: 1401\tAvg-Loss: 0.5362\n",
            "Epoch: 3\tBatch: 1501\tAvg-Loss: 0.5386\n",
            "Epoch: 3\tBatch: 1601\tAvg-Loss: 0.5413\n",
            "Epoch: 3\tBatch: 1701\tAvg-Loss: 0.5387\n",
            "Epoch: 3\tBatch: 1801\tAvg-Loss: 0.5452\n",
            "Epoch: 3\tBatch: 1901\tAvg-Loss: 0.5491\n",
            "Epoch: 3\tBatch: 2001\tAvg-Loss: 0.5471\n",
            "Epoch: 3\tBatch: 2101\tAvg-Loss: 0.5504\n",
            "Epoch: 3\tBatch: 2201\tAvg-Loss: 0.5499\n",
            "Epoch: 3\tBatch: 2301\tAvg-Loss: 0.5511\n",
            "Epoch: 3\tBatch: 2401\tAvg-Loss: 0.5505\n",
            "Epoch: 3\tBatch: 2501\tAvg-Loss: 0.5526\n",
            "Epoch: 3\tBatch: 2601\tAvg-Loss: 0.5520\n",
            "Epoch: 3\tBatch: 2701\tAvg-Loss: 0.5529\n",
            "Epoch: 3\tBatch: 2801\tAvg-Loss: 0.5562\n",
            "Epoch: 3\tBatch: 2901\tAvg-Loss: 0.5556\n",
            "Epoch: 3\tBatch: 3001\tAvg-Loss: 0.5571\n",
            "Epoch: 3\tBatch: 3101\tAvg-Loss: 0.5584\n",
            "Epoch: 3\tBatch: 3201\tAvg-Loss: 0.5589\n",
            "Epoch: 3\tBatch: 3301\tAvg-Loss: 0.5595\n",
            "Epoch: 3\tBatch: 3401\tAvg-Loss: 0.5594\n",
            "Epoch: 3\tBatch: 3501\tAvg-Loss: 0.5603\n",
            "Epoch: 3\tBatch: 3601\tAvg-Loss: 0.5583\n",
            "Epoch: 3\tBatch: 3701\tAvg-Loss: 0.5604\n",
            "Epoch: 3\tBatch: 3801\tAvg-Loss: 0.5604\n",
            "Epoch: 3\tBatch: 3901\tAvg-Loss: 0.5619\n",
            "Epoch: 3\tBatch: 4001\tAvg-Loss: 0.5630\n",
            "Epoch: 3\tBatch: 4101\tAvg-Loss: 0.5640\n",
            "Epoch: 3\tBatch: 4201\tAvg-Loss: 0.5644\n",
            "Epoch: 3\tBatch: 4301\tAvg-Loss: 0.5675\n",
            "Epoch: 3\tBatch: 4401\tAvg-Loss: 0.5686\n",
            "Epoch: 3\tBatch: 4501\tAvg-Loss: 0.5675\n",
            "Epoch: 3\tBatch: 4601\tAvg-Loss: 0.5677\n",
            "Epoch: 3\tBatch: 4701\tAvg-Loss: 0.5677\n",
            "Epoch: 3\tBatch: 4801\tAvg-Loss: 0.5690\n",
            "Epoch: 3\tBatch: 4901\tAvg-Loss: 0.5702\n",
            "Epoch: 3\tBatch: 5001\tAvg-Loss: 0.5701\n",
            "Epoch: 3\tBatch: 5101\tAvg-Loss: 0.5707\n",
            "Epoch: 3\tBatch: 5201\tAvg-Loss: 0.5708\n",
            "Epoch: 3\tBatch: 5301\tAvg-Loss: 0.5707\n",
            "Epoch: 3\tBatch: 5401\tAvg-Loss: 0.5687\n",
            "Epoch: 3\tBatch: 5501\tAvg-Loss: 0.5707\n",
            "Epoch: 3\tBatch: 5601\tAvg-Loss: 0.5699\n",
            "Epoch: 3\tBatch: 5701\tAvg-Loss: 0.5700\n",
            "Epoch: 3\tBatch: 5801\tAvg-Loss: 0.5704\n",
            "Epoch: 3\tBatch: 5901\tAvg-Loss: 0.5710\n",
            "Epoch: 3\tBatch: 6001\tAvg-Loss: 0.5715\n",
            "Epoch: 3\tBatch: 6101\tAvg-Loss: 0.5710\n",
            "Epoch: 3\tBatch: 6201\tAvg-Loss: 0.5712\n",
            "Epoch: 3\t Training Accuracy: 82.2220\t Validation Accuracy: 75.3700\t Avg-Loss: 0.5720\n",
            "Epoch: 4\tBatch: 1\tAvg-Loss: 0.2244\n",
            "Epoch: 4\tBatch: 101\tAvg-Loss: 0.4773\n",
            "Epoch: 4\tBatch: 201\tAvg-Loss: 0.4620\n",
            "Epoch: 4\tBatch: 301\tAvg-Loss: 0.4624\n",
            "Epoch: 4\tBatch: 401\tAvg-Loss: 0.4587\n",
            "Epoch: 4\tBatch: 501\tAvg-Loss: 0.4874\n",
            "Epoch: 4\tBatch: 601\tAvg-Loss: 0.4787\n",
            "Epoch: 4\tBatch: 701\tAvg-Loss: 0.4775\n",
            "Epoch: 4\tBatch: 801\tAvg-Loss: 0.4663\n",
            "Epoch: 4\tBatch: 901\tAvg-Loss: 0.4717\n",
            "Epoch: 4\tBatch: 1001\tAvg-Loss: 0.4657\n",
            "Epoch: 4\tBatch: 1101\tAvg-Loss: 0.4681\n",
            "Epoch: 4\tBatch: 1201\tAvg-Loss: 0.4647\n",
            "Epoch: 4\tBatch: 1301\tAvg-Loss: 0.4653\n",
            "Epoch: 4\tBatch: 1401\tAvg-Loss: 0.4684\n",
            "Epoch: 4\tBatch: 1501\tAvg-Loss: 0.4751\n",
            "Epoch: 4\tBatch: 1601\tAvg-Loss: 0.4742\n",
            "Epoch: 4\tBatch: 1701\tAvg-Loss: 0.4757\n",
            "Epoch: 4\tBatch: 1801\tAvg-Loss: 0.4765\n",
            "Epoch: 4\tBatch: 1901\tAvg-Loss: 0.4761\n",
            "Epoch: 4\tBatch: 2001\tAvg-Loss: 0.4781\n",
            "Epoch: 4\tBatch: 2101\tAvg-Loss: 0.4777\n",
            "Epoch: 4\tBatch: 2201\tAvg-Loss: 0.4779\n",
            "Epoch: 4\tBatch: 2301\tAvg-Loss: 0.4767\n",
            "Epoch: 4\tBatch: 2401\tAvg-Loss: 0.4788\n",
            "Epoch: 4\tBatch: 2501\tAvg-Loss: 0.4793\n",
            "Epoch: 4\tBatch: 2601\tAvg-Loss: 0.4787\n",
            "Epoch: 4\tBatch: 2701\tAvg-Loss: 0.4790\n",
            "Epoch: 4\tBatch: 2801\tAvg-Loss: 0.4804\n",
            "Epoch: 4\tBatch: 2901\tAvg-Loss: 0.4831\n",
            "Epoch: 4\tBatch: 3001\tAvg-Loss: 0.4814\n",
            "Epoch: 4\tBatch: 3101\tAvg-Loss: 0.4814\n",
            "Epoch: 4\tBatch: 3201\tAvg-Loss: 0.4824\n",
            "Epoch: 4\tBatch: 3301\tAvg-Loss: 0.4836\n",
            "Epoch: 4\tBatch: 3401\tAvg-Loss: 0.4864\n",
            "Epoch: 4\tBatch: 3501\tAvg-Loss: 0.4871\n",
            "Epoch: 4\tBatch: 3601\tAvg-Loss: 0.4887\n",
            "Epoch: 4\tBatch: 3701\tAvg-Loss: 0.4892\n",
            "Epoch: 4\tBatch: 3801\tAvg-Loss: 0.4875\n",
            "Epoch: 4\tBatch: 3901\tAvg-Loss: 0.4891\n",
            "Epoch: 4\tBatch: 4001\tAvg-Loss: 0.4912\n",
            "Epoch: 4\tBatch: 4101\tAvg-Loss: 0.4921\n",
            "Epoch: 4\tBatch: 4201\tAvg-Loss: 0.4938\n",
            "Epoch: 4\tBatch: 4301\tAvg-Loss: 0.4953\n",
            "Epoch: 4\tBatch: 4401\tAvg-Loss: 0.4935\n",
            "Epoch: 4\tBatch: 4501\tAvg-Loss: 0.4928\n",
            "Epoch: 4\tBatch: 4601\tAvg-Loss: 0.4944\n",
            "Epoch: 4\tBatch: 4701\tAvg-Loss: 0.4939\n",
            "Epoch: 4\tBatch: 4801\tAvg-Loss: 0.4942\n",
            "Epoch: 4\tBatch: 4901\tAvg-Loss: 0.4942\n",
            "Epoch: 4\tBatch: 5001\tAvg-Loss: 0.4945\n",
            "Epoch: 4\tBatch: 5101\tAvg-Loss: 0.4948\n",
            "Epoch: 4\tBatch: 5201\tAvg-Loss: 0.4953\n",
            "Epoch: 4\tBatch: 5301\tAvg-Loss: 0.4951\n",
            "Epoch: 4\tBatch: 5401\tAvg-Loss: 0.4944\n",
            "Epoch: 4\tBatch: 5501\tAvg-Loss: 0.4945\n",
            "Epoch: 4\tBatch: 5601\tAvg-Loss: 0.4961\n",
            "Epoch: 4\tBatch: 5701\tAvg-Loss: 0.4965\n",
            "Epoch: 4\tBatch: 5801\tAvg-Loss: 0.4972\n",
            "Epoch: 4\tBatch: 5901\tAvg-Loss: 0.4979\n",
            "Epoch: 4\tBatch: 6001\tAvg-Loss: 0.4983\n",
            "Epoch: 4\tBatch: 6101\tAvg-Loss: 0.4988\n",
            "Epoch: 4\tBatch: 6201\tAvg-Loss: 0.4996\n",
            "Epoch: 4\t Training Accuracy: 84.4460\t Validation Accuracy: 77.5200\t Avg-Loss: 0.4996\n",
            "Epoch: 5\tBatch: 1\tAvg-Loss: 0.0700\n",
            "Epoch: 5\tBatch: 101\tAvg-Loss: 0.4672\n",
            "Epoch: 5\tBatch: 201\tAvg-Loss: 0.4730\n",
            "Epoch: 5\tBatch: 301\tAvg-Loss: 0.4430\n",
            "Epoch: 5\tBatch: 401\tAvg-Loss: 0.4279\n",
            "Epoch: 5\tBatch: 501\tAvg-Loss: 0.4329\n",
            "Epoch: 5\tBatch: 601\tAvg-Loss: 0.4313\n",
            "Epoch: 5\tBatch: 701\tAvg-Loss: 0.4283\n",
            "Epoch: 5\tBatch: 801\tAvg-Loss: 0.4379\n",
            "Epoch: 5\tBatch: 901\tAvg-Loss: 0.4430\n",
            "Epoch: 5\tBatch: 1001\tAvg-Loss: 0.4463\n",
            "Epoch: 5\tBatch: 1101\tAvg-Loss: 0.4539\n",
            "Epoch: 5\tBatch: 1201\tAvg-Loss: 0.4554\n",
            "Epoch: 5\tBatch: 1301\tAvg-Loss: 0.4558\n",
            "Epoch: 5\tBatch: 1401\tAvg-Loss: 0.4566\n",
            "Epoch: 5\tBatch: 1501\tAvg-Loss: 0.4589\n",
            "Epoch: 5\tBatch: 1601\tAvg-Loss: 0.4575\n",
            "Epoch: 5\tBatch: 1701\tAvg-Loss: 0.4554\n",
            "Epoch: 5\tBatch: 1801\tAvg-Loss: 0.4572\n",
            "Epoch: 5\tBatch: 1901\tAvg-Loss: 0.4533\n",
            "Epoch: 5\tBatch: 2001\tAvg-Loss: 0.4575\n",
            "Epoch: 5\tBatch: 2101\tAvg-Loss: 0.4568\n",
            "Epoch: 5\tBatch: 2201\tAvg-Loss: 0.4552\n",
            "Epoch: 5\tBatch: 2301\tAvg-Loss: 0.4551\n",
            "Epoch: 5\tBatch: 2401\tAvg-Loss: 0.4535\n",
            "Epoch: 5\tBatch: 2501\tAvg-Loss: 0.4529\n",
            "Epoch: 5\tBatch: 2601\tAvg-Loss: 0.4526\n",
            "Epoch: 5\tBatch: 2701\tAvg-Loss: 0.4521\n",
            "Epoch: 5\tBatch: 2801\tAvg-Loss: 0.4514\n",
            "Epoch: 5\tBatch: 2901\tAvg-Loss: 0.4505\n",
            "Epoch: 5\tBatch: 3001\tAvg-Loss: 0.4502\n",
            "Epoch: 5\tBatch: 3101\tAvg-Loss: 0.4524\n",
            "Epoch: 5\tBatch: 3201\tAvg-Loss: 0.4530\n",
            "Epoch: 5\tBatch: 3301\tAvg-Loss: 0.4546\n",
            "Epoch: 5\tBatch: 3401\tAvg-Loss: 0.4579\n",
            "Epoch: 5\tBatch: 3501\tAvg-Loss: 0.4566\n",
            "Epoch: 5\tBatch: 3601\tAvg-Loss: 0.4560\n",
            "Epoch: 5\tBatch: 3701\tAvg-Loss: 0.4564\n",
            "Epoch: 5\tBatch: 3801\tAvg-Loss: 0.4557\n",
            "Epoch: 5\tBatch: 3901\tAvg-Loss: 0.4563\n",
            "Epoch: 5\tBatch: 4001\tAvg-Loss: 0.4558\n",
            "Epoch: 5\tBatch: 4101\tAvg-Loss: 0.4568\n",
            "Epoch: 5\tBatch: 4201\tAvg-Loss: 0.4589\n",
            "Epoch: 5\tBatch: 4301\tAvg-Loss: 0.4576\n",
            "Epoch: 5\tBatch: 4401\tAvg-Loss: 0.4561\n",
            "Epoch: 5\tBatch: 4501\tAvg-Loss: 0.4585\n",
            "Epoch: 5\tBatch: 4601\tAvg-Loss: 0.4597\n",
            "Epoch: 5\tBatch: 4701\tAvg-Loss: 0.4595\n",
            "Epoch: 5\tBatch: 4801\tAvg-Loss: 0.4595\n",
            "Epoch: 5\tBatch: 4901\tAvg-Loss: 0.4604\n",
            "Epoch: 5\tBatch: 5001\tAvg-Loss: 0.4604\n",
            "Epoch: 5\tBatch: 5101\tAvg-Loss: 0.4627\n",
            "Epoch: 5\tBatch: 5201\tAvg-Loss: 0.4624\n",
            "Epoch: 5\tBatch: 5301\tAvg-Loss: 0.4632\n",
            "Epoch: 5\tBatch: 5401\tAvg-Loss: 0.4627\n",
            "Epoch: 5\tBatch: 5501\tAvg-Loss: 0.4635\n",
            "Epoch: 5\tBatch: 5601\tAvg-Loss: 0.4644\n",
            "Epoch: 5\tBatch: 5701\tAvg-Loss: 0.4638\n",
            "Epoch: 5\tBatch: 5801\tAvg-Loss: 0.4646\n",
            "Epoch: 5\tBatch: 5901\tAvg-Loss: 0.4644\n",
            "Epoch: 5\tBatch: 6001\tAvg-Loss: 0.4650\n",
            "Epoch: 5\tBatch: 6101\tAvg-Loss: 0.4656\n",
            "Epoch: 5\tBatch: 6201\tAvg-Loss: 0.4655\n",
            "Epoch: 5\t Training Accuracy: 85.4420\t Validation Accuracy: 80.0600\t Avg-Loss: 0.4650\n",
            "Epoch: 6\tBatch: 1\tAvg-Loss: 0.1370\n",
            "Epoch: 6\tBatch: 101\tAvg-Loss: 0.3681\n",
            "Epoch: 6\tBatch: 201\tAvg-Loss: 0.3560\n",
            "Epoch: 6\tBatch: 301\tAvg-Loss: 0.3704\n",
            "Epoch: 6\tBatch: 401\tAvg-Loss: 0.3734\n",
            "Epoch: 6\tBatch: 501\tAvg-Loss: 0.3704\n",
            "Epoch: 6\tBatch: 601\tAvg-Loss: 0.3675\n",
            "Epoch: 6\tBatch: 701\tAvg-Loss: 0.3664\n",
            "Epoch: 6\tBatch: 801\tAvg-Loss: 0.3692\n",
            "Epoch: 6\tBatch: 901\tAvg-Loss: 0.3806\n",
            "Epoch: 6\tBatch: 1001\tAvg-Loss: 0.3813\n",
            "Epoch: 6\tBatch: 1101\tAvg-Loss: 0.3827\n",
            "Epoch: 6\tBatch: 1201\tAvg-Loss: 0.3832\n",
            "Epoch: 6\tBatch: 1301\tAvg-Loss: 0.3873\n",
            "Epoch: 6\tBatch: 1401\tAvg-Loss: 0.3886\n",
            "Epoch: 6\tBatch: 1501\tAvg-Loss: 0.3910\n",
            "Epoch: 6\tBatch: 1601\tAvg-Loss: 0.3938\n",
            "Epoch: 6\tBatch: 1701\tAvg-Loss: 0.3944\n",
            "Epoch: 6\tBatch: 1801\tAvg-Loss: 0.3941\n",
            "Epoch: 6\tBatch: 1901\tAvg-Loss: 0.3928\n",
            "Epoch: 6\tBatch: 2001\tAvg-Loss: 0.3942\n",
            "Epoch: 6\tBatch: 2101\tAvg-Loss: 0.3932\n",
            "Epoch: 6\tBatch: 2201\tAvg-Loss: 0.3925\n",
            "Epoch: 6\tBatch: 2301\tAvg-Loss: 0.3944\n",
            "Epoch: 6\tBatch: 2401\tAvg-Loss: 0.3938\n",
            "Epoch: 6\tBatch: 2501\tAvg-Loss: 0.3969\n",
            "Epoch: 6\tBatch: 2601\tAvg-Loss: 0.3967\n",
            "Epoch: 6\tBatch: 2701\tAvg-Loss: 0.3952\n",
            "Epoch: 6\tBatch: 2801\tAvg-Loss: 0.3991\n",
            "Epoch: 6\tBatch: 2901\tAvg-Loss: 0.3997\n",
            "Epoch: 6\tBatch: 3001\tAvg-Loss: 0.4010\n",
            "Epoch: 6\tBatch: 3101\tAvg-Loss: 0.4008\n",
            "Epoch: 6\tBatch: 3201\tAvg-Loss: 0.4022\n",
            "Epoch: 6\tBatch: 3301\tAvg-Loss: 0.4044\n",
            "Epoch: 6\tBatch: 3401\tAvg-Loss: 0.4073\n",
            "Epoch: 6\tBatch: 3501\tAvg-Loss: 0.4068\n",
            "Epoch: 6\tBatch: 3601\tAvg-Loss: 0.4085\n",
            "Epoch: 6\tBatch: 3701\tAvg-Loss: 0.4101\n",
            "Epoch: 6\tBatch: 3801\tAvg-Loss: 0.4114\n",
            "Epoch: 6\tBatch: 3901\tAvg-Loss: 0.4124\n",
            "Epoch: 6\tBatch: 4001\tAvg-Loss: 0.4121\n",
            "Epoch: 6\tBatch: 4101\tAvg-Loss: 0.4102\n",
            "Epoch: 6\tBatch: 4201\tAvg-Loss: 0.4109\n",
            "Epoch: 6\tBatch: 4301\tAvg-Loss: 0.4116\n",
            "Epoch: 6\tBatch: 4401\tAvg-Loss: 0.4131\n",
            "Epoch: 6\tBatch: 4501\tAvg-Loss: 0.4132\n",
            "Epoch: 6\tBatch: 4601\tAvg-Loss: 0.4134\n",
            "Epoch: 6\tBatch: 4701\tAvg-Loss: 0.4138\n",
            "Epoch: 6\tBatch: 4801\tAvg-Loss: 0.4145\n",
            "Epoch: 6\tBatch: 4901\tAvg-Loss: 0.4154\n",
            "Epoch: 6\tBatch: 5001\tAvg-Loss: 0.4165\n",
            "Epoch: 6\tBatch: 5101\tAvg-Loss: 0.4186\n",
            "Epoch: 6\tBatch: 5201\tAvg-Loss: 0.4196\n",
            "Epoch: 6\tBatch: 5301\tAvg-Loss: 0.4196\n",
            "Epoch: 6\tBatch: 5401\tAvg-Loss: 0.4187\n",
            "Epoch: 6\tBatch: 5501\tAvg-Loss: 0.4203\n",
            "Epoch: 6\tBatch: 5601\tAvg-Loss: 0.4206\n",
            "Epoch: 6\tBatch: 5701\tAvg-Loss: 0.4198\n",
            "Epoch: 6\tBatch: 5801\tAvg-Loss: 0.4203\n",
            "Epoch: 6\tBatch: 5901\tAvg-Loss: 0.4201\n",
            "Epoch: 6\tBatch: 6001\tAvg-Loss: 0.4204\n",
            "Epoch: 6\tBatch: 6101\tAvg-Loss: 0.4217\n",
            "Epoch: 6\tBatch: 6201\tAvg-Loss: 0.4216\n",
            "Epoch: 6\t Training Accuracy: 86.7000\t Validation Accuracy: 76.9300\t Avg-Loss: 0.4217\n",
            "Epoch: 7\tBatch: 1\tAvg-Loss: 0.0164\n",
            "Epoch: 7\tBatch: 101\tAvg-Loss: 0.2975\n",
            "Epoch: 7\tBatch: 201\tAvg-Loss: 0.3162\n",
            "Epoch: 7\tBatch: 301\tAvg-Loss: 0.3151\n",
            "Epoch: 7\tBatch: 401\tAvg-Loss: 0.3072\n",
            "Epoch: 7\tBatch: 501\tAvg-Loss: 0.3150\n",
            "Epoch: 7\tBatch: 601\tAvg-Loss: 0.3186\n",
            "Epoch: 7\tBatch: 701\tAvg-Loss: 0.3286\n",
            "Epoch: 7\tBatch: 801\tAvg-Loss: 0.3302\n",
            "Epoch: 7\tBatch: 901\tAvg-Loss: 0.3300\n",
            "Epoch: 7\tBatch: 1001\tAvg-Loss: 0.3334\n",
            "Epoch: 7\tBatch: 1101\tAvg-Loss: 0.3348\n",
            "Epoch: 7\tBatch: 1201\tAvg-Loss: 0.3360\n",
            "Epoch: 7\tBatch: 1301\tAvg-Loss: 0.3382\n",
            "Epoch: 7\tBatch: 1401\tAvg-Loss: 0.3386\n",
            "Epoch: 7\tBatch: 1501\tAvg-Loss: 0.3399\n",
            "Epoch: 7\tBatch: 1601\tAvg-Loss: 0.3433\n",
            "Epoch: 7\tBatch: 1701\tAvg-Loss: 0.3416\n",
            "Epoch: 7\tBatch: 1801\tAvg-Loss: 0.3470\n",
            "Epoch: 7\tBatch: 1901\tAvg-Loss: 0.3494\n",
            "Epoch: 7\tBatch: 2001\tAvg-Loss: 0.3501\n",
            "Epoch: 7\tBatch: 2101\tAvg-Loss: 0.3513\n",
            "Epoch: 7\tBatch: 2201\tAvg-Loss: 0.3517\n",
            "Epoch: 7\tBatch: 2301\tAvg-Loss: 0.3552\n",
            "Epoch: 7\tBatch: 2401\tAvg-Loss: 0.3552\n",
            "Epoch: 7\tBatch: 2501\tAvg-Loss: 0.3548\n",
            "Epoch: 7\tBatch: 2601\tAvg-Loss: 0.3570\n",
            "Epoch: 7\tBatch: 2701\tAvg-Loss: 0.3574\n",
            "Epoch: 7\tBatch: 2801\tAvg-Loss: 0.3604\n",
            "Epoch: 7\tBatch: 2901\tAvg-Loss: 0.3659\n",
            "Epoch: 7\tBatch: 3001\tAvg-Loss: 0.3653\n",
            "Epoch: 7\tBatch: 3101\tAvg-Loss: 0.3656\n",
            "Epoch: 7\tBatch: 3201\tAvg-Loss: 0.3667\n",
            "Epoch: 7\tBatch: 3301\tAvg-Loss: 0.3687\n",
            "Epoch: 7\tBatch: 3401\tAvg-Loss: 0.3677\n",
            "Epoch: 7\tBatch: 3501\tAvg-Loss: 0.3696\n",
            "Epoch: 7\tBatch: 3601\tAvg-Loss: 0.3707\n",
            "Epoch: 7\tBatch: 3701\tAvg-Loss: 0.3714\n",
            "Epoch: 7\tBatch: 3801\tAvg-Loss: 0.3728\n",
            "Epoch: 7\tBatch: 3901\tAvg-Loss: 0.3745\n",
            "Epoch: 7\tBatch: 4001\tAvg-Loss: 0.3746\n",
            "Epoch: 7\tBatch: 4101\tAvg-Loss: 0.3753\n",
            "Epoch: 7\tBatch: 4201\tAvg-Loss: 0.3759\n",
            "Epoch: 7\tBatch: 4301\tAvg-Loss: 0.3749\n",
            "Epoch: 7\tBatch: 4401\tAvg-Loss: 0.3772\n",
            "Epoch: 7\tBatch: 4501\tAvg-Loss: 0.3770\n",
            "Epoch: 7\tBatch: 4601\tAvg-Loss: 0.3761\n",
            "Epoch: 7\tBatch: 4701\tAvg-Loss: 0.3764\n",
            "Epoch: 7\tBatch: 4801\tAvg-Loss: 0.3771\n",
            "Epoch: 7\tBatch: 4901\tAvg-Loss: 0.3780\n",
            "Epoch: 7\tBatch: 5001\tAvg-Loss: 0.3776\n",
            "Epoch: 7\tBatch: 5101\tAvg-Loss: 0.3770\n",
            "Epoch: 7\tBatch: 5201\tAvg-Loss: 0.3776\n",
            "Epoch: 7\tBatch: 5301\tAvg-Loss: 0.3782\n",
            "Epoch: 7\tBatch: 5401\tAvg-Loss: 0.3925\n",
            "Epoch: 7\tBatch: 5501\tAvg-Loss: 0.3985\n",
            "Epoch: 7\tBatch: 5601\tAvg-Loss: 0.4015\n",
            "Epoch: 7\tBatch: 5701\tAvg-Loss: 0.4039\n",
            "Epoch: 7\tBatch: 5801\tAvg-Loss: 0.4056\n",
            "Epoch: 7\tBatch: 5901\tAvg-Loss: 0.4068\n",
            "Epoch: 7\tBatch: 6001\tAvg-Loss: 0.4075\n",
            "Epoch: 7\tBatch: 6101\tAvg-Loss: 0.4082\n",
            "Epoch: 7\tBatch: 6201\tAvg-Loss: 0.4094\n",
            "Epoch: 7\t Training Accuracy: 87.0300\t Validation Accuracy: 77.1400\t Avg-Loss: 0.4094\n",
            "Epoch: 8\tBatch: 1\tAvg-Loss: 0.9923\n",
            "Epoch: 8\tBatch: 101\tAvg-Loss: 0.4065\n",
            "Epoch: 8\tBatch: 201\tAvg-Loss: 0.3802\n",
            "Epoch: 8\tBatch: 301\tAvg-Loss: 0.3638\n",
            "Epoch: 8\tBatch: 401\tAvg-Loss: 0.3546\n",
            "Epoch: 8\tBatch: 501\tAvg-Loss: 0.3493\n",
            "Epoch: 8\tBatch: 601\tAvg-Loss: 0.3507\n",
            "Epoch: 8\tBatch: 701\tAvg-Loss: 0.3535\n",
            "Epoch: 8\tBatch: 801\tAvg-Loss: 0.3486\n",
            "Epoch: 8\tBatch: 901\tAvg-Loss: 0.3458\n",
            "Epoch: 8\tBatch: 1001\tAvg-Loss: 0.3433\n",
            "Epoch: 8\tBatch: 1101\tAvg-Loss: 0.3410\n",
            "Epoch: 8\tBatch: 1201\tAvg-Loss: 0.3445\n",
            "Epoch: 8\tBatch: 1301\tAvg-Loss: 0.3468\n",
            "Epoch: 8\tBatch: 1401\tAvg-Loss: 0.3453\n",
            "Epoch: 8\tBatch: 1501\tAvg-Loss: 0.3490\n",
            "Epoch: 8\tBatch: 1601\tAvg-Loss: 0.3479\n",
            "Epoch: 8\tBatch: 1701\tAvg-Loss: 0.3480\n",
            "Epoch: 8\tBatch: 1801\tAvg-Loss: 0.3507\n",
            "Epoch: 8\tBatch: 1901\tAvg-Loss: 0.3509\n",
            "Epoch: 8\tBatch: 2001\tAvg-Loss: 0.3548\n",
            "Epoch: 8\tBatch: 2101\tAvg-Loss: 0.3571\n",
            "Epoch: 8\tBatch: 2201\tAvg-Loss: 0.3587\n",
            "Epoch: 8\tBatch: 2301\tAvg-Loss: 0.3553\n",
            "Epoch: 8\tBatch: 2401\tAvg-Loss: 0.3553\n",
            "Epoch: 8\tBatch: 2501\tAvg-Loss: 0.3576\n",
            "Epoch: 8\tBatch: 2601\tAvg-Loss: 0.3599\n",
            "Epoch: 8\tBatch: 2701\tAvg-Loss: 0.3582\n",
            "Epoch: 8\tBatch: 2801\tAvg-Loss: 0.3585\n",
            "Epoch: 8\tBatch: 2901\tAvg-Loss: 0.3567\n",
            "Epoch: 8\tBatch: 3001\tAvg-Loss: 0.3552\n",
            "Epoch: 8\tBatch: 3101\tAvg-Loss: 0.3574\n",
            "Epoch: 8\tBatch: 3201\tAvg-Loss: 0.3577\n",
            "Epoch: 8\tBatch: 3301\tAvg-Loss: 0.3558\n",
            "Epoch: 8\tBatch: 3401\tAvg-Loss: 0.3562\n",
            "Epoch: 8\tBatch: 3501\tAvg-Loss: 0.3551\n",
            "Epoch: 8\tBatch: 3601\tAvg-Loss: 0.3556\n",
            "Epoch: 8\tBatch: 3701\tAvg-Loss: 0.3542\n",
            "Epoch: 8\tBatch: 3801\tAvg-Loss: 0.3548\n",
            "Epoch: 8\tBatch: 3901\tAvg-Loss: 0.3547\n",
            "Epoch: 8\tBatch: 4001\tAvg-Loss: 0.3544\n",
            "Epoch: 8\tBatch: 4101\tAvg-Loss: 0.3560\n",
            "Epoch: 8\tBatch: 4201\tAvg-Loss: 0.3555\n",
            "Epoch: 8\tBatch: 4301\tAvg-Loss: 0.3567\n",
            "Epoch: 8\tBatch: 4401\tAvg-Loss: 0.3566\n",
            "Epoch: 8\tBatch: 4501\tAvg-Loss: 0.3588\n",
            "Epoch: 8\tBatch: 4601\tAvg-Loss: 0.3596\n",
            "Epoch: 8\tBatch: 4701\tAvg-Loss: 0.3609\n",
            "Epoch: 8\tBatch: 4801\tAvg-Loss: 0.3605\n",
            "Epoch: 8\tBatch: 4901\tAvg-Loss: 0.3618\n",
            "Epoch: 8\tBatch: 5001\tAvg-Loss: 0.3637\n",
            "Epoch: 8\tBatch: 5101\tAvg-Loss: 0.3645\n",
            "Epoch: 8\tBatch: 5201\tAvg-Loss: 0.3660\n",
            "Epoch: 8\tBatch: 5301\tAvg-Loss: 0.3655\n",
            "Epoch: 8\tBatch: 5401\tAvg-Loss: 0.3646\n",
            "Epoch: 8\tBatch: 5501\tAvg-Loss: 0.3646\n",
            "Epoch: 8\tBatch: 5601\tAvg-Loss: 0.3645\n",
            "Epoch: 8\tBatch: 5701\tAvg-Loss: 0.3664\n",
            "Epoch: 8\tBatch: 5801\tAvg-Loss: 0.3671\n",
            "Epoch: 8\tBatch: 5901\tAvg-Loss: 0.3677\n",
            "Epoch: 8\tBatch: 6001\tAvg-Loss: 0.3677\n",
            "Epoch: 8\tBatch: 6101\tAvg-Loss: 0.3677\n",
            "Epoch: 8\tBatch: 6201\tAvg-Loss: 0.3677\n",
            "Epoch: 8\t Training Accuracy: 88.3180\t Validation Accuracy: 77.7900\t Avg-Loss: 0.3672\n",
            "Epoch: 9\tBatch: 1\tAvg-Loss: 1.0468\n",
            "Epoch: 9\tBatch: 101\tAvg-Loss: 0.2856\n",
            "Epoch: 9\tBatch: 201\tAvg-Loss: 0.2792\n",
            "Epoch: 9\tBatch: 301\tAvg-Loss: 0.3018\n",
            "Epoch: 9\tBatch: 401\tAvg-Loss: 0.2952\n",
            "Epoch: 9\tBatch: 501\tAvg-Loss: 0.2918\n",
            "Epoch: 9\tBatch: 601\tAvg-Loss: 0.2968\n",
            "Epoch: 9\tBatch: 701\tAvg-Loss: 0.2905\n",
            "Epoch: 9\tBatch: 801\tAvg-Loss: 0.2867\n",
            "Epoch: 9\tBatch: 901\tAvg-Loss: 0.2921\n",
            "Epoch: 9\tBatch: 1001\tAvg-Loss: 0.2867\n",
            "Epoch: 9\tBatch: 1101\tAvg-Loss: 0.2825\n",
            "Epoch: 9\tBatch: 1201\tAvg-Loss: 0.2936\n",
            "Epoch: 9\tBatch: 1301\tAvg-Loss: 0.2984\n",
            "Epoch: 9\tBatch: 1401\tAvg-Loss: 0.2971\n",
            "Epoch: 9\tBatch: 1501\tAvg-Loss: 0.2948\n",
            "Epoch: 9\tBatch: 1601\tAvg-Loss: 0.2948\n",
            "Epoch: 9\tBatch: 1701\tAvg-Loss: 0.2959\n",
            "Epoch: 9\tBatch: 1801\tAvg-Loss: 0.2973\n",
            "Epoch: 9\tBatch: 1901\tAvg-Loss: 0.3016\n",
            "Epoch: 9\tBatch: 2001\tAvg-Loss: 0.3019\n",
            "Epoch: 9\tBatch: 2101\tAvg-Loss: 0.3039\n",
            "Epoch: 9\tBatch: 2201\tAvg-Loss: 0.3092\n",
            "Epoch: 9\tBatch: 2301\tAvg-Loss: 0.3086\n",
            "Epoch: 9\tBatch: 2401\tAvg-Loss: 0.3066\n",
            "Epoch: 9\tBatch: 2501\tAvg-Loss: 0.3072\n",
            "Epoch: 9\tBatch: 2601\tAvg-Loss: 0.3091\n",
            "Epoch: 9\tBatch: 2701\tAvg-Loss: 0.3085\n",
            "Epoch: 9\tBatch: 2801\tAvg-Loss: 0.3099\n",
            "Epoch: 9\tBatch: 2901\tAvg-Loss: 0.3103\n",
            "Epoch: 9\tBatch: 3001\tAvg-Loss: 0.3120\n",
            "Epoch: 9\tBatch: 3101\tAvg-Loss: 0.3122\n",
            "Epoch: 9\tBatch: 3201\tAvg-Loss: 0.3116\n",
            "Epoch: 9\tBatch: 3301\tAvg-Loss: 0.3128\n",
            "Epoch: 9\tBatch: 3401\tAvg-Loss: 0.3150\n",
            "Epoch: 9\tBatch: 3501\tAvg-Loss: 0.3160\n",
            "Epoch: 9\tBatch: 3601\tAvg-Loss: 0.3167\n",
            "Epoch: 9\tBatch: 3701\tAvg-Loss: 0.3155\n",
            "Epoch: 9\tBatch: 3801\tAvg-Loss: 0.3143\n",
            "Epoch: 9\tBatch: 3901\tAvg-Loss: 0.3166\n",
            "Epoch: 9\tBatch: 4001\tAvg-Loss: 0.3158\n",
            "Epoch: 9\tBatch: 4101\tAvg-Loss: 0.3154\n",
            "Epoch: 9\tBatch: 4201\tAvg-Loss: 0.3150\n",
            "Epoch: 9\tBatch: 4301\tAvg-Loss: 0.3145\n",
            "Epoch: 9\tBatch: 4401\tAvg-Loss: 0.3149\n",
            "Epoch: 9\tBatch: 4501\tAvg-Loss: 0.3144\n",
            "Epoch: 9\tBatch: 4601\tAvg-Loss: 0.3144\n",
            "Epoch: 9\tBatch: 4701\tAvg-Loss: 0.3156\n",
            "Epoch: 9\tBatch: 4801\tAvg-Loss: 0.3169\n",
            "Epoch: 9\tBatch: 4901\tAvg-Loss: 0.3166\n",
            "Epoch: 9\tBatch: 5001\tAvg-Loss: 0.3163\n",
            "Epoch: 9\tBatch: 5101\tAvg-Loss: 0.3170\n",
            "Epoch: 9\tBatch: 5201\tAvg-Loss: 0.3183\n",
            "Epoch: 9\tBatch: 5301\tAvg-Loss: 0.3182\n",
            "Epoch: 9\tBatch: 5401\tAvg-Loss: 0.3191\n",
            "Epoch: 9\tBatch: 5501\tAvg-Loss: 0.3187\n",
            "Epoch: 9\tBatch: 5601\tAvg-Loss: 0.3190\n",
            "Epoch: 9\tBatch: 5701\tAvg-Loss: 0.3185\n",
            "Epoch: 9\tBatch: 5801\tAvg-Loss: 0.3192\n",
            "Epoch: 9\tBatch: 5901\tAvg-Loss: 0.3184\n",
            "Epoch: 9\tBatch: 6001\tAvg-Loss: 0.3201\n",
            "Epoch: 9\tBatch: 6101\tAvg-Loss: 0.3194\n",
            "Epoch: 9\tBatch: 6201\tAvg-Loss: 0.3195\n",
            "Epoch: 9\t Training Accuracy: 89.8480\t Validation Accuracy: 78.8500\t Avg-Loss: 0.3196\n",
            "Epoch: 10\tBatch: 1\tAvg-Loss: 0.0239\n",
            "Epoch: 10\tBatch: 101\tAvg-Loss: 0.2469\n",
            "Epoch: 10\tBatch: 201\tAvg-Loss: 0.2393\n",
            "Epoch: 10\tBatch: 301\tAvg-Loss: 0.2232\n",
            "Epoch: 10\tBatch: 401\tAvg-Loss: 0.2250\n",
            "Epoch: 10\tBatch: 501\tAvg-Loss: 0.2402\n",
            "Epoch: 10\tBatch: 601\tAvg-Loss: 0.2376\n",
            "Epoch: 10\tBatch: 701\tAvg-Loss: 0.2327\n",
            "Epoch: 10\tBatch: 801\tAvg-Loss: 0.2384\n",
            "Epoch: 10\tBatch: 901\tAvg-Loss: 0.2400\n",
            "Epoch: 10\tBatch: 1001\tAvg-Loss: 0.2413\n",
            "Epoch: 10\tBatch: 1101\tAvg-Loss: 0.2384\n",
            "Epoch: 10\tBatch: 1201\tAvg-Loss: 0.2397\n",
            "Epoch: 10\tBatch: 1301\tAvg-Loss: 0.2412\n",
            "Epoch: 10\tBatch: 1401\tAvg-Loss: 0.2395\n",
            "Epoch: 10\tBatch: 1501\tAvg-Loss: 0.2440\n",
            "Epoch: 10\tBatch: 1601\tAvg-Loss: 0.2482\n",
            "Epoch: 10\tBatch: 1701\tAvg-Loss: 0.2497\n",
            "Epoch: 10\tBatch: 1801\tAvg-Loss: 0.2504\n",
            "Epoch: 10\tBatch: 1901\tAvg-Loss: 0.2480\n",
            "Epoch: 10\tBatch: 2001\tAvg-Loss: 0.2502\n",
            "Epoch: 10\tBatch: 2101\tAvg-Loss: 0.2518\n",
            "Epoch: 10\tBatch: 2201\tAvg-Loss: 0.2551\n",
            "Epoch: 10\tBatch: 2301\tAvg-Loss: 0.2552\n",
            "Epoch: 10\tBatch: 2401\tAvg-Loss: 0.2549\n",
            "Epoch: 10\tBatch: 2501\tAvg-Loss: 0.2590\n",
            "Epoch: 10\tBatch: 2601\tAvg-Loss: 0.2602\n",
            "Epoch: 10\tBatch: 2701\tAvg-Loss: 0.2608\n",
            "Epoch: 10\tBatch: 2801\tAvg-Loss: 0.2628\n",
            "Epoch: 10\tBatch: 2901\tAvg-Loss: 0.2659\n",
            "Epoch: 10\tBatch: 3001\tAvg-Loss: 0.2654\n",
            "Epoch: 10\tBatch: 3101\tAvg-Loss: 0.2656\n",
            "Epoch: 10\tBatch: 3201\tAvg-Loss: 0.2652\n",
            "Epoch: 10\tBatch: 3301\tAvg-Loss: 0.2663\n",
            "Epoch: 10\tBatch: 3401\tAvg-Loss: 0.2658\n",
            "Epoch: 10\tBatch: 3501\tAvg-Loss: 0.2674\n",
            "Epoch: 10\tBatch: 3601\tAvg-Loss: 0.2674\n",
            "Epoch: 10\tBatch: 3701\tAvg-Loss: 0.2676\n",
            "Epoch: 10\tBatch: 3801\tAvg-Loss: 0.2679\n",
            "Epoch: 10\tBatch: 3901\tAvg-Loss: 0.2685\n",
            "Epoch: 10\tBatch: 4001\tAvg-Loss: 0.2690\n",
            "Epoch: 10\tBatch: 4101\tAvg-Loss: 0.2685\n",
            "Epoch: 10\tBatch: 4201\tAvg-Loss: 0.2696\n",
            "Epoch: 10\tBatch: 4301\tAvg-Loss: 0.2724\n",
            "Epoch: 10\tBatch: 4401\tAvg-Loss: 0.2731\n",
            "Epoch: 10\tBatch: 4501\tAvg-Loss: 0.2739\n",
            "Epoch: 10\tBatch: 4601\tAvg-Loss: 0.2748\n",
            "Epoch: 10\tBatch: 4701\tAvg-Loss: 0.2756\n",
            "Epoch: 10\tBatch: 4801\tAvg-Loss: 0.2750\n",
            "Epoch: 10\tBatch: 4901\tAvg-Loss: 0.2756\n",
            "Epoch: 10\tBatch: 5001\tAvg-Loss: 0.2777\n",
            "Epoch: 10\tBatch: 5101\tAvg-Loss: 0.2792\n",
            "Epoch: 10\tBatch: 5201\tAvg-Loss: 0.2786\n",
            "Epoch: 10\tBatch: 5301\tAvg-Loss: 0.2807\n",
            "Epoch: 10\tBatch: 5401\tAvg-Loss: 0.2817\n",
            "Epoch: 10\tBatch: 5501\tAvg-Loss: 0.2821\n",
            "Epoch: 10\tBatch: 5601\tAvg-Loss: 0.2826\n",
            "Epoch: 10\tBatch: 5701\tAvg-Loss: 0.2822\n",
            "Epoch: 10\tBatch: 5801\tAvg-Loss: 0.2822\n",
            "Epoch: 10\tBatch: 5901\tAvg-Loss: 0.2830\n",
            "Epoch: 10\tBatch: 6001\tAvg-Loss: 0.2841\n",
            "Epoch: 10\tBatch: 6101\tAvg-Loss: 0.2833\n",
            "Epoch: 10\tBatch: 6201\tAvg-Loss: 0.2840\n",
            "Epoch: 10\t Training Accuracy: 90.6620\t Validation Accuracy: 75.9700\t Avg-Loss: 0.2848\n",
            "Epoch: 11\tBatch: 1\tAvg-Loss: 0.0266\n",
            "Epoch: 11\tBatch: 101\tAvg-Loss: 0.2761\n",
            "Epoch: 11\tBatch: 201\tAvg-Loss: 0.2251\n",
            "Epoch: 11\tBatch: 301\tAvg-Loss: 0.2111\n",
            "Epoch: 11\tBatch: 401\tAvg-Loss: 0.2198\n",
            "Epoch: 11\tBatch: 501\tAvg-Loss: 0.2148\n",
            "Epoch: 11\tBatch: 601\tAvg-Loss: 0.2331\n",
            "Epoch: 11\tBatch: 701\tAvg-Loss: 0.2313\n",
            "Epoch: 11\tBatch: 801\tAvg-Loss: 0.2422\n",
            "Epoch: 11\tBatch: 901\tAvg-Loss: 0.2472\n",
            "Epoch: 11\tBatch: 1001\tAvg-Loss: 0.2470\n",
            "Epoch: 11\tBatch: 1101\tAvg-Loss: 0.2464\n",
            "Epoch: 11\tBatch: 1201\tAvg-Loss: 0.2492\n",
            "Epoch: 11\tBatch: 1301\tAvg-Loss: 0.2451\n",
            "Epoch: 11\tBatch: 1401\tAvg-Loss: 0.2473\n",
            "Epoch: 11\tBatch: 1501\tAvg-Loss: 0.2489\n",
            "Epoch: 11\tBatch: 1601\tAvg-Loss: 0.2479\n",
            "Epoch: 11\tBatch: 1701\tAvg-Loss: 0.2492\n",
            "Epoch: 11\tBatch: 1801\tAvg-Loss: 0.2495\n",
            "Epoch: 11\tBatch: 1901\tAvg-Loss: 0.2497\n",
            "Epoch: 11\tBatch: 2001\tAvg-Loss: 0.2547\n",
            "Epoch: 11\tBatch: 2101\tAvg-Loss: 0.2585\n",
            "Epoch: 11\tBatch: 2201\tAvg-Loss: 0.2572\n",
            "Epoch: 11\tBatch: 2301\tAvg-Loss: 0.2556\n",
            "Epoch: 11\tBatch: 2401\tAvg-Loss: 0.2562\n",
            "Epoch: 11\tBatch: 2501\tAvg-Loss: 0.2550\n",
            "Epoch: 11\tBatch: 2601\tAvg-Loss: 0.2570\n",
            "Epoch: 11\tBatch: 2701\tAvg-Loss: 0.2564\n",
            "Epoch: 11\tBatch: 2801\tAvg-Loss: 0.2565\n",
            "Epoch: 11\tBatch: 2901\tAvg-Loss: 0.2576\n",
            "Epoch: 11\tBatch: 3001\tAvg-Loss: 0.2570\n",
            "Epoch: 11\tBatch: 3101\tAvg-Loss: 0.2560\n",
            "Epoch: 11\tBatch: 3201\tAvg-Loss: 0.2545\n",
            "Epoch: 11\tBatch: 3301\tAvg-Loss: 0.2577\n",
            "Epoch: 11\tBatch: 3401\tAvg-Loss: 0.2593\n",
            "Epoch: 11\tBatch: 3501\tAvg-Loss: 0.2584\n",
            "Epoch: 11\tBatch: 3601\tAvg-Loss: 0.2583\n",
            "Epoch: 11\tBatch: 3701\tAvg-Loss: 0.2587\n",
            "Epoch: 11\tBatch: 3801\tAvg-Loss: 0.2593\n",
            "Epoch: 11\tBatch: 3901\tAvg-Loss: 0.2604\n",
            "Epoch: 11\tBatch: 4001\tAvg-Loss: 0.2601\n",
            "Epoch: 11\tBatch: 4101\tAvg-Loss: 0.2611\n",
            "Epoch: 11\tBatch: 4201\tAvg-Loss: 0.2599\n",
            "Epoch: 11\tBatch: 4301\tAvg-Loss: 0.2604\n",
            "Epoch: 11\tBatch: 4401\tAvg-Loss: 0.2614\n",
            "Epoch: 11\tBatch: 4501\tAvg-Loss: 0.2609\n",
            "Epoch: 11\tBatch: 4601\tAvg-Loss: 0.2607\n",
            "Epoch: 11\tBatch: 4701\tAvg-Loss: 0.2622\n",
            "Epoch: 11\tBatch: 4801\tAvg-Loss: 0.2630\n",
            "Epoch: 11\tBatch: 4901\tAvg-Loss: 0.2649\n",
            "Epoch: 11\tBatch: 5001\tAvg-Loss: 0.2653\n",
            "Epoch: 11\tBatch: 5101\tAvg-Loss: 0.2650\n",
            "Epoch: 11\tBatch: 5201\tAvg-Loss: 0.2651\n",
            "Epoch: 11\tBatch: 5301\tAvg-Loss: 0.2645\n",
            "Epoch: 11\tBatch: 5401\tAvg-Loss: 0.2646\n",
            "Epoch: 11\tBatch: 5501\tAvg-Loss: 0.2648\n",
            "Epoch: 11\tBatch: 5601\tAvg-Loss: 0.2656\n",
            "Epoch: 11\tBatch: 5701\tAvg-Loss: 0.2666\n",
            "Epoch: 11\tBatch: 5801\tAvg-Loss: 0.2682\n",
            "Epoch: 11\tBatch: 5901\tAvg-Loss: 0.2668\n",
            "Epoch: 11\tBatch: 6001\tAvg-Loss: 0.2674\n",
            "Epoch: 11\tBatch: 6101\tAvg-Loss: 0.2679\n",
            "Epoch: 11\tBatch: 6201\tAvg-Loss: 0.2692\n",
            "Epoch: 11\t Training Accuracy: 91.3520\t Validation Accuracy: 78.4900\t Avg-Loss: 0.2694\n",
            "Epoch    15: reducing learning rate of group 0 to 4.0000e-03.\n",
            "Epoch: 12\tBatch: 1\tAvg-Loss: 0.0383\n",
            "Epoch: 12\tBatch: 101\tAvg-Loss: 0.1530\n",
            "Epoch: 12\tBatch: 201\tAvg-Loss: 0.1398\n",
            "Epoch: 12\tBatch: 301\tAvg-Loss: 0.1277\n",
            "Epoch: 12\tBatch: 401\tAvg-Loss: 0.1281\n",
            "Epoch: 12\tBatch: 501\tAvg-Loss: 0.1293\n",
            "Epoch: 12\tBatch: 601\tAvg-Loss: 0.1228\n",
            "Epoch: 12\tBatch: 701\tAvg-Loss: 0.1218\n",
            "Epoch: 12\tBatch: 801\tAvg-Loss: 0.1181\n",
            "Epoch: 12\tBatch: 901\tAvg-Loss: 0.1150\n",
            "Epoch: 12\tBatch: 1001\tAvg-Loss: 0.1133\n",
            "Epoch: 12\tBatch: 1101\tAvg-Loss: 0.1104\n",
            "Epoch: 12\tBatch: 1201\tAvg-Loss: 0.1093\n",
            "Epoch: 12\tBatch: 1301\tAvg-Loss: 0.1069\n",
            "Epoch: 12\tBatch: 1401\tAvg-Loss: 0.1032\n",
            "Epoch: 12\tBatch: 1501\tAvg-Loss: 0.1010\n",
            "Epoch: 12\tBatch: 1601\tAvg-Loss: 0.0994\n",
            "Epoch: 12\tBatch: 1701\tAvg-Loss: 0.0969\n",
            "Epoch: 12\tBatch: 1801\tAvg-Loss: 0.0961\n",
            "Epoch: 12\tBatch: 1901\tAvg-Loss: 0.0962\n",
            "Epoch: 12\tBatch: 2001\tAvg-Loss: 0.0961\n",
            "Epoch: 12\tBatch: 2101\tAvg-Loss: 0.0947\n",
            "Epoch: 12\tBatch: 2201\tAvg-Loss: 0.0928\n",
            "Epoch: 12\tBatch: 2301\tAvg-Loss: 0.0913\n",
            "Epoch: 12\tBatch: 2401\tAvg-Loss: 0.0910\n",
            "Epoch: 12\tBatch: 2501\tAvg-Loss: 0.0899\n",
            "Epoch: 12\tBatch: 2601\tAvg-Loss: 0.0896\n",
            "Epoch: 12\tBatch: 2701\tAvg-Loss: 0.0893\n",
            "Epoch: 12\tBatch: 2801\tAvg-Loss: 0.0890\n",
            "Epoch: 12\tBatch: 2901\tAvg-Loss: 0.0874\n",
            "Epoch: 12\tBatch: 3001\tAvg-Loss: 0.0874\n",
            "Epoch: 12\tBatch: 3101\tAvg-Loss: 0.0874\n",
            "Epoch: 12\tBatch: 3201\tAvg-Loss: 0.0879\n",
            "Epoch: 12\tBatch: 3301\tAvg-Loss: 0.0872\n",
            "Epoch: 12\tBatch: 3401\tAvg-Loss: 0.0861\n",
            "Epoch: 12\tBatch: 3501\tAvg-Loss: 0.0854\n",
            "Epoch: 12\tBatch: 3601\tAvg-Loss: 0.0852\n",
            "Epoch: 12\tBatch: 3701\tAvg-Loss: 0.0851\n",
            "Epoch: 12\tBatch: 3801\tAvg-Loss: 0.0847\n",
            "Epoch: 12\tBatch: 3901\tAvg-Loss: 0.0846\n",
            "Epoch: 12\tBatch: 4001\tAvg-Loss: 0.0836\n",
            "Epoch: 12\tBatch: 4101\tAvg-Loss: 0.0837\n",
            "Epoch: 12\tBatch: 4201\tAvg-Loss: 0.0833\n",
            "Epoch: 12\tBatch: 4301\tAvg-Loss: 0.0829\n",
            "Epoch: 12\tBatch: 4401\tAvg-Loss: 0.0825\n",
            "Epoch: 12\tBatch: 4501\tAvg-Loss: 0.0825\n",
            "Epoch: 12\tBatch: 4601\tAvg-Loss: 0.0826\n",
            "Epoch: 12\tBatch: 4701\tAvg-Loss: 0.0828\n",
            "Epoch: 12\tBatch: 4801\tAvg-Loss: 0.0827\n",
            "Epoch: 12\tBatch: 4901\tAvg-Loss: 0.0831\n",
            "Epoch: 12\tBatch: 5001\tAvg-Loss: 0.0824\n",
            "Epoch: 12\tBatch: 5101\tAvg-Loss: 0.0817\n",
            "Epoch: 12\tBatch: 5201\tAvg-Loss: 0.0810\n",
            "Epoch: 12\tBatch: 5301\tAvg-Loss: 0.0804\n",
            "Epoch: 12\tBatch: 5401\tAvg-Loss: 0.0803\n",
            "Epoch: 12\tBatch: 5501\tAvg-Loss: 0.0797\n",
            "Epoch: 12\tBatch: 5601\tAvg-Loss: 0.0791\n",
            "Epoch: 12\tBatch: 5701\tAvg-Loss: 0.0789\n",
            "Epoch: 12\tBatch: 5801\tAvg-Loss: 0.0784\n",
            "Epoch: 12\tBatch: 5901\tAvg-Loss: 0.0781\n",
            "Epoch: 12\tBatch: 6001\tAvg-Loss: 0.0784\n",
            "Epoch: 12\tBatch: 6101\tAvg-Loss: 0.0789\n",
            "Epoch: 12\tBatch: 6201\tAvg-Loss: 0.0789\n",
            "Epoch: 12\t Training Accuracy: 97.3780\t Validation Accuracy: 82.9400\t Avg-Loss: 0.0787\n",
            "Epoch: 13\tBatch: 1\tAvg-Loss: 0.0316\n",
            "Epoch: 13\tBatch: 101\tAvg-Loss: 0.0490\n",
            "Epoch: 13\tBatch: 201\tAvg-Loss: 0.0370\n",
            "Epoch: 13\tBatch: 301\tAvg-Loss: 0.0363\n",
            "Epoch: 13\tBatch: 401\tAvg-Loss: 0.0354\n",
            "Epoch: 13\tBatch: 501\tAvg-Loss: 0.0396\n",
            "Epoch: 13\tBatch: 601\tAvg-Loss: 0.0417\n",
            "Epoch: 13\tBatch: 701\tAvg-Loss: 0.0437\n",
            "Epoch: 13\tBatch: 801\tAvg-Loss: 0.0434\n",
            "Epoch: 13\tBatch: 901\tAvg-Loss: 0.0443\n",
            "Epoch: 13\tBatch: 1001\tAvg-Loss: 0.0436\n",
            "Epoch: 13\tBatch: 1101\tAvg-Loss: 0.0436\n",
            "Epoch: 13\tBatch: 1201\tAvg-Loss: 0.0425\n",
            "Epoch: 13\tBatch: 1301\tAvg-Loss: 0.0441\n",
            "Epoch: 13\tBatch: 1401\tAvg-Loss: 0.0442\n",
            "Epoch: 13\tBatch: 1501\tAvg-Loss: 0.0437\n",
            "Epoch: 13\tBatch: 1601\tAvg-Loss: 0.0441\n",
            "Epoch: 13\tBatch: 1701\tAvg-Loss: 0.0437\n",
            "Epoch: 13\tBatch: 1801\tAvg-Loss: 0.0442\n",
            "Epoch: 13\tBatch: 1901\tAvg-Loss: 0.0437\n",
            "Epoch: 13\tBatch: 2001\tAvg-Loss: 0.0449\n",
            "Epoch: 13\tBatch: 2101\tAvg-Loss: 0.0454\n",
            "Epoch: 13\tBatch: 2201\tAvg-Loss: 0.0451\n",
            "Epoch: 13\tBatch: 2301\tAvg-Loss: 0.0446\n",
            "Epoch: 13\tBatch: 2401\tAvg-Loss: 0.0443\n",
            "Epoch: 13\tBatch: 2501\tAvg-Loss: 0.0449\n",
            "Epoch: 13\tBatch: 2601\tAvg-Loss: 0.0460\n",
            "Epoch: 13\tBatch: 2701\tAvg-Loss: 0.0458\n",
            "Epoch: 13\tBatch: 2801\tAvg-Loss: 0.0459\n",
            "Epoch: 13\tBatch: 2901\tAvg-Loss: 0.0458\n",
            "Epoch: 13\tBatch: 3001\tAvg-Loss: 0.0458\n",
            "Epoch: 13\tBatch: 3101\tAvg-Loss: 0.0460\n",
            "Epoch: 13\tBatch: 3201\tAvg-Loss: 0.0459\n",
            "Epoch: 13\tBatch: 3301\tAvg-Loss: 0.0459\n",
            "Epoch: 13\tBatch: 3401\tAvg-Loss: 0.0462\n",
            "Epoch: 13\tBatch: 3501\tAvg-Loss: 0.0461\n",
            "Epoch: 13\tBatch: 3601\tAvg-Loss: 0.0460\n",
            "Epoch: 13\tBatch: 3701\tAvg-Loss: 0.0456\n",
            "Epoch: 13\tBatch: 3801\tAvg-Loss: 0.0455\n",
            "Epoch: 13\tBatch: 3901\tAvg-Loss: 0.0453\n",
            "Epoch: 13\tBatch: 4001\tAvg-Loss: 0.0452\n",
            "Epoch: 13\tBatch: 4101\tAvg-Loss: 0.0449\n",
            "Epoch: 13\tBatch: 4201\tAvg-Loss: 0.0447\n",
            "Epoch: 13\tBatch: 4301\tAvg-Loss: 0.0446\n",
            "Epoch: 13\tBatch: 4401\tAvg-Loss: 0.0443\n",
            "Epoch: 13\tBatch: 4501\tAvg-Loss: 0.0441\n",
            "Epoch: 13\tBatch: 4601\tAvg-Loss: 0.0445\n",
            "Epoch: 13\tBatch: 4701\tAvg-Loss: 0.0445\n",
            "Epoch: 13\tBatch: 4801\tAvg-Loss: 0.0447\n",
            "Epoch: 13\tBatch: 4901\tAvg-Loss: 0.0443\n",
            "Epoch: 13\tBatch: 5001\tAvg-Loss: 0.0446\n",
            "Epoch: 13\tBatch: 5101\tAvg-Loss: 0.0449\n",
            "Epoch: 13\tBatch: 5201\tAvg-Loss: 0.0448\n",
            "Epoch: 13\tBatch: 5301\tAvg-Loss: 0.0449\n",
            "Epoch: 13\tBatch: 5401\tAvg-Loss: 0.0447\n",
            "Epoch: 13\tBatch: 5501\tAvg-Loss: 0.0446\n",
            "Epoch: 13\tBatch: 5601\tAvg-Loss: 0.0451\n",
            "Epoch: 13\tBatch: 5701\tAvg-Loss: 0.0452\n",
            "Epoch: 13\tBatch: 5801\tAvg-Loss: 0.0449\n",
            "Epoch: 13\tBatch: 5901\tAvg-Loss: 0.0449\n",
            "Epoch: 13\tBatch: 6001\tAvg-Loss: 0.0449\n",
            "Epoch: 13\tBatch: 6101\tAvg-Loss: 0.0448\n",
            "Epoch: 13\tBatch: 6201\tAvg-Loss: 0.0446\n",
            "Epoch: 13\t Training Accuracy: 98.4660\t Validation Accuracy: 82.6500\t Avg-Loss: 0.0449\n",
            "Epoch: 14\tBatch: 1\tAvg-Loss: 0.0002\n",
            "Epoch: 14\tBatch: 101\tAvg-Loss: 0.0269\n",
            "Epoch: 14\tBatch: 201\tAvg-Loss: 0.0342\n",
            "Epoch: 14\tBatch: 301\tAvg-Loss: 0.0356\n",
            "Epoch: 14\tBatch: 401\tAvg-Loss: 0.0347\n",
            "Epoch: 14\tBatch: 501\tAvg-Loss: 0.0360\n",
            "Epoch: 14\tBatch: 601\tAvg-Loss: 0.0342\n",
            "Epoch: 14\tBatch: 701\tAvg-Loss: 0.0319\n",
            "Epoch: 14\tBatch: 801\tAvg-Loss: 0.0324\n",
            "Epoch: 14\tBatch: 901\tAvg-Loss: 0.0319\n",
            "Epoch: 14\tBatch: 1001\tAvg-Loss: 0.0311\n",
            "Epoch: 14\tBatch: 1101\tAvg-Loss: 0.0313\n",
            "Epoch: 14\tBatch: 1201\tAvg-Loss: 0.0320\n",
            "Epoch: 14\tBatch: 1301\tAvg-Loss: 0.0324\n",
            "Epoch: 14\tBatch: 1401\tAvg-Loss: 0.0314\n",
            "Epoch: 14\tBatch: 1501\tAvg-Loss: 0.0307\n",
            "Epoch: 14\tBatch: 1601\tAvg-Loss: 0.0295\n",
            "Epoch: 14\tBatch: 1701\tAvg-Loss: 0.0293\n",
            "Epoch: 14\tBatch: 1801\tAvg-Loss: 0.0290\n",
            "Epoch: 14\tBatch: 1901\tAvg-Loss: 0.0291\n",
            "Epoch: 14\tBatch: 2001\tAvg-Loss: 0.0301\n",
            "Epoch: 14\tBatch: 2101\tAvg-Loss: 0.0307\n",
            "Epoch: 14\tBatch: 2201\tAvg-Loss: 0.0314\n",
            "Epoch: 14\tBatch: 2301\tAvg-Loss: 0.0315\n",
            "Epoch: 14\tBatch: 2401\tAvg-Loss: 0.0312\n",
            "Epoch: 14\tBatch: 2501\tAvg-Loss: 0.0311\n",
            "Epoch: 14\tBatch: 2601\tAvg-Loss: 0.0305\n",
            "Epoch: 14\tBatch: 2701\tAvg-Loss: 0.0304\n",
            "Epoch: 14\tBatch: 2801\tAvg-Loss: 0.0303\n",
            "Epoch: 14\tBatch: 2901\tAvg-Loss: 0.0306\n",
            "Epoch: 14\tBatch: 3001\tAvg-Loss: 0.0309\n",
            "Epoch: 14\tBatch: 3101\tAvg-Loss: 0.0313\n",
            "Epoch: 14\tBatch: 3201\tAvg-Loss: 0.0314\n",
            "Epoch: 14\tBatch: 3301\tAvg-Loss: 0.0323\n",
            "Epoch: 14\tBatch: 3401\tAvg-Loss: 0.0326\n",
            "Epoch: 14\tBatch: 3501\tAvg-Loss: 0.0325\n",
            "Epoch: 14\tBatch: 3601\tAvg-Loss: 0.0326\n",
            "Epoch: 14\tBatch: 3701\tAvg-Loss: 0.0327\n",
            "Epoch: 14\tBatch: 3801\tAvg-Loss: 0.0325\n",
            "Epoch: 14\tBatch: 3901\tAvg-Loss: 0.0325\n",
            "Epoch: 14\tBatch: 4001\tAvg-Loss: 0.0327\n",
            "Epoch: 14\tBatch: 4101\tAvg-Loss: 0.0329\n",
            "Epoch: 14\tBatch: 4201\tAvg-Loss: 0.0327\n",
            "Epoch: 14\tBatch: 4301\tAvg-Loss: 0.0324\n",
            "Epoch: 14\tBatch: 4401\tAvg-Loss: 0.0328\n",
            "Epoch: 14\tBatch: 4501\tAvg-Loss: 0.0329\n",
            "Epoch: 14\tBatch: 4601\tAvg-Loss: 0.0332\n",
            "Epoch: 14\tBatch: 4701\tAvg-Loss: 0.0336\n",
            "Epoch: 14\tBatch: 4801\tAvg-Loss: 0.0335\n",
            "Epoch: 14\tBatch: 4901\tAvg-Loss: 0.0343\n",
            "Epoch: 14\tBatch: 5001\tAvg-Loss: 0.0346\n",
            "Epoch: 14\tBatch: 5101\tAvg-Loss: 0.0352\n",
            "Epoch: 14\tBatch: 5201\tAvg-Loss: 0.0354\n",
            "Epoch: 14\tBatch: 5301\tAvg-Loss: 0.0356\n",
            "Epoch: 14\tBatch: 5401\tAvg-Loss: 0.0358\n",
            "Epoch: 14\tBatch: 5501\tAvg-Loss: 0.0357\n",
            "Epoch: 14\tBatch: 5601\tAvg-Loss: 0.0356\n",
            "Epoch: 14\tBatch: 5701\tAvg-Loss: 0.0357\n",
            "Epoch: 14\tBatch: 5801\tAvg-Loss: 0.0355\n",
            "Epoch: 14\tBatch: 5901\tAvg-Loss: 0.0355\n",
            "Epoch: 14\tBatch: 6001\tAvg-Loss: 0.0353\n",
            "Epoch: 14\tBatch: 6101\tAvg-Loss: 0.0352\n",
            "Epoch: 14\tBatch: 6201\tAvg-Loss: 0.0351\n",
            "Epoch: 14\t Training Accuracy: 98.8520\t Validation Accuracy: 82.9600\t Avg-Loss: 0.0351\n",
            "Epoch: 15\tBatch: 1\tAvg-Loss: 0.0194\n",
            "Epoch: 15\tBatch: 101\tAvg-Loss: 0.0285\n",
            "Epoch: 15\tBatch: 201\tAvg-Loss: 0.0296\n",
            "Epoch: 15\tBatch: 301\tAvg-Loss: 0.0284\n",
            "Epoch: 15\tBatch: 401\tAvg-Loss: 0.0299\n",
            "Epoch: 15\tBatch: 501\tAvg-Loss: 0.0275\n",
            "Epoch: 15\tBatch: 601\tAvg-Loss: 0.0289\n",
            "Epoch: 15\tBatch: 701\tAvg-Loss: 0.0302\n",
            "Epoch: 15\tBatch: 801\tAvg-Loss: 0.0292\n",
            "Epoch: 15\tBatch: 901\tAvg-Loss: 0.0274\n",
            "Epoch: 15\tBatch: 1001\tAvg-Loss: 0.0282\n",
            "Epoch: 15\tBatch: 1101\tAvg-Loss: 0.0295\n",
            "Epoch: 15\tBatch: 1201\tAvg-Loss: 0.0299\n",
            "Epoch: 15\tBatch: 1301\tAvg-Loss: 0.0289\n",
            "Epoch: 15\tBatch: 1401\tAvg-Loss: 0.0292\n",
            "Epoch: 15\tBatch: 1501\tAvg-Loss: 0.0287\n",
            "Epoch: 15\tBatch: 1601\tAvg-Loss: 0.0282\n",
            "Epoch: 15\tBatch: 1701\tAvg-Loss: 0.0272\n",
            "Epoch: 15\tBatch: 1801\tAvg-Loss: 0.0272\n",
            "Epoch: 15\tBatch: 1901\tAvg-Loss: 0.0266\n",
            "Epoch: 15\tBatch: 2001\tAvg-Loss: 0.0269\n",
            "Epoch: 15\tBatch: 2101\tAvg-Loss: 0.0268\n",
            "Epoch: 15\tBatch: 2201\tAvg-Loss: 0.0265\n",
            "Epoch: 15\tBatch: 2301\tAvg-Loss: 0.0272\n",
            "Epoch: 15\tBatch: 2401\tAvg-Loss: 0.0272\n",
            "Epoch: 15\tBatch: 2501\tAvg-Loss: 0.0275\n",
            "Epoch: 15\tBatch: 2601\tAvg-Loss: 0.0277\n",
            "Epoch: 15\tBatch: 2701\tAvg-Loss: 0.0273\n",
            "Epoch: 15\tBatch: 2801\tAvg-Loss: 0.0274\n",
            "Epoch: 15\tBatch: 2901\tAvg-Loss: 0.0275\n",
            "Epoch: 15\tBatch: 3001\tAvg-Loss: 0.0270\n",
            "Epoch: 15\tBatch: 3101\tAvg-Loss: 0.0269\n",
            "Epoch: 15\tBatch: 3201\tAvg-Loss: 0.0267\n",
            "Epoch: 15\tBatch: 3301\tAvg-Loss: 0.0263\n",
            "Epoch: 15\tBatch: 3401\tAvg-Loss: 0.0260\n",
            "Epoch: 15\tBatch: 3501\tAvg-Loss: 0.0260\n",
            "Epoch: 15\tBatch: 3601\tAvg-Loss: 0.0268\n",
            "Epoch: 15\tBatch: 3701\tAvg-Loss: 0.0267\n",
            "Epoch: 15\tBatch: 3801\tAvg-Loss: 0.0263\n",
            "Epoch: 15\tBatch: 3901\tAvg-Loss: 0.0260\n",
            "Epoch: 15\tBatch: 4001\tAvg-Loss: 0.0264\n",
            "Epoch: 15\tBatch: 4101\tAvg-Loss: 0.0267\n",
            "Epoch: 15\tBatch: 4201\tAvg-Loss: 0.0270\n",
            "Epoch: 15\tBatch: 4301\tAvg-Loss: 0.0270\n",
            "Epoch: 15\tBatch: 4401\tAvg-Loss: 0.0270\n",
            "Epoch: 15\tBatch: 4501\tAvg-Loss: 0.0271\n",
            "Epoch: 15\tBatch: 4601\tAvg-Loss: 0.0268\n",
            "Epoch: 15\tBatch: 4701\tAvg-Loss: 0.0266\n",
            "Epoch: 15\tBatch: 4801\tAvg-Loss: 0.0265\n",
            "Epoch: 15\tBatch: 4901\tAvg-Loss: 0.0263\n",
            "Epoch: 15\tBatch: 5001\tAvg-Loss: 0.0262\n",
            "Epoch: 15\tBatch: 5101\tAvg-Loss: 0.0263\n",
            "Epoch: 15\tBatch: 5201\tAvg-Loss: 0.0266\n",
            "Epoch: 15\tBatch: 5301\tAvg-Loss: 0.0263\n",
            "Epoch: 15\tBatch: 5401\tAvg-Loss: 0.0263\n",
            "Epoch: 15\tBatch: 5501\tAvg-Loss: 0.0265\n",
            "Epoch: 15\tBatch: 5601\tAvg-Loss: 0.0263\n",
            "Epoch: 15\tBatch: 5701\tAvg-Loss: 0.0265\n",
            "Epoch: 15\tBatch: 5801\tAvg-Loss: 0.0268\n",
            "Epoch: 15\tBatch: 5901\tAvg-Loss: 0.0269\n",
            "Epoch: 15\tBatch: 6001\tAvg-Loss: 0.0267\n",
            "Epoch: 15\tBatch: 6101\tAvg-Loss: 0.0269\n",
            "Epoch: 15\tBatch: 6201\tAvg-Loss: 0.0268\n",
            "Epoch: 15\t Training Accuracy: 99.1060\t Validation Accuracy: 83.4400\t Avg-Loss: 0.0268\n",
            "Epoch: 16\tBatch: 1\tAvg-Loss: 0.0077\n",
            "Epoch: 16\tBatch: 101\tAvg-Loss: 0.0233\n",
            "Epoch: 16\tBatch: 201\tAvg-Loss: 0.0202\n",
            "Epoch: 16\tBatch: 301\tAvg-Loss: 0.0204\n",
            "Epoch: 16\tBatch: 401\tAvg-Loss: 0.0261\n",
            "Epoch: 16\tBatch: 501\tAvg-Loss: 0.0256\n",
            "Epoch: 16\tBatch: 601\tAvg-Loss: 0.0239\n",
            "Epoch: 16\tBatch: 701\tAvg-Loss: 0.0263\n",
            "Epoch: 16\tBatch: 801\tAvg-Loss: 0.0277\n",
            "Epoch: 16\tBatch: 901\tAvg-Loss: 0.0273\n",
            "Epoch: 16\tBatch: 1001\tAvg-Loss: 0.0274\n",
            "Epoch: 16\tBatch: 1101\tAvg-Loss: 0.0275\n",
            "Epoch: 16\tBatch: 1201\tAvg-Loss: 0.0277\n",
            "Epoch: 16\tBatch: 1301\tAvg-Loss: 0.0270\n",
            "Epoch: 16\tBatch: 1401\tAvg-Loss: 0.0271\n",
            "Epoch: 16\tBatch: 1501\tAvg-Loss: 0.0267\n",
            "Epoch: 16\tBatch: 1601\tAvg-Loss: 0.0265\n",
            "Epoch: 16\tBatch: 1701\tAvg-Loss: 0.0256\n",
            "Epoch: 16\tBatch: 1801\tAvg-Loss: 0.0257\n",
            "Epoch: 16\tBatch: 1901\tAvg-Loss: 0.0256\n",
            "Epoch: 16\tBatch: 2001\tAvg-Loss: 0.0248\n",
            "Epoch: 16\tBatch: 2101\tAvg-Loss: 0.0243\n",
            "Epoch: 16\tBatch: 2201\tAvg-Loss: 0.0237\n",
            "Epoch: 16\tBatch: 2301\tAvg-Loss: 0.0238\n",
            "Epoch: 16\tBatch: 2401\tAvg-Loss: 0.0238\n",
            "Epoch: 16\tBatch: 2501\tAvg-Loss: 0.0234\n",
            "Epoch: 16\tBatch: 2601\tAvg-Loss: 0.0233\n",
            "Epoch: 16\tBatch: 2701\tAvg-Loss: 0.0230\n",
            "Epoch: 16\tBatch: 2801\tAvg-Loss: 0.0231\n",
            "Epoch: 16\tBatch: 2901\tAvg-Loss: 0.0230\n",
            "Epoch: 16\tBatch: 3001\tAvg-Loss: 0.0225\n",
            "Epoch: 16\tBatch: 3101\tAvg-Loss: 0.0221\n",
            "Epoch: 16\tBatch: 3201\tAvg-Loss: 0.0221\n",
            "Epoch: 16\tBatch: 3301\tAvg-Loss: 0.0218\n",
            "Epoch: 16\tBatch: 3401\tAvg-Loss: 0.0216\n",
            "Epoch: 16\tBatch: 3501\tAvg-Loss: 0.0212\n",
            "Epoch: 16\tBatch: 3601\tAvg-Loss: 0.0214\n",
            "Epoch: 16\tBatch: 3701\tAvg-Loss: 0.0212\n",
            "Epoch: 16\tBatch: 3801\tAvg-Loss: 0.0213\n",
            "Epoch: 16\tBatch: 3901\tAvg-Loss: 0.0212\n",
            "Epoch: 16\tBatch: 4001\tAvg-Loss: 0.0211\n",
            "Epoch: 16\tBatch: 4101\tAvg-Loss: 0.0215\n",
            "Epoch: 16\tBatch: 4201\tAvg-Loss: 0.0213\n",
            "Epoch: 16\tBatch: 4301\tAvg-Loss: 0.0216\n",
            "Epoch: 16\tBatch: 4401\tAvg-Loss: 0.0217\n",
            "Epoch: 16\tBatch: 4501\tAvg-Loss: 0.0222\n",
            "Epoch: 16\tBatch: 4601\tAvg-Loss: 0.0222\n",
            "Epoch: 16\tBatch: 4701\tAvg-Loss: 0.0222\n",
            "Epoch: 16\tBatch: 4801\tAvg-Loss: 0.0226\n",
            "Epoch: 16\tBatch: 4901\tAvg-Loss: 0.0227\n",
            "Epoch: 16\tBatch: 5001\tAvg-Loss: 0.0230\n",
            "Epoch: 16\tBatch: 5101\tAvg-Loss: 0.0233\n",
            "Epoch: 16\tBatch: 5201\tAvg-Loss: 0.0233\n",
            "Epoch: 16\tBatch: 5301\tAvg-Loss: 0.0232\n",
            "Epoch: 16\tBatch: 5401\tAvg-Loss: 0.0230\n",
            "Epoch: 16\tBatch: 5501\tAvg-Loss: 0.0234\n",
            "Epoch: 16\tBatch: 5601\tAvg-Loss: 0.0233\n",
            "Epoch: 16\tBatch: 5701\tAvg-Loss: 0.0235\n",
            "Epoch: 16\tBatch: 5801\tAvg-Loss: 0.0234\n",
            "Epoch: 16\tBatch: 5901\tAvg-Loss: 0.0233\n",
            "Epoch: 16\tBatch: 6001\tAvg-Loss: 0.0234\n",
            "Epoch: 16\tBatch: 6101\tAvg-Loss: 0.0234\n",
            "Epoch: 16\tBatch: 6201\tAvg-Loss: 0.0235\n",
            "Epoch: 16\t Training Accuracy: 99.2620\t Validation Accuracy: 83.0900\t Avg-Loss: 0.0236\n",
            "Epoch: 17\tBatch: 1\tAvg-Loss: 0.0186\n",
            "Epoch: 17\tBatch: 101\tAvg-Loss: 0.0272\n",
            "Epoch: 17\tBatch: 201\tAvg-Loss: 0.0186\n",
            "Epoch: 17\tBatch: 301\tAvg-Loss: 0.0167\n",
            "Epoch: 17\tBatch: 401\tAvg-Loss: 0.0153\n",
            "Epoch: 17\tBatch: 501\tAvg-Loss: 0.0144\n",
            "Epoch: 17\tBatch: 601\tAvg-Loss: 0.0149\n",
            "Epoch: 17\tBatch: 701\tAvg-Loss: 0.0144\n",
            "Epoch: 17\tBatch: 801\tAvg-Loss: 0.0160\n",
            "Epoch: 17\tBatch: 901\tAvg-Loss: 0.0163\n",
            "Epoch: 17\tBatch: 1001\tAvg-Loss: 0.0163\n",
            "Epoch: 17\tBatch: 1101\tAvg-Loss: 0.0164\n",
            "Epoch: 17\tBatch: 1201\tAvg-Loss: 0.0169\n",
            "Epoch: 17\tBatch: 1301\tAvg-Loss: 0.0173\n",
            "Epoch: 17\tBatch: 1401\tAvg-Loss: 0.0185\n",
            "Epoch: 17\tBatch: 1501\tAvg-Loss: 0.0182\n",
            "Epoch: 17\tBatch: 1601\tAvg-Loss: 0.0187\n",
            "Epoch: 17\tBatch: 1701\tAvg-Loss: 0.0201\n",
            "Epoch: 17\tBatch: 1801\tAvg-Loss: 0.0207\n",
            "Epoch: 17\tBatch: 1901\tAvg-Loss: 0.0203\n",
            "Epoch: 17\tBatch: 2001\tAvg-Loss: 0.0205\n",
            "Epoch: 17\tBatch: 2101\tAvg-Loss: 0.0200\n",
            "Epoch: 17\tBatch: 2201\tAvg-Loss: 0.0196\n",
            "Epoch: 17\tBatch: 2301\tAvg-Loss: 0.0191\n",
            "Epoch: 17\tBatch: 2401\tAvg-Loss: 0.0191\n",
            "Epoch: 17\tBatch: 2501\tAvg-Loss: 0.0187\n",
            "Epoch: 17\tBatch: 2601\tAvg-Loss: 0.0187\n",
            "Epoch: 17\tBatch: 2701\tAvg-Loss: 0.0187\n",
            "Epoch: 17\tBatch: 2801\tAvg-Loss: 0.0184\n",
            "Epoch: 17\tBatch: 2901\tAvg-Loss: 0.0186\n",
            "Epoch: 17\tBatch: 3001\tAvg-Loss: 0.0187\n",
            "Epoch: 17\tBatch: 3101\tAvg-Loss: 0.0187\n",
            "Epoch: 17\tBatch: 3201\tAvg-Loss: 0.0187\n",
            "Epoch: 17\tBatch: 3301\tAvg-Loss: 0.0185\n",
            "Epoch: 17\tBatch: 3401\tAvg-Loss: 0.0186\n",
            "Epoch: 17\tBatch: 3501\tAvg-Loss: 0.0187\n",
            "Epoch: 17\tBatch: 3601\tAvg-Loss: 0.0185\n",
            "Epoch: 17\tBatch: 3701\tAvg-Loss: 0.0188\n",
            "Epoch: 17\tBatch: 3801\tAvg-Loss: 0.0191\n",
            "Epoch: 17\tBatch: 3901\tAvg-Loss: 0.0189\n",
            "Epoch: 17\tBatch: 4001\tAvg-Loss: 0.0190\n",
            "Epoch: 17\tBatch: 4101\tAvg-Loss: 0.0190\n",
            "Epoch: 17\tBatch: 4201\tAvg-Loss: 0.0189\n",
            "Epoch: 17\tBatch: 4301\tAvg-Loss: 0.0187\n",
            "Epoch: 17\tBatch: 4401\tAvg-Loss: 0.0185\n",
            "Epoch: 17\tBatch: 4501\tAvg-Loss: 0.0182\n",
            "Epoch: 17\tBatch: 4601\tAvg-Loss: 0.0187\n",
            "Epoch: 17\tBatch: 4701\tAvg-Loss: 0.0189\n",
            "Epoch: 17\tBatch: 4801\tAvg-Loss: 0.0187\n",
            "Epoch: 17\tBatch: 4901\tAvg-Loss: 0.0193\n",
            "Epoch: 17\tBatch: 5001\tAvg-Loss: 0.0192\n",
            "Epoch: 17\tBatch: 5101\tAvg-Loss: 0.0191\n",
            "Epoch: 17\tBatch: 5201\tAvg-Loss: 0.0192\n",
            "Epoch: 17\tBatch: 5301\tAvg-Loss: 0.0194\n",
            "Epoch: 17\tBatch: 5401\tAvg-Loss: 0.0193\n",
            "Epoch: 17\tBatch: 5501\tAvg-Loss: 0.0192\n",
            "Epoch: 17\tBatch: 5601\tAvg-Loss: 0.0193\n",
            "Epoch: 17\tBatch: 5701\tAvg-Loss: 0.0195\n",
            "Epoch: 17\tBatch: 5801\tAvg-Loss: 0.0195\n",
            "Epoch: 17\tBatch: 5901\tAvg-Loss: 0.0194\n",
            "Epoch: 17\tBatch: 6001\tAvg-Loss: 0.0193\n",
            "Epoch: 17\tBatch: 6101\tAvg-Loss: 0.0193\n",
            "Epoch: 17\tBatch: 6201\tAvg-Loss: 0.0191\n",
            "Epoch: 17\t Training Accuracy: 99.4140\t Validation Accuracy: 83.5300\t Avg-Loss: 0.0191\n",
            "Epoch: 18\tBatch: 1\tAvg-Loss: 0.0003\n",
            "Epoch: 18\tBatch: 101\tAvg-Loss: 0.0182\n",
            "Epoch: 18\tBatch: 201\tAvg-Loss: 0.0142\n",
            "Epoch: 18\tBatch: 301\tAvg-Loss: 0.0151\n",
            "Epoch: 18\tBatch: 401\tAvg-Loss: 0.0147\n",
            "Epoch: 18\tBatch: 501\tAvg-Loss: 0.0161\n",
            "Epoch: 18\tBatch: 601\tAvg-Loss: 0.0157\n",
            "Epoch: 18\tBatch: 701\tAvg-Loss: 0.0165\n",
            "Epoch: 18\tBatch: 801\tAvg-Loss: 0.0165\n",
            "Epoch: 18\tBatch: 901\tAvg-Loss: 0.0168\n",
            "Epoch: 18\tBatch: 1001\tAvg-Loss: 0.0164\n",
            "Epoch: 18\tBatch: 1101\tAvg-Loss: 0.0162\n",
            "Epoch: 18\tBatch: 1201\tAvg-Loss: 0.0163\n",
            "Epoch: 18\tBatch: 1301\tAvg-Loss: 0.0157\n",
            "Epoch: 18\tBatch: 1401\tAvg-Loss: 0.0155\n",
            "Epoch: 18\tBatch: 1501\tAvg-Loss: 0.0155\n",
            "Epoch: 18\tBatch: 1601\tAvg-Loss: 0.0167\n",
            "Epoch: 18\tBatch: 1701\tAvg-Loss: 0.0164\n",
            "Epoch: 18\tBatch: 1801\tAvg-Loss: 0.0167\n",
            "Epoch: 18\tBatch: 1901\tAvg-Loss: 0.0161\n",
            "Epoch: 18\tBatch: 2001\tAvg-Loss: 0.0159\n",
            "Epoch: 18\tBatch: 2101\tAvg-Loss: 0.0154\n",
            "Epoch: 18\tBatch: 2201\tAvg-Loss: 0.0152\n",
            "Epoch: 18\tBatch: 2301\tAvg-Loss: 0.0151\n",
            "Epoch: 18\tBatch: 2401\tAvg-Loss: 0.0152\n",
            "Epoch: 18\tBatch: 2501\tAvg-Loss: 0.0154\n",
            "Epoch: 18\tBatch: 2601\tAvg-Loss: 0.0151\n",
            "Epoch: 18\tBatch: 2701\tAvg-Loss: 0.0158\n",
            "Epoch: 18\tBatch: 2801\tAvg-Loss: 0.0159\n",
            "Epoch: 18\tBatch: 2901\tAvg-Loss: 0.0161\n",
            "Epoch: 18\tBatch: 3001\tAvg-Loss: 0.0162\n",
            "Epoch: 18\tBatch: 3101\tAvg-Loss: 0.0160\n",
            "Epoch: 18\tBatch: 3201\tAvg-Loss: 0.0156\n",
            "Epoch: 18\tBatch: 3301\tAvg-Loss: 0.0158\n",
            "Epoch: 18\tBatch: 3401\tAvg-Loss: 0.0157\n",
            "Epoch: 18\tBatch: 3501\tAvg-Loss: 0.0157\n",
            "Epoch: 18\tBatch: 3601\tAvg-Loss: 0.0156\n",
            "Epoch: 18\tBatch: 3701\tAvg-Loss: 0.0154\n",
            "Epoch: 18\tBatch: 3801\tAvg-Loss: 0.0153\n",
            "Epoch: 18\tBatch: 3901\tAvg-Loss: 0.0152\n",
            "Epoch: 18\tBatch: 4001\tAvg-Loss: 0.0150\n",
            "Epoch: 18\tBatch: 4101\tAvg-Loss: 0.0158\n",
            "Epoch: 18\tBatch: 4201\tAvg-Loss: 0.0162\n",
            "Epoch: 18\tBatch: 4301\tAvg-Loss: 0.0162\n",
            "Epoch: 18\tBatch: 4401\tAvg-Loss: 0.0161\n",
            "Epoch: 18\tBatch: 4501\tAvg-Loss: 0.0167\n",
            "Epoch: 18\tBatch: 4601\tAvg-Loss: 0.0166\n",
            "Epoch: 18\tBatch: 4701\tAvg-Loss: 0.0164\n",
            "Epoch: 18\tBatch: 4801\tAvg-Loss: 0.0165\n",
            "Epoch: 18\tBatch: 4901\tAvg-Loss: 0.0166\n",
            "Epoch: 18\tBatch: 5001\tAvg-Loss: 0.0165\n",
            "Epoch: 18\tBatch: 5101\tAvg-Loss: 0.0165\n",
            "Epoch: 18\tBatch: 5201\tAvg-Loss: 0.0167\n",
            "Epoch: 18\tBatch: 5301\tAvg-Loss: 0.0167\n",
            "Epoch: 18\tBatch: 5401\tAvg-Loss: 0.0170\n",
            "Epoch: 18\tBatch: 5501\tAvg-Loss: 0.0172\n",
            "Epoch: 18\tBatch: 5601\tAvg-Loss: 0.0172\n",
            "Epoch: 18\tBatch: 5701\tAvg-Loss: 0.0174\n",
            "Epoch: 18\tBatch: 5801\tAvg-Loss: 0.0175\n",
            "Epoch: 18\tBatch: 5901\tAvg-Loss: 0.0174\n",
            "Epoch: 18\tBatch: 6001\tAvg-Loss: 0.0175\n",
            "Epoch: 18\tBatch: 6101\tAvg-Loss: 0.0175\n",
            "Epoch: 18\tBatch: 6201\tAvg-Loss: 0.0174\n",
            "Epoch: 18\t Training Accuracy: 99.4560\t Validation Accuracy: 83.1000\t Avg-Loss: 0.0174\n",
            "Epoch: 19\tBatch: 1\tAvg-Loss: 0.0025\n",
            "Epoch: 19\tBatch: 101\tAvg-Loss: 0.0086\n",
            "Epoch: 19\tBatch: 201\tAvg-Loss: 0.0149\n",
            "Epoch: 19\tBatch: 301\tAvg-Loss: 0.0145\n",
            "Epoch: 19\tBatch: 401\tAvg-Loss: 0.0151\n",
            "Epoch: 19\tBatch: 501\tAvg-Loss: 0.0166\n",
            "Epoch: 19\tBatch: 601\tAvg-Loss: 0.0157\n",
            "Epoch: 19\tBatch: 701\tAvg-Loss: 0.0160\n",
            "Epoch: 19\tBatch: 801\tAvg-Loss: 0.0159\n",
            "Epoch: 19\tBatch: 901\tAvg-Loss: 0.0156\n",
            "Epoch: 19\tBatch: 1001\tAvg-Loss: 0.0152\n",
            "Epoch: 19\tBatch: 1101\tAvg-Loss: 0.0162\n",
            "Epoch: 19\tBatch: 1201\tAvg-Loss: 0.0157\n",
            "Epoch: 19\tBatch: 1301\tAvg-Loss: 0.0161\n",
            "Epoch: 19\tBatch: 1401\tAvg-Loss: 0.0164\n",
            "Epoch: 19\tBatch: 1501\tAvg-Loss: 0.0165\n",
            "Epoch: 19\tBatch: 1601\tAvg-Loss: 0.0162\n",
            "Epoch: 19\tBatch: 1701\tAvg-Loss: 0.0161\n",
            "Epoch: 19\tBatch: 1801\tAvg-Loss: 0.0169\n",
            "Epoch: 19\tBatch: 1901\tAvg-Loss: 0.0168\n",
            "Epoch: 19\tBatch: 2001\tAvg-Loss: 0.0165\n",
            "Epoch: 19\tBatch: 2101\tAvg-Loss: 0.0163\n",
            "Epoch: 19\tBatch: 2201\tAvg-Loss: 0.0165\n",
            "Epoch: 19\tBatch: 2301\tAvg-Loss: 0.0167\n",
            "Epoch: 19\tBatch: 2401\tAvg-Loss: 0.0169\n",
            "Epoch: 19\tBatch: 2501\tAvg-Loss: 0.0166\n",
            "Epoch: 19\tBatch: 2601\tAvg-Loss: 0.0167\n",
            "Epoch: 19\tBatch: 2701\tAvg-Loss: 0.0165\n",
            "Epoch: 19\tBatch: 2801\tAvg-Loss: 0.0165\n",
            "Epoch: 19\tBatch: 2901\tAvg-Loss: 0.0162\n",
            "Epoch: 19\tBatch: 3001\tAvg-Loss: 0.0159\n",
            "Epoch: 19\tBatch: 3101\tAvg-Loss: 0.0156\n",
            "Epoch: 19\tBatch: 3201\tAvg-Loss: 0.0153\n",
            "Epoch: 19\tBatch: 3301\tAvg-Loss: 0.0151\n",
            "Epoch: 19\tBatch: 3401\tAvg-Loss: 0.0149\n",
            "Epoch: 19\tBatch: 3501\tAvg-Loss: 0.0146\n",
            "Epoch: 19\tBatch: 3601\tAvg-Loss: 0.0147\n",
            "Epoch: 19\tBatch: 3701\tAvg-Loss: 0.0147\n",
            "Epoch: 19\tBatch: 3801\tAvg-Loss: 0.0146\n",
            "Epoch: 19\tBatch: 3901\tAvg-Loss: 0.0145\n",
            "Epoch: 19\tBatch: 4001\tAvg-Loss: 0.0142\n",
            "Epoch: 19\tBatch: 4101\tAvg-Loss: 0.0141\n",
            "Epoch: 19\tBatch: 4201\tAvg-Loss: 0.0143\n",
            "Epoch: 19\tBatch: 4301\tAvg-Loss: 0.0142\n",
            "Epoch: 19\tBatch: 4401\tAvg-Loss: 0.0143\n",
            "Epoch: 19\tBatch: 4501\tAvg-Loss: 0.0142\n",
            "Epoch: 19\tBatch: 4601\tAvg-Loss: 0.0142\n",
            "Epoch: 19\tBatch: 4701\tAvg-Loss: 0.0141\n",
            "Epoch: 19\tBatch: 4801\tAvg-Loss: 0.0142\n",
            "Epoch: 19\tBatch: 4901\tAvg-Loss: 0.0143\n",
            "Epoch: 19\tBatch: 5001\tAvg-Loss: 0.0143\n",
            "Epoch: 19\tBatch: 5101\tAvg-Loss: 0.0143\n",
            "Epoch: 19\tBatch: 5201\tAvg-Loss: 0.0142\n",
            "Epoch: 19\tBatch: 5301\tAvg-Loss: 0.0141\n",
            "Epoch: 19\tBatch: 5401\tAvg-Loss: 0.0139\n",
            "Epoch: 19\tBatch: 5501\tAvg-Loss: 0.0139\n",
            "Epoch: 19\tBatch: 5601\tAvg-Loss: 0.0138\n",
            "Epoch: 19\tBatch: 5701\tAvg-Loss: 0.0139\n",
            "Epoch: 19\tBatch: 5801\tAvg-Loss: 0.0140\n",
            "Epoch: 19\tBatch: 5901\tAvg-Loss: 0.0143\n",
            "Epoch: 19\tBatch: 6001\tAvg-Loss: 0.0143\n",
            "Epoch: 19\tBatch: 6101\tAvg-Loss: 0.0144\n",
            "Epoch: 19\tBatch: 6201\tAvg-Loss: 0.0146\n",
            "Epoch: 19\t Training Accuracy: 99.5340\t Validation Accuracy: 83.3100\t Avg-Loss: 0.0147\n",
            "Epoch: 20\tBatch: 1\tAvg-Loss: 0.0025\n",
            "Epoch: 20\tBatch: 101\tAvg-Loss: 0.0120\n",
            "Epoch: 20\tBatch: 201\tAvg-Loss: 0.0122\n",
            "Epoch: 20\tBatch: 301\tAvg-Loss: 0.0133\n",
            "Epoch: 20\tBatch: 401\tAvg-Loss: 0.0151\n",
            "Epoch: 20\tBatch: 501\tAvg-Loss: 0.0152\n",
            "Epoch: 20\tBatch: 601\tAvg-Loss: 0.0145\n",
            "Epoch: 20\tBatch: 701\tAvg-Loss: 0.0156\n",
            "Epoch: 20\tBatch: 801\tAvg-Loss: 0.0146\n",
            "Epoch: 20\tBatch: 901\tAvg-Loss: 0.0141\n",
            "Epoch: 20\tBatch: 1001\tAvg-Loss: 0.0136\n",
            "Epoch: 20\tBatch: 1101\tAvg-Loss: 0.0134\n",
            "Epoch: 20\tBatch: 1201\tAvg-Loss: 0.0127\n",
            "Epoch: 20\tBatch: 1301\tAvg-Loss: 0.0124\n",
            "Epoch: 20\tBatch: 1401\tAvg-Loss: 0.0129\n",
            "Epoch: 20\tBatch: 1501\tAvg-Loss: 0.0130\n",
            "Epoch: 20\tBatch: 1601\tAvg-Loss: 0.0136\n",
            "Epoch: 20\tBatch: 1701\tAvg-Loss: 0.0150\n",
            "Epoch: 20\tBatch: 1801\tAvg-Loss: 0.0150\n",
            "Epoch: 20\tBatch: 1901\tAvg-Loss: 0.0144\n",
            "Epoch: 20\tBatch: 2001\tAvg-Loss: 0.0144\n",
            "Epoch: 20\tBatch: 2101\tAvg-Loss: 0.0143\n",
            "Epoch: 20\tBatch: 2201\tAvg-Loss: 0.0141\n",
            "Epoch: 20\tBatch: 2301\tAvg-Loss: 0.0146\n",
            "Epoch: 20\tBatch: 2401\tAvg-Loss: 0.0145\n",
            "Epoch: 20\tBatch: 2501\tAvg-Loss: 0.0142\n",
            "Epoch: 20\tBatch: 2601\tAvg-Loss: 0.0146\n",
            "Epoch: 20\tBatch: 2701\tAvg-Loss: 0.0145\n",
            "Epoch: 20\tBatch: 2801\tAvg-Loss: 0.0143\n",
            "Epoch: 20\tBatch: 2901\tAvg-Loss: 0.0142\n",
            "Epoch: 20\tBatch: 3001\tAvg-Loss: 0.0143\n",
            "Epoch: 20\tBatch: 3101\tAvg-Loss: 0.0140\n",
            "Epoch: 20\tBatch: 3201\tAvg-Loss: 0.0142\n",
            "Epoch: 20\tBatch: 3301\tAvg-Loss: 0.0142\n",
            "Epoch: 20\tBatch: 3401\tAvg-Loss: 0.0143\n",
            "Epoch: 20\tBatch: 3501\tAvg-Loss: 0.0144\n",
            "Epoch: 20\tBatch: 3601\tAvg-Loss: 0.0147\n",
            "Epoch: 20\tBatch: 3701\tAvg-Loss: 0.0150\n",
            "Epoch: 20\tBatch: 3801\tAvg-Loss: 0.0150\n",
            "Epoch: 20\tBatch: 3901\tAvg-Loss: 0.0149\n",
            "Epoch: 20\tBatch: 4001\tAvg-Loss: 0.0150\n",
            "Epoch: 20\tBatch: 4101\tAvg-Loss: 0.0150\n",
            "Epoch: 20\tBatch: 4201\tAvg-Loss: 0.0148\n",
            "Epoch: 20\tBatch: 4301\tAvg-Loss: 0.0147\n",
            "Epoch: 20\tBatch: 4401\tAvg-Loss: 0.0146\n",
            "Epoch: 20\tBatch: 4501\tAvg-Loss: 0.0147\n",
            "Epoch: 20\tBatch: 4601\tAvg-Loss: 0.0147\n",
            "Epoch: 20\tBatch: 4701\tAvg-Loss: 0.0147\n",
            "Epoch: 20\tBatch: 4801\tAvg-Loss: 0.0147\n",
            "Epoch: 20\tBatch: 4901\tAvg-Loss: 0.0146\n",
            "Epoch: 20\tBatch: 5001\tAvg-Loss: 0.0145\n",
            "Epoch: 20\tBatch: 5101\tAvg-Loss: 0.0146\n",
            "Epoch: 20\tBatch: 5201\tAvg-Loss: 0.0145\n",
            "Epoch: 20\tBatch: 5301\tAvg-Loss: 0.0144\n",
            "Epoch: 20\tBatch: 5401\tAvg-Loss: 0.0144\n",
            "Epoch: 20\tBatch: 5501\tAvg-Loss: 0.0143\n",
            "Epoch: 20\tBatch: 5601\tAvg-Loss: 0.0142\n",
            "Epoch: 20\tBatch: 5701\tAvg-Loss: 0.0142\n",
            "Epoch: 20\tBatch: 5801\tAvg-Loss: 0.0141\n",
            "Epoch: 20\tBatch: 5901\tAvg-Loss: 0.0142\n",
            "Epoch: 20\tBatch: 6001\tAvg-Loss: 0.0142\n",
            "Epoch: 20\tBatch: 6101\tAvg-Loss: 0.0142\n",
            "Epoch: 20\tBatch: 6201\tAvg-Loss: 0.0142\n",
            "Epoch: 20\t Training Accuracy: 99.5520\t Validation Accuracy: 83.0000\t Avg-Loss: 0.0144\n",
            "Epoch: 21\tBatch: 1\tAvg-Loss: 0.0006\n",
            "Epoch: 21\tBatch: 101\tAvg-Loss: 0.0164\n",
            "Epoch: 21\tBatch: 201\tAvg-Loss: 0.0127\n",
            "Epoch: 21\tBatch: 301\tAvg-Loss: 0.0135\n",
            "Epoch: 21\tBatch: 401\tAvg-Loss: 0.0114\n",
            "Epoch: 21\tBatch: 501\tAvg-Loss: 0.0104\n",
            "Epoch: 21\tBatch: 601\tAvg-Loss: 0.0119\n",
            "Epoch: 21\tBatch: 701\tAvg-Loss: 0.0130\n",
            "Epoch: 21\tBatch: 801\tAvg-Loss: 0.0125\n",
            "Epoch: 21\tBatch: 901\tAvg-Loss: 0.0117\n",
            "Epoch: 21\tBatch: 1001\tAvg-Loss: 0.0114\n",
            "Epoch: 21\tBatch: 1101\tAvg-Loss: 0.0109\n",
            "Epoch: 21\tBatch: 1201\tAvg-Loss: 0.0104\n",
            "Epoch: 21\tBatch: 1301\tAvg-Loss: 0.0106\n",
            "Epoch: 21\tBatch: 1401\tAvg-Loss: 0.0108\n",
            "Epoch: 21\tBatch: 1501\tAvg-Loss: 0.0113\n",
            "Epoch: 21\tBatch: 1601\tAvg-Loss: 0.0111\n",
            "Epoch: 21\tBatch: 1701\tAvg-Loss: 0.0126\n",
            "Epoch: 21\tBatch: 1801\tAvg-Loss: 0.0136\n",
            "Epoch: 21\tBatch: 1901\tAvg-Loss: 0.0140\n",
            "Epoch: 21\tBatch: 2001\tAvg-Loss: 0.0138\n",
            "Epoch: 21\tBatch: 2101\tAvg-Loss: 0.0136\n",
            "Epoch: 21\tBatch: 2201\tAvg-Loss: 0.0134\n",
            "Epoch: 21\tBatch: 2301\tAvg-Loss: 0.0141\n",
            "Epoch: 21\tBatch: 2401\tAvg-Loss: 0.0138\n",
            "Epoch: 21\tBatch: 2501\tAvg-Loss: 0.0144\n",
            "Epoch: 21\tBatch: 2601\tAvg-Loss: 0.0149\n",
            "Epoch: 21\tBatch: 2701\tAvg-Loss: 0.0149\n",
            "Epoch: 21\tBatch: 2801\tAvg-Loss: 0.0152\n",
            "Epoch: 21\tBatch: 2901\tAvg-Loss: 0.0149\n",
            "Epoch: 21\tBatch: 3001\tAvg-Loss: 0.0146\n",
            "Epoch: 21\tBatch: 3101\tAvg-Loss: 0.0146\n",
            "Epoch: 21\tBatch: 3201\tAvg-Loss: 0.0145\n",
            "Epoch: 21\tBatch: 3301\tAvg-Loss: 0.0145\n",
            "Epoch: 21\tBatch: 3401\tAvg-Loss: 0.0144\n",
            "Epoch: 21\tBatch: 3501\tAvg-Loss: 0.0144\n",
            "Epoch: 21\tBatch: 3601\tAvg-Loss: 0.0143\n",
            "Epoch: 21\tBatch: 3701\tAvg-Loss: 0.0146\n",
            "Epoch: 21\tBatch: 3801\tAvg-Loss: 0.0145\n",
            "Epoch: 21\tBatch: 3901\tAvg-Loss: 0.0148\n",
            "Epoch: 21\tBatch: 4001\tAvg-Loss: 0.0148\n",
            "Epoch: 21\tBatch: 4101\tAvg-Loss: 0.0149\n",
            "Epoch: 21\tBatch: 4201\tAvg-Loss: 0.0152\n",
            "Epoch: 21\tBatch: 4301\tAvg-Loss: 0.0151\n",
            "Epoch: 21\tBatch: 4401\tAvg-Loss: 0.0154\n",
            "Epoch: 21\tBatch: 4501\tAvg-Loss: 0.0155\n",
            "Epoch: 21\tBatch: 4601\tAvg-Loss: 0.0154\n",
            "Epoch: 21\tBatch: 4701\tAvg-Loss: 0.0154\n",
            "Epoch: 21\tBatch: 4801\tAvg-Loss: 0.0154\n",
            "Epoch: 21\tBatch: 4901\tAvg-Loss: 0.0155\n",
            "Epoch: 21\tBatch: 5001\tAvg-Loss: 0.0154\n",
            "Epoch: 21\tBatch: 5101\tAvg-Loss: 0.0153\n",
            "Epoch: 21\tBatch: 5201\tAvg-Loss: 0.0151\n",
            "Epoch: 21\tBatch: 5301\tAvg-Loss: 0.0152\n",
            "Epoch: 21\tBatch: 5401\tAvg-Loss: 0.0152\n",
            "Epoch: 21\tBatch: 5501\tAvg-Loss: 0.0156\n",
            "Epoch: 21\tBatch: 5601\tAvg-Loss: 0.0155\n",
            "Epoch: 21\tBatch: 5701\tAvg-Loss: 0.0156\n",
            "Epoch: 21\tBatch: 5801\tAvg-Loss: 0.0156\n",
            "Epoch: 21\tBatch: 5901\tAvg-Loss: 0.0156\n",
            "Epoch: 21\tBatch: 6001\tAvg-Loss: 0.0154\n",
            "Epoch: 21\tBatch: 6101\tAvg-Loss: 0.0155\n",
            "Epoch: 21\tBatch: 6201\tAvg-Loss: 0.0154\n",
            "Epoch: 21\t Training Accuracy: 99.5460\t Validation Accuracy: 83.5700\t Avg-Loss: 0.0153\n",
            "Epoch    25: reducing learning rate of group 0 to 1.6000e-03.\n",
            "Epoch: 22\tBatch: 1\tAvg-Loss: 0.0014\n",
            "Epoch: 22\tBatch: 101\tAvg-Loss: 0.0232\n",
            "Epoch: 22\tBatch: 201\tAvg-Loss: 0.0151\n",
            "Epoch: 22\tBatch: 301\tAvg-Loss: 0.0133\n",
            "Epoch: 22\tBatch: 401\tAvg-Loss: 0.0134\n",
            "Epoch: 22\tBatch: 501\tAvg-Loss: 0.0123\n",
            "Epoch: 22\tBatch: 601\tAvg-Loss: 0.0125\n",
            "Epoch: 22\tBatch: 701\tAvg-Loss: 0.0115\n",
            "Epoch: 22\tBatch: 801\tAvg-Loss: 0.0107\n",
            "Epoch: 22\tBatch: 901\tAvg-Loss: 0.0114\n",
            "Epoch: 22\tBatch: 1001\tAvg-Loss: 0.0105\n",
            "Epoch: 22\tBatch: 1101\tAvg-Loss: 0.0100\n",
            "Epoch: 22\tBatch: 1201\tAvg-Loss: 0.0098\n",
            "Epoch: 22\tBatch: 1301\tAvg-Loss: 0.0093\n",
            "Epoch: 22\tBatch: 1401\tAvg-Loss: 0.0092\n",
            "Epoch: 22\tBatch: 1501\tAvg-Loss: 0.0091\n",
            "Epoch: 22\tBatch: 1601\tAvg-Loss: 0.0090\n",
            "Epoch: 22\tBatch: 1701\tAvg-Loss: 0.0089\n",
            "Epoch: 22\tBatch: 1801\tAvg-Loss: 0.0086\n",
            "Epoch: 22\tBatch: 1901\tAvg-Loss: 0.0089\n",
            "Epoch: 22\tBatch: 2001\tAvg-Loss: 0.0087\n",
            "Epoch: 22\tBatch: 2101\tAvg-Loss: 0.0085\n",
            "Epoch: 22\tBatch: 2201\tAvg-Loss: 0.0086\n",
            "Epoch: 22\tBatch: 2301\tAvg-Loss: 0.0084\n",
            "Epoch: 22\tBatch: 2401\tAvg-Loss: 0.0085\n",
            "Epoch: 22\tBatch: 2501\tAvg-Loss: 0.0083\n",
            "Epoch: 22\tBatch: 2601\tAvg-Loss: 0.0085\n",
            "Epoch: 22\tBatch: 2701\tAvg-Loss: 0.0084\n",
            "Epoch: 22\tBatch: 2801\tAvg-Loss: 0.0083\n",
            "Epoch: 22\tBatch: 2901\tAvg-Loss: 0.0082\n",
            "Epoch: 22\tBatch: 3001\tAvg-Loss: 0.0085\n",
            "Epoch: 22\tBatch: 3101\tAvg-Loss: 0.0085\n",
            "Epoch: 22\tBatch: 3201\tAvg-Loss: 0.0087\n",
            "Epoch: 22\tBatch: 3301\tAvg-Loss: 0.0086\n",
            "Epoch: 22\tBatch: 3401\tAvg-Loss: 0.0084\n",
            "Epoch: 22\tBatch: 3501\tAvg-Loss: 0.0085\n",
            "Epoch: 22\tBatch: 3601\tAvg-Loss: 0.0085\n",
            "Epoch: 22\tBatch: 3701\tAvg-Loss: 0.0085\n",
            "Epoch: 22\tBatch: 3801\tAvg-Loss: 0.0083\n",
            "Epoch: 22\tBatch: 3901\tAvg-Loss: 0.0082\n",
            "Epoch: 22\tBatch: 4001\tAvg-Loss: 0.0081\n",
            "Epoch: 22\tBatch: 4101\tAvg-Loss: 0.0081\n",
            "Epoch: 22\tBatch: 4201\tAvg-Loss: 0.0080\n",
            "Epoch: 22\tBatch: 4301\tAvg-Loss: 0.0079\n",
            "Epoch: 22\tBatch: 4401\tAvg-Loss: 0.0078\n",
            "Epoch: 22\tBatch: 4501\tAvg-Loss: 0.0079\n",
            "Epoch: 22\tBatch: 4601\tAvg-Loss: 0.0082\n",
            "Epoch: 22\tBatch: 4701\tAvg-Loss: 0.0080\n",
            "Epoch: 22\tBatch: 4801\tAvg-Loss: 0.0079\n",
            "Epoch: 22\tBatch: 4901\tAvg-Loss: 0.0078\n",
            "Epoch: 22\tBatch: 5001\tAvg-Loss: 0.0079\n",
            "Epoch: 22\tBatch: 5101\tAvg-Loss: 0.0078\n",
            "Epoch: 22\tBatch: 5201\tAvg-Loss: 0.0079\n",
            "Epoch: 22\tBatch: 5301\tAvg-Loss: 0.0081\n",
            "Epoch: 22\tBatch: 5401\tAvg-Loss: 0.0081\n",
            "Epoch: 22\tBatch: 5501\tAvg-Loss: 0.0081\n",
            "Epoch: 22\tBatch: 5601\tAvg-Loss: 0.0080\n",
            "Epoch: 22\tBatch: 5701\tAvg-Loss: 0.0079\n",
            "Epoch: 22\tBatch: 5801\tAvg-Loss: 0.0079\n",
            "Epoch: 22\tBatch: 5901\tAvg-Loss: 0.0079\n",
            "Epoch: 22\tBatch: 6001\tAvg-Loss: 0.0079\n",
            "Epoch: 22\tBatch: 6101\tAvg-Loss: 0.0078\n",
            "Epoch: 22\tBatch: 6201\tAvg-Loss: 0.0077\n",
            "Epoch: 22\t Training Accuracy: 99.7580\t Validation Accuracy: 84.8500\t Avg-Loss: 0.0077\n",
            "Epoch: 23\tBatch: 1\tAvg-Loss: 0.0003\n",
            "Epoch: 23\tBatch: 101\tAvg-Loss: 0.0035\n",
            "Epoch: 23\tBatch: 201\tAvg-Loss: 0.0035\n",
            "Epoch: 23\tBatch: 301\tAvg-Loss: 0.0043\n",
            "Epoch: 23\tBatch: 401\tAvg-Loss: 0.0059\n",
            "Epoch: 23\tBatch: 501\tAvg-Loss: 0.0051\n",
            "Epoch: 23\tBatch: 601\tAvg-Loss: 0.0044\n",
            "Epoch: 23\tBatch: 701\tAvg-Loss: 0.0053\n",
            "Epoch: 23\tBatch: 801\tAvg-Loss: 0.0049\n",
            "Epoch: 23\tBatch: 901\tAvg-Loss: 0.0050\n",
            "Epoch: 23\tBatch: 1001\tAvg-Loss: 0.0049\n",
            "Epoch: 23\tBatch: 1101\tAvg-Loss: 0.0048\n",
            "Epoch: 23\tBatch: 1201\tAvg-Loss: 0.0053\n",
            "Epoch: 23\tBatch: 1301\tAvg-Loss: 0.0051\n",
            "Epoch: 23\tBatch: 1401\tAvg-Loss: 0.0051\n",
            "Epoch: 23\tBatch: 1501\tAvg-Loss: 0.0049\n",
            "Epoch: 23\tBatch: 1601\tAvg-Loss: 0.0049\n",
            "Epoch: 23\tBatch: 1701\tAvg-Loss: 0.0047\n",
            "Epoch: 23\tBatch: 1801\tAvg-Loss: 0.0049\n",
            "Epoch: 23\tBatch: 1901\tAvg-Loss: 0.0050\n",
            "Epoch: 23\tBatch: 2001\tAvg-Loss: 0.0049\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}