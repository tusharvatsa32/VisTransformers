{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of VisionTransformer_ViT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMnTKXTUt6UHESvivX32ErC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e04cd8d466df4c65b96cdcbb19bded3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_519b4aa484e74ef18d2e273d97d1cc77",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cffea3745cc14460933da4f0f3bbbd2f",
              "IPY_MODEL_b8a257eb0b5c4772ac47062f4b06007c"
            ]
          }
        },
        "519b4aa484e74ef18d2e273d97d1cc77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cffea3745cc14460933da4f0f3bbbd2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b168826472c440e3893a08c82790c4a9",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178728960,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178728960,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58e97157ba0948409b2bbb45d6b99902"
          }
        },
        "b8a257eb0b5c4772ac47062f4b06007c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ea456986ff3e41039e535b7327ce2f83",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:02&lt;00:00, 77.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_903b07b6a1364826aa01cda352552da4"
          }
        },
        "b168826472c440e3893a08c82790c4a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58e97157ba0948409b2bbb45d6b99902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea456986ff3e41039e535b7327ce2f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "903b07b6a1364826aa01cda352552da4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tusharvatsa32/VisTransformers/blob/main/Code/Copy_of_VisionTransformer_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdip0rmcCXaJ",
        "outputId": "9d4aa774-1cbe-460c-fcf0-2d1b4a3a4b42"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 29 16:08:51 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WwmseiFJf6p"
      },
      "source": [
        "# License: BSD\n",
        "# Author: Sasank Chilamkurthy\n",
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE7XUQHHZd2c",
        "outputId": "6a71ec60-0afc-4573-a196-e6c420047091"
      },
      "source": [
        "!pip install vit-pytorch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vit-pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/54/cb/d1414f270753c605cf93fce61017425a883499977848726877136faa505c/vit_pytorch-0.16.12-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from vit-pytorch) (1.8.1+cu101)\n",
            "Collecting einops>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->vit-pytorch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->vit-pytorch) (3.7.4.3)\n",
            "Installing collected packages: einops, vit-pytorch\n",
            "Successfully installed einops-0.3.0 vit-pytorch-0.16.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ald4LiCiZtJp"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch import ViT\n",
        "\n",
        "v = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 10,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.3,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "preds = v(img) # (1, 1000)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRiKjLu_zn20"
      },
      "source": [
        "# Importing Libraries\n",
        "import torch                       # pytorch\n",
        "import torch.nn as nn              # pytorch for neural network\n",
        "import numpy as np                 # for algebric functions\n",
        "import matplotlib.pyplot as plt    # to plot graph\n",
        "import PIL\n",
        "\n",
        "\n",
        "# torch vision package\n",
        "import torchvision                 # for handling image & has CNN architecture\n",
        "from torchvision import transforms"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvDH-bVR5a4u",
        "outputId": "129e166d-2d6a-498b-c863-3a02783449f5"
      },
      "source": [
        "import PIL\n",
        "# img_size = ((224, 224)) For ResNet models\n",
        "img_size = ((256, 256)) # For ViT predefined weights\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "    transforms.RandomHorizontalFlip(p=0.3),\n",
        "    transforms.RandomVerticalFlip(p=0.3),\n",
        "    transforms.RandomRotation(10, resample=PIL.Image.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomCrop(img_size, fill=0),\n",
        "    transforms.RandomAffine(10, translate=None, scale=(0.8, 1.2), shear=None, fill=0, fillcolor=None, resample=None),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transforms_val = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1201: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
            "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzKPKosJ0YEE",
        "outputId": "2634872b-4ca7-416b-ebf3-fb04ed14d8d8"
      },
      "source": [
        "#transform --> transform the data during creation (ToTensor())\n",
        "#download  --> to download to local file\n",
        "#root      --> data storage place\n",
        "#train     --> means training data from training set \n",
        "#type(trainset)\n",
        "trainset = torchvision.datasets.CIFAR10(train=True,download=True,root= \"./cifar10/train_data\", transform=transforms_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
        "                                          shuffle=True, num_workers=8)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(train=False,download=True,root= \"./cifar10/test_data\", transform=transforms_val)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False, num_workers=8)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h58iA5UIgup",
        "outputId": "01102014-948d-4286-e2bc-5ab5040042e6"
      },
      "source": [
        "trainset.classes"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['airplane',\n",
              " 'automobile',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'deer',\n",
              " 'dog',\n",
              " 'frog',\n",
              " 'horse',\n",
              " 'ship',\n",
              " 'truck']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd-csdpy0bXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2aa68f-b1d5-491b-bac0-f345ddab5dc3"
      },
      "source": [
        "#class labels [there are 10 lables]\n",
        "#this is the order of lable of this dataset\n",
        "classes = trainset.classes\n",
        "print(classes)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmtAWU6E_XKQ"
      },
      "source": [
        "numEpochs = 100\n",
        "in_features = 3 # RGB channels\n",
        "\n",
        "learningRate = 0.03\n",
        "weightDecay = 5e-5\n",
        "\n",
        "num_classes = len(trainset.classes)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 10,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.3,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9, nesterov=True)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.3, patience=3, threshold=0.002, verbose=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3oydRQVafxI",
        "outputId": "bc6aa5c1-10c8-4be5-c596-06ff02c8dc45"
      },
      "source": [
        "model"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (to_patch_embedding): Sequential(\n",
              "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
              "    (1): Linear(in_features=3072, out_features=1024, bias=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (transformer): Transformer(\n",
              "    (layers): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (1): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.3, inplace=False)\n",
              "              (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "              (4): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (1): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.3, inplace=False)\n",
              "              (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "              (4): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (1): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.3, inplace=False)\n",
              "              (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "              (4): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (1): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.3, inplace=False)\n",
              "              (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "              (4): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (1): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.3, inplace=False)\n",
              "              (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "              (4): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (1): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Dropout(p=0.3, inplace=False)\n",
              "              (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "              (4): Dropout(p=0.3, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (to_latent): Identity()\n",
              "  (mlp_head): Sequential(\n",
              "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=1024, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YGmB-KwVkDF",
        "outputId": "bbd97ae5-635f-4158-e078-8a9db760cbad"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  (transformer): Encoder(\n",
              "    (pos_embedding): PositionEmbs(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder_layers): ModuleList(\n",
              "      (0): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): EncoderBlock(\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (query): LinearGeneral()\n",
              "          (key): LinearGeneral()\n",
              "          (value): LinearGeneral()\n",
              "          (out): LinearGeneral()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MlpBlock(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1YE5vNRbTYf"
      },
      "source": [
        "# model = models.resnet101(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1FgdQp0bU0B"
      },
      "source": [
        "for param in model.parameters():\n",
        "  print(param.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj48SsAMbQnn"
      },
      "source": [
        "# for param in model_ft.parameters():\n",
        "#     param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQmLX4NyI-iy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "e04cd8d466df4c65b96cdcbb19bded3e",
            "519b4aa484e74ef18d2e273d97d1cc77",
            "cffea3745cc14460933da4f0f3bbbd2f",
            "b8a257eb0b5c4772ac47062f4b06007c",
            "b168826472c440e3893a08c82790c4a9",
            "58e97157ba0948409b2bbb45d6b99902",
            "ea456986ff3e41039e535b7327ce2f83",
            "903b07b6a1364826aa01cda352552da4"
          ]
        },
        "outputId": "1543d5d8-99ef-4cd6-90d8-f125016b2583"
      },
      "source": [
        "model = models.resnet101(pretrained=True)\n",
        "\n",
        "# for param in model_ft.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "  \n",
        "num_ftrs = model.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "\n",
        "num_classes = len(trainset.classes)\n",
        "\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "learningRate = 0.01\n",
        "weightDecay = 5e-5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9, nesterov=True)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, threshold=0.002, verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e04cd8d466df4c65b96cdcbb19bded3e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN8NPf5CBx3E"
      },
      "source": [
        "my_acc = []\n",
        "my_loss = []"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eSMuBfeV1bq",
        "outputId": "1564df3d-2eac-4b74-ab50-183ff250d633"
      },
      "source": [
        "# Train!\n",
        "numEpochs = 100\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        del(outputs)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(trainset)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(testloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(testset)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_d3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 2.3813\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 7.5015\n",
            "Epoch: 0\t Training Accuracy: 16.3000\t Validation Accuracy: 16.6400\t Avg-Loss: 4.9104\n",
            "Epoch: 1\tBatch: 1\tAvg-Loss: 2.2053\n",
            "Epoch: 1\tBatch: 101\tAvg-Loss: 2.1176\n",
            "Epoch: 1\t Training Accuracy: 21.1780\t Validation Accuracy: 24.0600\t Avg-Loss: 2.0735\n",
            "Epoch: 2\tBatch: 1\tAvg-Loss: 2.0606\n",
            "Epoch: 2\tBatch: 101\tAvg-Loss: 1.9741\n",
            "Epoch: 2\t Training Accuracy: 26.1400\t Validation Accuracy: 28.0700\t Avg-Loss: 1.9577\n",
            "Epoch: 3\tBatch: 1\tAvg-Loss: 1.9816\n",
            "Epoch: 3\tBatch: 101\tAvg-Loss: 1.9090\n",
            "Epoch: 3\t Training Accuracy: 29.2040\t Validation Accuracy: 32.5600\t Avg-Loss: 1.8757\n",
            "Epoch: 4\tBatch: 1\tAvg-Loss: 1.8631\n",
            "Epoch: 4\tBatch: 101\tAvg-Loss: 1.8162\n",
            "Epoch: 4\t Training Accuracy: 32.5520\t Validation Accuracy: 35.6400\t Avg-Loss: 1.7867\n",
            "Epoch: 5\tBatch: 1\tAvg-Loss: 1.7688\n",
            "Epoch: 5\tBatch: 101\tAvg-Loss: 1.6953\n",
            "Epoch: 5\t Training Accuracy: 37.5220\t Validation Accuracy: 41.0800\t Avg-Loss: 1.6764\n",
            "Epoch: 6\tBatch: 1\tAvg-Loss: 1.5842\n",
            "Epoch: 6\tBatch: 101\tAvg-Loss: 1.6113\n",
            "Epoch: 6\t Training Accuracy: 41.2800\t Validation Accuracy: 45.0400\t Avg-Loss: 1.5870\n",
            "Epoch: 7\tBatch: 1\tAvg-Loss: 1.5081\n",
            "Epoch: 7\tBatch: 101\tAvg-Loss: 1.5338\n",
            "Epoch: 7\t Training Accuracy: 44.6080\t Validation Accuracy: 45.8200\t Avg-Loss: 1.5144\n",
            "Epoch: 8\tBatch: 1\tAvg-Loss: 1.4045\n",
            "Epoch: 8\tBatch: 101\tAvg-Loss: 1.4660\n",
            "Epoch: 8\t Training Accuracy: 46.9860\t Validation Accuracy: 47.9200\t Avg-Loss: 1.4617\n",
            "Epoch: 9\tBatch: 1\tAvg-Loss: 1.4463\n",
            "Epoch: 9\tBatch: 101\tAvg-Loss: 1.4228\n",
            "Epoch: 9\t Training Accuracy: 48.7280\t Validation Accuracy: 50.9700\t Avg-Loss: 1.4098\n",
            "Epoch: 10\tBatch: 1\tAvg-Loss: 1.4793\n",
            "Epoch: 10\tBatch: 101\tAvg-Loss: 1.3852\n",
            "Epoch: 10\t Training Accuracy: 50.4780\t Validation Accuracy: 51.2400\t Avg-Loss: 1.3717\n",
            "Epoch: 11\tBatch: 1\tAvg-Loss: 1.3410\n",
            "Epoch: 11\tBatch: 101\tAvg-Loss: 1.3417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rwpyS9SafwE6",
        "outputId": "8503ce9c-e46d-4098-d2d6-c7a8107a3b9e"
      },
      "source": [
        "# Train!\n",
        "numEpochs = 100\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        del(outputs)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(trainset)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(testloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(testset)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_d3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 2.2711\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 3.0298\n",
            "Epoch: 0\tBatch: 201\tAvg-Loss: 2.6981\n",
            "Epoch: 0\tBatch: 301\tAvg-Loss: 2.5748\n",
            "Epoch: 0\tBatch: 401\tAvg-Loss: 2.4844\n",
            "Epoch: 0\tBatch: 501\tAvg-Loss: 2.4158\n",
            "Epoch: 0\tBatch: 601\tAvg-Loss: 2.3639\n",
            "Epoch: 0\tBatch: 701\tAvg-Loss: 2.3241\n",
            "Epoch: 0\tBatch: 801\tAvg-Loss: 2.2968\n",
            "Epoch: 0\tBatch: 901\tAvg-Loss: 2.2637\n",
            "Epoch: 0\tBatch: 1001\tAvg-Loss: 2.2473\n",
            "Epoch: 0\tBatch: 1101\tAvg-Loss: 2.2248\n",
            "Epoch: 0\tBatch: 1201\tAvg-Loss: 2.2034\n",
            "Epoch: 0\tBatch: 1301\tAvg-Loss: 2.1900\n",
            "Epoch: 0\tBatch: 1401\tAvg-Loss: 2.1761\n",
            "Epoch: 0\tBatch: 1501\tAvg-Loss: 2.1634\n",
            "Epoch: 0\tBatch: 1601\tAvg-Loss: 2.1485\n",
            "Epoch: 0\tBatch: 1701\tAvg-Loss: 2.1341\n",
            "Epoch: 0\tBatch: 1801\tAvg-Loss: 2.1234\n",
            "Epoch: 0\tBatch: 1901\tAvg-Loss: 2.1115\n",
            "Epoch: 0\tBatch: 2001\tAvg-Loss: 2.0997\n",
            "Epoch: 0\tBatch: 2101\tAvg-Loss: 2.0867\n",
            "Epoch: 0\tBatch: 2201\tAvg-Loss: 2.0780\n",
            "Epoch: 0\tBatch: 2301\tAvg-Loss: 2.0682\n",
            "Epoch: 0\tBatch: 2401\tAvg-Loss: 2.0593\n",
            "Epoch: 0\tBatch: 2501\tAvg-Loss: 2.0514\n",
            "Epoch: 0\tBatch: 2601\tAvg-Loss: 2.0423\n",
            "Epoch: 0\tBatch: 2701\tAvg-Loss: 2.0325\n",
            "Epoch: 0\tBatch: 2801\tAvg-Loss: 2.0241\n",
            "Epoch: 0\tBatch: 2901\tAvg-Loss: 2.0171\n",
            "Epoch: 0\tBatch: 3001\tAvg-Loss: 2.0080\n",
            "Epoch: 0\tBatch: 3101\tAvg-Loss: 1.9988\n",
            "Epoch: 0\tBatch: 3201\tAvg-Loss: 1.9926\n",
            "Epoch: 0\tBatch: 3301\tAvg-Loss: 1.9837\n",
            "Epoch: 0\tBatch: 3401\tAvg-Loss: 1.9758\n",
            "Epoch: 0\tBatch: 3501\tAvg-Loss: 1.9692\n",
            "Epoch: 0\tBatch: 3601\tAvg-Loss: 1.9621\n",
            "Epoch: 0\tBatch: 3701\tAvg-Loss: 1.9562\n",
            "Epoch: 0\tBatch: 3801\tAvg-Loss: 1.9480\n",
            "Epoch: 0\tBatch: 3901\tAvg-Loss: 1.9407\n",
            "Epoch: 0\tBatch: 4001\tAvg-Loss: 1.9345\n",
            "Epoch: 0\tBatch: 4101\tAvg-Loss: 1.9290\n",
            "Epoch: 0\tBatch: 4201\tAvg-Loss: 1.9216\n",
            "Epoch: 0\tBatch: 4301\tAvg-Loss: 1.9155\n",
            "Epoch: 0\tBatch: 4401\tAvg-Loss: 1.9088\n",
            "Epoch: 0\tBatch: 4501\tAvg-Loss: 1.9032\n",
            "Epoch: 0\tBatch: 4601\tAvg-Loss: 1.8972\n",
            "Epoch: 0\tBatch: 4701\tAvg-Loss: 1.8916\n",
            "Epoch: 0\tBatch: 4801\tAvg-Loss: 1.8849\n",
            "Epoch: 0\tBatch: 4901\tAvg-Loss: 1.8796\n",
            "Epoch: 0\t Training Accuracy: 31.1980\t Validation Accuracy: 44.4100\t Avg-Loss: 1.8755\n",
            "Epoch: 1\tBatch: 1\tAvg-Loss: 1.4226\n",
            "Epoch: 1\tBatch: 101\tAvg-Loss: 1.6077\n",
            "Epoch: 1\tBatch: 201\tAvg-Loss: 1.5635\n",
            "Epoch: 1\tBatch: 301\tAvg-Loss: 1.5605\n",
            "Epoch: 1\tBatch: 401\tAvg-Loss: 1.5589\n",
            "Epoch: 1\tBatch: 501\tAvg-Loss: 1.5496\n",
            "Epoch: 1\tBatch: 601\tAvg-Loss: 1.5453\n",
            "Epoch: 1\tBatch: 701\tAvg-Loss: 1.5483\n",
            "Epoch: 1\tBatch: 801\tAvg-Loss: 1.5513\n",
            "Epoch: 1\tBatch: 901\tAvg-Loss: 1.5520\n",
            "Epoch: 1\tBatch: 1001\tAvg-Loss: 1.5526\n",
            "Epoch: 1\tBatch: 1101\tAvg-Loss: 1.5484\n",
            "Epoch: 1\tBatch: 1201\tAvg-Loss: 1.5466\n",
            "Epoch: 1\tBatch: 1301\tAvg-Loss: 1.5479\n",
            "Epoch: 1\tBatch: 1401\tAvg-Loss: 1.5444\n",
            "Epoch: 1\tBatch: 1501\tAvg-Loss: 1.5451\n",
            "Epoch: 1\tBatch: 1601\tAvg-Loss: 1.5439\n",
            "Epoch: 1\tBatch: 1701\tAvg-Loss: 1.5461\n",
            "Epoch: 1\tBatch: 1801\tAvg-Loss: 1.5439\n",
            "Epoch: 1\tBatch: 1901\tAvg-Loss: 1.5429\n",
            "Epoch: 1\tBatch: 2001\tAvg-Loss: 1.5405\n",
            "Epoch: 1\tBatch: 2101\tAvg-Loss: 1.5385\n",
            "Epoch: 1\tBatch: 2201\tAvg-Loss: 1.5333\n",
            "Epoch: 1\tBatch: 2301\tAvg-Loss: 1.5328\n",
            "Epoch: 1\tBatch: 2401\tAvg-Loss: 1.5305\n",
            "Epoch: 1\tBatch: 2501\tAvg-Loss: 1.5272\n",
            "Epoch: 1\tBatch: 2601\tAvg-Loss: 1.5279\n",
            "Epoch: 1\tBatch: 2701\tAvg-Loss: 1.5241\n",
            "Epoch: 1\tBatch: 2801\tAvg-Loss: 1.5215\n",
            "Epoch: 1\tBatch: 2901\tAvg-Loss: 1.5200\n",
            "Epoch: 1\tBatch: 3001\tAvg-Loss: 1.5169\n",
            "Epoch: 1\tBatch: 3101\tAvg-Loss: 1.5153\n",
            "Epoch: 1\tBatch: 3201\tAvg-Loss: 1.5147\n",
            "Epoch: 1\tBatch: 3301\tAvg-Loss: 1.5116\n",
            "Epoch: 1\tBatch: 3401\tAvg-Loss: 1.5097\n",
            "Epoch: 1\tBatch: 3501\tAvg-Loss: 1.5085\n",
            "Epoch: 1\tBatch: 3601\tAvg-Loss: 1.5051\n",
            "Epoch: 1\tBatch: 3701\tAvg-Loss: 1.5031\n",
            "Epoch: 1\tBatch: 3801\tAvg-Loss: 1.5019\n",
            "Epoch: 1\tBatch: 3901\tAvg-Loss: 1.4990\n",
            "Epoch: 1\tBatch: 4001\tAvg-Loss: 1.4962\n",
            "Epoch: 1\tBatch: 4101\tAvg-Loss: 1.4947\n",
            "Epoch: 1\tBatch: 4201\tAvg-Loss: 1.4931\n",
            "Epoch: 1\tBatch: 4301\tAvg-Loss: 1.4892\n",
            "Epoch: 1\tBatch: 4401\tAvg-Loss: 1.4879\n",
            "Epoch: 1\tBatch: 4501\tAvg-Loss: 1.4875\n",
            "Epoch: 1\tBatch: 4601\tAvg-Loss: 1.4864\n",
            "Epoch: 1\tBatch: 4701\tAvg-Loss: 1.4833\n",
            "Epoch: 1\tBatch: 4801\tAvg-Loss: 1.4808\n",
            "Epoch: 1\tBatch: 4901\tAvg-Loss: 1.4801\n",
            "Epoch: 1\t Training Accuracy: 46.4740\t Validation Accuracy: 53.1600\t Avg-Loss: 1.4768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-979cc71a4a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/vision_transformer_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m# classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/vision_transformer_pytorch/model.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/vision_transformer_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/vision_transformer_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/vision_transformer_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 154.00 MiB (GPU 0; 15.78 GiB total capacity; 11.27 GiB already allocated; 60.75 MiB free; 14.42 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VyjG6R7OJMrL",
        "outputId": "74fd8586-605e-4264-97e2-b9d7492f0595"
      },
      "source": [
        "# Train!\n",
        "numEpochs = 100\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        del(outputs)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(trainset)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(testloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(testset)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_d3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 4.5812\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 2.9234\n",
            "Epoch: 0\tBatch: 201\tAvg-Loss: 2.3388\n",
            "Epoch: 0\tBatch: 301\tAvg-Loss: 2.0906\n",
            "Epoch: 0\tBatch: 401\tAvg-Loss: 1.9510\n",
            "Epoch: 0\tBatch: 501\tAvg-Loss: 1.8462\n",
            "Epoch: 0\tBatch: 601\tAvg-Loss: 1.7737\n",
            "Epoch: 0\tBatch: 701\tAvg-Loss: 1.7129\n",
            "Epoch: 0\t Training Accuracy: 53.8980\t Validation Accuracy: 66.3800\t Avg-Loss: 1.6778\n",
            "Epoch: 1\tBatch: 1\tAvg-Loss: 1.2406\n",
            "Epoch: 1\tBatch: 101\tAvg-Loss: 1.2045\n",
            "Epoch: 1\tBatch: 201\tAvg-Loss: 1.1665\n",
            "Epoch: 1\tBatch: 301\tAvg-Loss: 1.1613\n",
            "Epoch: 1\tBatch: 401\tAvg-Loss: 1.1522\n",
            "Epoch: 1\tBatch: 501\tAvg-Loss: 1.1423\n",
            "Epoch: 1\tBatch: 601\tAvg-Loss: 1.1416\n",
            "Epoch: 1\tBatch: 701\tAvg-Loss: 1.1299\n",
            "Epoch: 1\t Training Accuracy: 67.1060\t Validation Accuracy: 69.4100\t Avg-Loss: 1.1280\n",
            "Epoch: 2\tBatch: 1\tAvg-Loss: 1.3802\n",
            "Epoch: 2\tBatch: 101\tAvg-Loss: 0.9010\n",
            "Epoch: 2\tBatch: 201\tAvg-Loss: 0.9174\n",
            "Epoch: 2\tBatch: 301\tAvg-Loss: 0.9298\n",
            "Epoch: 2\tBatch: 401\tAvg-Loss: 0.9250\n",
            "Epoch: 2\tBatch: 501\tAvg-Loss: 0.9297\n",
            "Epoch: 2\tBatch: 601\tAvg-Loss: 0.9342\n",
            "Epoch: 2\tBatch: 701\tAvg-Loss: 0.9342\n",
            "Epoch: 2\t Training Accuracy: 72.2660\t Validation Accuracy: 73.9700\t Avg-Loss: 0.9361\n",
            "Epoch: 3\tBatch: 1\tAvg-Loss: 0.6415\n",
            "Epoch: 3\tBatch: 101\tAvg-Loss: 0.7962\n",
            "Epoch: 3\tBatch: 201\tAvg-Loss: 0.8090\n",
            "Epoch: 3\tBatch: 301\tAvg-Loss: 0.8086\n",
            "Epoch: 3\tBatch: 401\tAvg-Loss: 0.8064\n",
            "Epoch: 3\tBatch: 501\tAvg-Loss: 0.8107\n",
            "Epoch: 3\tBatch: 601\tAvg-Loss: 0.8081\n",
            "Epoch: 3\tBatch: 701\tAvg-Loss: 0.8092\n",
            "Epoch: 3\t Training Accuracy: 75.2040\t Validation Accuracy: 74.9800\t Avg-Loss: 0.8113\n",
            "Epoch: 4\tBatch: 1\tAvg-Loss: 0.5524\n",
            "Epoch: 4\tBatch: 101\tAvg-Loss: 0.6931\n",
            "Epoch: 4\tBatch: 201\tAvg-Loss: 0.7017\n",
            "Epoch: 4\tBatch: 301\tAvg-Loss: 0.7009\n",
            "Epoch: 4\tBatch: 401\tAvg-Loss: 0.7054\n",
            "Epoch: 4\tBatch: 501\tAvg-Loss: 0.7049\n",
            "Epoch: 4\tBatch: 601\tAvg-Loss: 0.7120\n",
            "Epoch: 4\tBatch: 701\tAvg-Loss: 0.7183\n",
            "Epoch: 4\t Training Accuracy: 77.7620\t Validation Accuracy: 76.3400\t Avg-Loss: 0.7217\n",
            "Epoch: 5\tBatch: 1\tAvg-Loss: 0.3384\n",
            "Epoch: 5\tBatch: 101\tAvg-Loss: 0.6127\n",
            "Epoch: 5\tBatch: 201\tAvg-Loss: 0.6099\n",
            "Epoch: 5\tBatch: 301\tAvg-Loss: 0.6051\n",
            "Epoch: 5\tBatch: 401\tAvg-Loss: 0.6116\n",
            "Epoch: 5\tBatch: 501\tAvg-Loss: 0.6184\n",
            "Epoch: 5\tBatch: 601\tAvg-Loss: 0.6216\n",
            "Epoch: 5\tBatch: 701\tAvg-Loss: 0.6251\n",
            "Epoch: 5\t Training Accuracy: 80.4300\t Validation Accuracy: 75.0900\t Avg-Loss: 0.6308\n",
            "Epoch: 6\tBatch: 1\tAvg-Loss: 0.5272\n",
            "Epoch: 6\tBatch: 101\tAvg-Loss: 0.5582\n",
            "Epoch: 6\tBatch: 201\tAvg-Loss: 0.5476\n",
            "Epoch: 6\tBatch: 301\tAvg-Loss: 0.5588\n",
            "Epoch: 6\tBatch: 401\tAvg-Loss: 0.5639\n",
            "Epoch: 6\tBatch: 501\tAvg-Loss: 0.5678\n",
            "Epoch: 6\tBatch: 601\tAvg-Loss: 0.5778\n",
            "Epoch: 6\tBatch: 701\tAvg-Loss: 0.5817\n",
            "Epoch: 6\t Training Accuracy: 81.6520\t Validation Accuracy: 76.3200\t Avg-Loss: 0.5839\n",
            "Epoch: 7\tBatch: 1\tAvg-Loss: 0.3669\n",
            "Epoch: 7\tBatch: 101\tAvg-Loss: 0.4625\n",
            "Epoch: 7\tBatch: 201\tAvg-Loss: 0.4769\n",
            "Epoch: 7\tBatch: 301\tAvg-Loss: 0.4932\n",
            "Epoch: 7\tBatch: 401\tAvg-Loss: 0.5029\n",
            "Epoch: 7\tBatch: 501\tAvg-Loss: 0.5038\n",
            "Epoch: 7\tBatch: 601\tAvg-Loss: 0.5077\n",
            "Epoch: 7\tBatch: 701\tAvg-Loss: 0.5103\n",
            "Epoch: 7\t Training Accuracy: 83.7660\t Validation Accuracy: 76.9600\t Avg-Loss: 0.5177\n",
            "Epoch: 8\tBatch: 1\tAvg-Loss: 0.4070\n",
            "Epoch: 8\tBatch: 101\tAvg-Loss: 0.4706\n",
            "Epoch: 8\tBatch: 201\tAvg-Loss: 0.4612\n",
            "Epoch: 8\tBatch: 301\tAvg-Loss: 0.4519\n",
            "Epoch: 8\tBatch: 401\tAvg-Loss: 0.4557\n",
            "Epoch: 8\tBatch: 501\tAvg-Loss: 0.4622\n",
            "Epoch: 8\tBatch: 601\tAvg-Loss: 0.4709\n",
            "Epoch: 8\tBatch: 701\tAvg-Loss: 0.4780\n",
            "Epoch: 8\t Training Accuracy: 84.8440\t Validation Accuracy: 77.7200\t Avg-Loss: 0.4789\n",
            "Epoch: 9\tBatch: 1\tAvg-Loss: 0.3856\n",
            "Epoch: 9\tBatch: 101\tAvg-Loss: 0.4123\n",
            "Epoch: 9\tBatch: 201\tAvg-Loss: 0.4016\n",
            "Epoch: 9\tBatch: 301\tAvg-Loss: 0.4088\n",
            "Epoch: 9\tBatch: 401\tAvg-Loss: 0.4150\n",
            "Epoch: 9\tBatch: 501\tAvg-Loss: 0.4249\n",
            "Epoch: 9\tBatch: 601\tAvg-Loss: 0.4309\n",
            "Epoch: 9\tBatch: 701\tAvg-Loss: 0.4378\n",
            "Epoch: 9\t Training Accuracy: 85.8880\t Validation Accuracy: 76.7300\t Avg-Loss: 0.4403\n",
            "Epoch: 10\tBatch: 1\tAvg-Loss: 0.2623\n",
            "Epoch: 10\tBatch: 101\tAvg-Loss: 0.3764\n",
            "Epoch: 10\tBatch: 201\tAvg-Loss: 0.3761\n",
            "Epoch: 10\tBatch: 301\tAvg-Loss: 0.3773\n",
            "Epoch: 10\tBatch: 401\tAvg-Loss: 0.3834\n",
            "Epoch: 10\tBatch: 501\tAvg-Loss: 0.3871\n",
            "Epoch: 10\tBatch: 601\tAvg-Loss: 0.3920\n",
            "Epoch: 10\tBatch: 701\tAvg-Loss: 0.3981\n",
            "Epoch: 10\t Training Accuracy: 87.3180\t Validation Accuracy: 77.7100\t Avg-Loss: 0.4017\n",
            "Epoch: 11\tBatch: 1\tAvg-Loss: 0.3468\n",
            "Epoch: 11\tBatch: 101\tAvg-Loss: 0.3456\n",
            "Epoch: 11\tBatch: 201\tAvg-Loss: 0.3350\n",
            "Epoch: 11\tBatch: 301\tAvg-Loss: 0.3368\n",
            "Epoch: 11\tBatch: 401\tAvg-Loss: 0.3504\n",
            "Epoch: 11\tBatch: 501\tAvg-Loss: 0.3593\n",
            "Epoch: 11\tBatch: 601\tAvg-Loss: 0.3641\n",
            "Epoch: 11\tBatch: 701\tAvg-Loss: 0.3677\n",
            "Epoch: 11\t Training Accuracy: 87.9780\t Validation Accuracy: 79.5100\t Avg-Loss: 0.3710\n",
            "Epoch: 12\tBatch: 1\tAvg-Loss: 0.2619\n",
            "Epoch: 12\tBatch: 101\tAvg-Loss: 0.3216\n",
            "Epoch: 12\tBatch: 201\tAvg-Loss: 0.3217\n",
            "Epoch: 12\tBatch: 301\tAvg-Loss: 0.3206\n",
            "Epoch: 12\tBatch: 401\tAvg-Loss: 0.3252\n",
            "Epoch: 12\tBatch: 501\tAvg-Loss: 0.3253\n",
            "Epoch: 12\tBatch: 601\tAvg-Loss: 0.3302\n",
            "Epoch: 12\tBatch: 701\tAvg-Loss: 0.3358\n",
            "Epoch: 12\t Training Accuracy: 89.0160\t Validation Accuracy: 79.9700\t Avg-Loss: 0.3379\n",
            "Epoch: 13\tBatch: 1\tAvg-Loss: 0.2849\n",
            "Epoch: 13\tBatch: 101\tAvg-Loss: 0.2682\n",
            "Epoch: 13\tBatch: 201\tAvg-Loss: 0.2778\n",
            "Epoch: 13\tBatch: 301\tAvg-Loss: 0.2817\n",
            "Epoch: 13\tBatch: 401\tAvg-Loss: 0.2918\n",
            "Epoch: 13\tBatch: 501\tAvg-Loss: 0.2958\n",
            "Epoch: 13\tBatch: 601\tAvg-Loss: 0.3023\n",
            "Epoch: 13\tBatch: 701\tAvg-Loss: 0.3033\n",
            "Epoch: 13\t Training Accuracy: 90.0220\t Validation Accuracy: 80.1400\t Avg-Loss: 0.3087\n",
            "Epoch: 14\tBatch: 1\tAvg-Loss: 0.1805\n",
            "Epoch: 14\tBatch: 101\tAvg-Loss: 0.2834\n",
            "Epoch: 14\tBatch: 201\tAvg-Loss: 0.2813\n",
            "Epoch: 14\tBatch: 301\tAvg-Loss: 0.2805\n",
            "Epoch: 14\tBatch: 401\tAvg-Loss: 0.2849\n",
            "Epoch: 14\tBatch: 501\tAvg-Loss: 0.2873\n",
            "Epoch: 14\tBatch: 601\tAvg-Loss: 0.2888\n",
            "Epoch: 14\tBatch: 701\tAvg-Loss: 0.2907\n",
            "Epoch: 14\t Training Accuracy: 90.4360\t Validation Accuracy: 79.3500\t Avg-Loss: 0.2961\n",
            "Epoch: 15\tBatch: 1\tAvg-Loss: 0.2616\n",
            "Epoch: 15\tBatch: 101\tAvg-Loss: 0.2723\n",
            "Epoch: 15\tBatch: 201\tAvg-Loss: 0.2615\n",
            "Epoch: 15\tBatch: 301\tAvg-Loss: 0.2586\n",
            "Epoch: 15\tBatch: 401\tAvg-Loss: 0.2558\n",
            "Epoch: 15\tBatch: 501\tAvg-Loss: 0.2622\n",
            "Epoch: 15\tBatch: 601\tAvg-Loss: 0.2633\n",
            "Epoch: 15\tBatch: 701\tAvg-Loss: 0.2665\n",
            "Epoch: 15\t Training Accuracy: 91.2460\t Validation Accuracy: 80.9700\t Avg-Loss: 0.2688\n",
            "Epoch: 16\tBatch: 1\tAvg-Loss: 0.1720\n",
            "Epoch: 16\tBatch: 101\tAvg-Loss: 0.2357\n",
            "Epoch: 16\tBatch: 201\tAvg-Loss: 0.2421\n",
            "Epoch: 16\tBatch: 301\tAvg-Loss: 0.2378\n",
            "Epoch: 16\tBatch: 401\tAvg-Loss: 0.2367\n",
            "Epoch: 16\tBatch: 501\tAvg-Loss: 0.2406\n",
            "Epoch: 16\tBatch: 601\tAvg-Loss: 0.2459\n",
            "Epoch: 16\tBatch: 701\tAvg-Loss: 0.2482\n",
            "Epoch: 16\t Training Accuracy: 91.8180\t Validation Accuracy: 78.6900\t Avg-Loss: 0.2503\n",
            "Epoch: 17\tBatch: 1\tAvg-Loss: 0.2797\n",
            "Epoch: 17\tBatch: 101\tAvg-Loss: 0.2294\n",
            "Epoch: 17\tBatch: 201\tAvg-Loss: 0.2250\n",
            "Epoch: 17\tBatch: 301\tAvg-Loss: 0.2297\n",
            "Epoch: 17\tBatch: 401\tAvg-Loss: 0.2299\n",
            "Epoch: 17\tBatch: 501\tAvg-Loss: 0.2332\n",
            "Epoch: 17\tBatch: 601\tAvg-Loss: 0.2368\n",
            "Epoch: 17\tBatch: 701\tAvg-Loss: 0.2411\n",
            "Epoch: 17\t Training Accuracy: 92.0700\t Validation Accuracy: 80.6200\t Avg-Loss: 0.2446\n",
            "Epoch: 18\tBatch: 1\tAvg-Loss: 0.1604\n",
            "Epoch: 18\tBatch: 101\tAvg-Loss: 0.2031\n",
            "Epoch: 18\tBatch: 201\tAvg-Loss: 0.2035\n",
            "Epoch: 18\tBatch: 301\tAvg-Loss: 0.2010\n",
            "Epoch: 18\tBatch: 401\tAvg-Loss: 0.2019\n",
            "Epoch: 18\tBatch: 501\tAvg-Loss: 0.2088\n",
            "Epoch: 18\tBatch: 601\tAvg-Loss: 0.2110\n",
            "Epoch: 18\tBatch: 701\tAvg-Loss: 0.2145\n",
            "Epoch: 18\t Training Accuracy: 92.8960\t Validation Accuracy: 80.1000\t Avg-Loss: 0.2173\n",
            "Epoch: 19\tBatch: 1\tAvg-Loss: 0.1220\n",
            "Epoch: 19\tBatch: 101\tAvg-Loss: 0.1956\n",
            "Epoch: 19\tBatch: 201\tAvg-Loss: 0.1938\n",
            "Epoch: 19\tBatch: 301\tAvg-Loss: 0.1924\n",
            "Epoch: 19\tBatch: 401\tAvg-Loss: 0.1928\n",
            "Epoch: 19\tBatch: 501\tAvg-Loss: 0.1976\n",
            "Epoch: 19\tBatch: 601\tAvg-Loss: 0.1992\n",
            "Epoch: 19\tBatch: 701\tAvg-Loss: 0.2030\n",
            "Epoch: 19\t Training Accuracy: 93.2780\t Validation Accuracy: 79.3600\t Avg-Loss: 0.2074\n",
            "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Epoch: 20\tBatch: 1\tAvg-Loss: 0.0803\n",
            "Epoch: 20\tBatch: 101\tAvg-Loss: 0.1514\n",
            "Epoch: 20\tBatch: 201\tAvg-Loss: 0.1369\n",
            "Epoch: 20\tBatch: 301\tAvg-Loss: 0.1304\n",
            "Epoch: 20\tBatch: 401\tAvg-Loss: 0.1224\n",
            "Epoch: 20\tBatch: 501\tAvg-Loss: 0.1178\n",
            "Epoch: 20\tBatch: 601\tAvg-Loss: 0.1130\n",
            "Epoch: 20\tBatch: 701\tAvg-Loss: 0.1095\n",
            "Epoch: 20\t Training Accuracy: 96.7380\t Validation Accuracy: 84.0600\t Avg-Loss: 0.1070\n",
            "Epoch: 21\tBatch: 1\tAvg-Loss: 0.0660\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-979cc71a4a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                   \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                   nesterov)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# update momentum_buffers in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc7KdLL6BHyj",
        "outputId": "8f23fc69-567d-4134-dfbe-2a02baef4f4e"
      },
      "source": [
        "# Train!\n",
        "numEpochs = 100\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        del(outputs)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(trainset)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(testloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(testset)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_d3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 2.3129\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 0.9381\n",
            "Epoch: 0\tBatch: 201\tAvg-Loss: 0.7681\n",
            "Epoch: 0\tBatch: 301\tAvg-Loss: 0.6827\n",
            "Epoch: 0\tBatch: 401\tAvg-Loss: 0.6277\n",
            "Epoch: 0\tBatch: 501\tAvg-Loss: 0.5897\n",
            "Epoch: 0\tBatch: 601\tAvg-Loss: 0.5605\n",
            "Epoch: 0\tBatch: 701\tAvg-Loss: 0.5337\n",
            "Epoch: 0\t Training Accuracy: 82.4640\t Validation Accuracy: 90.7100\t Avg-Loss: 0.5125\n",
            "Epoch: 1\tBatch: 1\tAvg-Loss: 0.3835\n",
            "Epoch: 1\tBatch: 101\tAvg-Loss: 0.3248\n",
            "Epoch: 1\tBatch: 201\tAvg-Loss: 0.3260\n",
            "Epoch: 1\tBatch: 301\tAvg-Loss: 0.3207\n",
            "Epoch: 1\tBatch: 401\tAvg-Loss: 0.3191\n",
            "Epoch: 1\tBatch: 501\tAvg-Loss: 0.3156\n",
            "Epoch: 1\tBatch: 601\tAvg-Loss: 0.3126\n",
            "Epoch: 1\tBatch: 701\tAvg-Loss: 0.3103\n",
            "Epoch: 1\t Training Accuracy: 89.3980\t Validation Accuracy: 90.8000\t Avg-Loss: 0.3056\n",
            "Epoch: 2\tBatch: 1\tAvg-Loss: 0.2628\n",
            "Epoch: 2\tBatch: 101\tAvg-Loss: 0.2328\n",
            "Epoch: 2\tBatch: 201\tAvg-Loss: 0.2375\n",
            "Epoch: 2\tBatch: 301\tAvg-Loss: 0.2359\n",
            "Epoch: 2\tBatch: 401\tAvg-Loss: 0.2337\n",
            "Epoch: 2\tBatch: 501\tAvg-Loss: 0.2338\n",
            "Epoch: 2\tBatch: 601\tAvg-Loss: 0.2372\n",
            "Epoch: 2\tBatch: 701\tAvg-Loss: 0.2377\n",
            "Epoch: 2\t Training Accuracy: 91.7320\t Validation Accuracy: 92.7500\t Avg-Loss: 0.2373\n",
            "Epoch: 3\tBatch: 1\tAvg-Loss: 0.2509\n",
            "Epoch: 3\tBatch: 101\tAvg-Loss: 0.1994\n",
            "Epoch: 3\tBatch: 201\tAvg-Loss: 0.1997\n",
            "Epoch: 3\tBatch: 301\tAvg-Loss: 0.2032\n",
            "Epoch: 3\tBatch: 401\tAvg-Loss: 0.2033\n",
            "Epoch: 3\tBatch: 501\tAvg-Loss: 0.2035\n",
            "Epoch: 3\tBatch: 601\tAvg-Loss: 0.2047\n",
            "Epoch: 3\tBatch: 701\tAvg-Loss: 0.2049\n",
            "Epoch: 3\t Training Accuracy: 92.9480\t Validation Accuracy: 93.8700\t Avg-Loss: 0.2022\n",
            "Epoch: 4\tBatch: 1\tAvg-Loss: 0.4133\n",
            "Epoch: 4\tBatch: 101\tAvg-Loss: 0.1790\n",
            "Epoch: 4\tBatch: 201\tAvg-Loss: 0.1755\n",
            "Epoch: 4\tBatch: 301\tAvg-Loss: 0.1774\n",
            "Epoch: 4\tBatch: 401\tAvg-Loss: 0.1766\n",
            "Epoch: 4\tBatch: 501\tAvg-Loss: 0.1770\n",
            "Epoch: 4\tBatch: 601\tAvg-Loss: 0.1760\n",
            "Epoch: 4\tBatch: 701\tAvg-Loss: 0.1750\n",
            "Epoch: 4\t Training Accuracy: 94.1080\t Validation Accuracy: 93.4200\t Avg-Loss: 0.1733\n",
            "Epoch: 5\tBatch: 1\tAvg-Loss: 0.2310\n",
            "Epoch: 5\tBatch: 101\tAvg-Loss: 0.1510\n",
            "Epoch: 5\tBatch: 201\tAvg-Loss: 0.1478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "kyX-vvG49Jki",
        "outputId": "b562d104-7ffd-4130-e95c-0f5d95403190"
      },
      "source": [
        "# Train!\n",
        "numEpochs = 100\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(trainset)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(testloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(testset)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_d3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 2.3511\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 0.9306\n",
            "Epoch: 0\tBatch: 201\tAvg-Loss: 0.7690\n",
            "Epoch: 0\tBatch: 301\tAvg-Loss: 0.6924\n",
            "Epoch: 0\tBatch: 401\tAvg-Loss: 0.6461\n",
            "Epoch: 0\tBatch: 501\tAvg-Loss: 0.6004\n",
            "Epoch: 0\tBatch: 601\tAvg-Loss: 0.5677\n",
            "Epoch: 0\tBatch: 701\tAvg-Loss: 0.5406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a8847f7aee12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 15.78 GiB total capacity; 14.15 GiB already allocated; 2.75 MiB free; 14.48 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "begqtI0NpHKI",
        "outputId": "0858c81b-e89a-420d-c3f8-5652e21793e4"
      },
      "source": [
        "# Train!\n",
        "numEpochs = 100\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(trainset)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(testloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(testset)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_d3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 2.3927\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 1.5158\n",
            "Epoch: 0\tBatch: 201\tAvg-Loss: 1.2682\n",
            "Epoch: 0\tBatch: 301\tAvg-Loss: 1.1363\n",
            "Epoch: 0\tBatch: 401\tAvg-Loss: 1.0519\n",
            "Epoch: 0\tBatch: 501\tAvg-Loss: 0.9904\n",
            "Epoch: 0\tBatch: 601\tAvg-Loss: 0.9412\n",
            "Epoch: 0\tBatch: 701\tAvg-Loss: 0.9059\n",
            "Epoch: 0\t Training Accuracy: 69.3040\t Validation Accuracy: 43.7900\t Avg-Loss: 0.8818\n",
            "Epoch: 1\tBatch: 1\tAvg-Loss: 1.0808\n",
            "Epoch: 1\tBatch: 101\tAvg-Loss: 0.9907\n",
            "Epoch: 1\tBatch: 201\tAvg-Loss: 0.8702\n",
            "Epoch: 1\tBatch: 301\tAvg-Loss: 0.8138\n",
            "Epoch: 1\tBatch: 401\tAvg-Loss: 0.7680\n",
            "Epoch: 1\tBatch: 501\tAvg-Loss: 0.7417\n",
            "Epoch: 1\tBatch: 601\tAvg-Loss: 0.7166\n",
            "Epoch: 1\tBatch: 701\tAvg-Loss: 0.7043\n",
            "Epoch: 1\t Training Accuracy: 76.0920\t Validation Accuracy: 81.1500\t Avg-Loss: 0.6935\n",
            "Epoch: 2\tBatch: 1\tAvg-Loss: 0.7420\n",
            "Epoch: 2\tBatch: 101\tAvg-Loss: 0.5578\n",
            "Epoch: 2\tBatch: 201\tAvg-Loss: 0.5364\n",
            "Epoch: 2\tBatch: 301\tAvg-Loss: 0.5347\n",
            "Epoch: 2\tBatch: 401\tAvg-Loss: 0.5313\n",
            "Epoch: 2\tBatch: 501\tAvg-Loss: 0.5248\n",
            "Epoch: 2\tBatch: 601\tAvg-Loss: 0.5215\n",
            "Epoch: 2\tBatch: 701\tAvg-Loss: 0.5184\n",
            "Epoch: 2\t Training Accuracy: 82.0640\t Validation Accuracy: 83.1500\t Avg-Loss: 0.5144\n",
            "Epoch: 3\tBatch: 1\tAvg-Loss: 0.3628\n",
            "Epoch: 3\tBatch: 101\tAvg-Loss: 0.4659\n",
            "Epoch: 3\tBatch: 201\tAvg-Loss: 0.4834\n",
            "Epoch: 3\tBatch: 301\tAvg-Loss: 0.4777\n",
            "Epoch: 3\tBatch: 401\tAvg-Loss: 0.4746\n",
            "Epoch: 3\tBatch: 501\tAvg-Loss: 0.4705\n",
            "Epoch: 3\tBatch: 601\tAvg-Loss: 0.4649\n",
            "Epoch: 3\tBatch: 701\tAvg-Loss: 0.4649\n",
            "Epoch: 3\t Training Accuracy: 83.7680\t Validation Accuracy: 86.8100\t Avg-Loss: 0.4650\n",
            "Epoch: 4\tBatch: 1\tAvg-Loss: 0.4819\n",
            "Epoch: 4\tBatch: 101\tAvg-Loss: 0.3963\n",
            "Epoch: 4\tBatch: 201\tAvg-Loss: 0.3983\n",
            "Epoch: 4\tBatch: 301\tAvg-Loss: 0.4003\n",
            "Epoch: 4\tBatch: 401\tAvg-Loss: 0.3977\n",
            "Epoch: 4\tBatch: 501\tAvg-Loss: 0.4034\n",
            "Epoch: 4\tBatch: 601\tAvg-Loss: 0.4038\n",
            "Epoch: 4\tBatch: 701\tAvg-Loss: 0.4016\n",
            "Epoch: 4\t Training Accuracy: 86.1160\t Validation Accuracy: 87.3500\t Avg-Loss: 0.4011\n",
            "Epoch: 5\tBatch: 1\tAvg-Loss: 0.2308\n",
            "Epoch: 5\tBatch: 101\tAvg-Loss: 0.3598\n",
            "Epoch: 5\tBatch: 201\tAvg-Loss: 0.3644\n",
            "Epoch: 5\tBatch: 301\tAvg-Loss: 0.3637\n",
            "Epoch: 5\tBatch: 401\tAvg-Loss: 0.3620\n",
            "Epoch: 5\tBatch: 501\tAvg-Loss: 0.3648\n",
            "Epoch: 5\tBatch: 601\tAvg-Loss: 0.3643\n",
            "Epoch: 5\tBatch: 701\tAvg-Loss: 0.3650\n",
            "Epoch: 5\t Training Accuracy: 87.3660\t Validation Accuracy: 89.1400\t Avg-Loss: 0.3674\n",
            "Epoch: 6\tBatch: 1\tAvg-Loss: 0.4425\n",
            "Epoch: 6\tBatch: 101\tAvg-Loss: 0.3357\n",
            "Epoch: 6\tBatch: 201\tAvg-Loss: 0.3425\n",
            "Epoch: 6\tBatch: 301\tAvg-Loss: 0.3454\n",
            "Epoch: 6\tBatch: 401\tAvg-Loss: 0.3437\n",
            "Epoch: 6\tBatch: 501\tAvg-Loss: 0.3447\n",
            "Epoch: 6\tBatch: 601\tAvg-Loss: 0.3415\n",
            "Epoch: 6\tBatch: 701\tAvg-Loss: 0.3399\n",
            "Epoch: 6\t Training Accuracy: 88.0000\t Validation Accuracy: 88.3600\t Avg-Loss: 0.3417\n",
            "Epoch: 7\tBatch: 1\tAvg-Loss: 0.2915\n",
            "Epoch: 7\tBatch: 101\tAvg-Loss: 0.3037\n",
            "Epoch: 7\tBatch: 201\tAvg-Loss: 0.3094\n",
            "Epoch: 7\tBatch: 301\tAvg-Loss: 0.3081\n",
            "Epoch: 7\tBatch: 401\tAvg-Loss: 0.3091\n",
            "Epoch: 7\tBatch: 501\tAvg-Loss: 0.3096\n",
            "Epoch: 7\tBatch: 601\tAvg-Loss: 0.3125\n",
            "Epoch: 7\tBatch: 701\tAvg-Loss: 0.3135\n",
            "Epoch: 7\t Training Accuracy: 89.0600\t Validation Accuracy: 88.8700\t Avg-Loss: 0.3136\n",
            "Epoch: 8\tBatch: 1\tAvg-Loss: 0.1525\n",
            "Epoch: 8\tBatch: 101\tAvg-Loss: 0.2792\n",
            "Epoch: 8\tBatch: 201\tAvg-Loss: 0.2833\n",
            "Epoch: 8\tBatch: 301\tAvg-Loss: 0.2863\n",
            "Epoch: 8\tBatch: 401\tAvg-Loss: 0.2912\n",
            "Epoch: 8\tBatch: 501\tAvg-Loss: 0.2953\n",
            "Epoch: 8\tBatch: 601\tAvg-Loss: 0.2973\n",
            "Epoch: 8\tBatch: 701\tAvg-Loss: 0.2961\n",
            "Epoch: 8\t Training Accuracy: 89.6420\t Validation Accuracy: 81.1500\t Avg-Loss: 0.2955\n",
            "Epoch: 9\tBatch: 1\tAvg-Loss: 0.3014\n",
            "Epoch: 9\tBatch: 101\tAvg-Loss: 0.3623\n",
            "Epoch: 9\tBatch: 201\tAvg-Loss: 0.3353\n",
            "Epoch: 9\tBatch: 301\tAvg-Loss: 0.3215\n",
            "Epoch: 9\tBatch: 401\tAvg-Loss: 0.3133\n",
            "Epoch: 9\tBatch: 501\tAvg-Loss: 0.3053\n",
            "Epoch: 9\tBatch: 601\tAvg-Loss: 0.3044\n",
            "Epoch: 9\tBatch: 701\tAvg-Loss: 0.3022\n",
            "Epoch: 9\t Training Accuracy: 89.5180\t Validation Accuracy: 89.0800\t Avg-Loss: 0.2998\n",
            "Epoch    10: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Epoch: 10\tBatch: 1\tAvg-Loss: 0.1975\n",
            "Epoch: 10\tBatch: 101\tAvg-Loss: 0.2282\n",
            "Epoch: 10\tBatch: 201\tAvg-Loss: 0.2198\n",
            "Epoch: 10\tBatch: 301\tAvg-Loss: 0.2131\n",
            "Epoch: 10\tBatch: 401\tAvg-Loss: 0.2064\n",
            "Epoch: 10\tBatch: 501\tAvg-Loss: 0.2024\n",
            "Epoch: 10\tBatch: 601\tAvg-Loss: 0.1984\n",
            "Epoch: 10\tBatch: 701\tAvg-Loss: 0.1952\n",
            "Epoch: 10\t Training Accuracy: 93.4200\t Validation Accuracy: 91.6900\t Avg-Loss: 0.1940\n",
            "Epoch: 11\tBatch: 1\tAvg-Loss: 0.0690\n",
            "Epoch: 11\tBatch: 101\tAvg-Loss: 0.1491\n",
            "Epoch: 11\tBatch: 201\tAvg-Loss: 0.1512\n",
            "Epoch: 11\tBatch: 301\tAvg-Loss: 0.1519\n",
            "Epoch: 11\tBatch: 401\tAvg-Loss: 0.1528\n",
            "Epoch: 11\tBatch: 501\tAvg-Loss: 0.1555\n",
            "Epoch: 11\tBatch: 601\tAvg-Loss: 0.1579\n",
            "Epoch: 11\tBatch: 701\tAvg-Loss: 0.1588\n",
            "Epoch: 11\t Training Accuracy: 94.6160\t Validation Accuracy: 92.1000\t Avg-Loss: 0.1579\n",
            "Epoch: 12\tBatch: 1\tAvg-Loss: 0.1108\n",
            "Epoch: 12\tBatch: 101\tAvg-Loss: 0.1433\n",
            "Epoch: 12\tBatch: 201\tAvg-Loss: 0.1458\n",
            "Epoch: 12\tBatch: 301\tAvg-Loss: 0.1474\n",
            "Epoch: 12\tBatch: 401\tAvg-Loss: 0.1485\n",
            "Epoch: 12\tBatch: 501\tAvg-Loss: 0.1474\n",
            "Epoch: 12\tBatch: 601\tAvg-Loss: 0.1447\n",
            "Epoch: 12\tBatch: 701\tAvg-Loss: 0.1459\n",
            "Epoch: 12\t Training Accuracy: 95.0360\t Validation Accuracy: 92.0500\t Avg-Loss: 0.1448\n",
            "Epoch: 13\tBatch: 1\tAvg-Loss: 0.1073\n",
            "Epoch: 13\tBatch: 101\tAvg-Loss: 0.1314\n",
            "Epoch: 13\tBatch: 201\tAvg-Loss: 0.1339\n",
            "Epoch: 13\tBatch: 301\tAvg-Loss: 0.1330\n",
            "Epoch: 13\tBatch: 401\tAvg-Loss: 0.1332\n",
            "Epoch: 13\tBatch: 501\tAvg-Loss: 0.1318\n",
            "Epoch: 13\tBatch: 601\tAvg-Loss: 0.1324\n",
            "Epoch: 13\tBatch: 701\tAvg-Loss: 0.1324\n",
            "Epoch: 13\t Training Accuracy: 95.5360\t Validation Accuracy: 92.2900\t Avg-Loss: 0.1311\n",
            "Epoch: 14\tBatch: 1\tAvg-Loss: 0.1829\n",
            "Epoch: 14\tBatch: 101\tAvg-Loss: 0.1256\n",
            "Epoch: 14\tBatch: 201\tAvg-Loss: 0.1271\n",
            "Epoch: 14\tBatch: 301\tAvg-Loss: 0.1243\n",
            "Epoch: 14\tBatch: 401\tAvg-Loss: 0.1242\n",
            "Epoch: 14\tBatch: 501\tAvg-Loss: 0.1243\n",
            "Epoch: 14\tBatch: 601\tAvg-Loss: 0.1256\n",
            "Epoch: 14\tBatch: 701\tAvg-Loss: 0.1249\n",
            "Epoch: 14\t Training Accuracy: 95.8040\t Validation Accuracy: 92.3100\t Avg-Loss: 0.1243\n",
            "Epoch: 15\tBatch: 1\tAvg-Loss: 0.1029\n",
            "Epoch: 15\tBatch: 101\tAvg-Loss: 0.1090\n",
            "Epoch: 15\tBatch: 201\tAvg-Loss: 0.1169\n",
            "Epoch: 15\tBatch: 301\tAvg-Loss: 0.1182\n",
            "Epoch: 15\tBatch: 401\tAvg-Loss: 0.1175\n",
            "Epoch: 15\tBatch: 501\tAvg-Loss: 0.1156\n",
            "Epoch: 15\tBatch: 601\tAvg-Loss: 0.1138\n",
            "Epoch: 15\tBatch: 701\tAvg-Loss: 0.1113\n",
            "Epoch: 15\t Training Accuracy: 96.2940\t Validation Accuracy: 92.4200\t Avg-Loss: 0.1110\n",
            "Epoch: 16\tBatch: 1\tAvg-Loss: 0.0368\n",
            "Epoch: 16\tBatch: 101\tAvg-Loss: 0.1103\n",
            "Epoch: 16\tBatch: 201\tAvg-Loss: 0.1084\n",
            "Epoch: 16\tBatch: 301\tAvg-Loss: 0.1052\n",
            "Epoch: 16\tBatch: 401\tAvg-Loss: 0.1054\n",
            "Epoch: 16\tBatch: 501\tAvg-Loss: 0.1077\n",
            "Epoch: 16\tBatch: 601\tAvg-Loss: 0.1079\n",
            "Epoch: 16\tBatch: 701\tAvg-Loss: 0.1090\n",
            "Epoch: 16\t Training Accuracy: 96.2880\t Validation Accuracy: 92.5200\t Avg-Loss: 0.1092\n",
            "Epoch: 17\tBatch: 1\tAvg-Loss: 0.1695\n",
            "Epoch: 17\tBatch: 101\tAvg-Loss: 0.1007\n",
            "Epoch: 17\tBatch: 201\tAvg-Loss: 0.1007\n",
            "Epoch: 17\tBatch: 301\tAvg-Loss: 0.1011\n",
            "Epoch: 17\tBatch: 401\tAvg-Loss: 0.1009\n",
            "Epoch: 17\tBatch: 501\tAvg-Loss: 0.1023\n",
            "Epoch: 17\tBatch: 601\tAvg-Loss: 0.1023\n",
            "Epoch: 17\tBatch: 701\tAvg-Loss: 0.1035\n",
            "Epoch: 17\t Training Accuracy: 96.4200\t Validation Accuracy: 92.6200\t Avg-Loss: 0.1035\n",
            "Epoch: 18\tBatch: 1\tAvg-Loss: 0.0716\n",
            "Epoch: 18\tBatch: 101\tAvg-Loss: 0.0968\n",
            "Epoch: 18\tBatch: 201\tAvg-Loss: 0.0930\n",
            "Epoch: 18\tBatch: 301\tAvg-Loss: 0.0972\n",
            "Epoch: 18\tBatch: 401\tAvg-Loss: 0.0958\n",
            "Epoch: 18\tBatch: 501\tAvg-Loss: 0.0940\n",
            "Epoch: 18\tBatch: 601\tAvg-Loss: 0.0954\n",
            "Epoch: 18\tBatch: 701\tAvg-Loss: 0.0955\n",
            "Epoch: 18\t Training Accuracy: 96.6660\t Validation Accuracy: 92.4600\t Avg-Loss: 0.0956\n",
            "Epoch: 19\tBatch: 1\tAvg-Loss: 0.0742\n",
            "Epoch: 19\tBatch: 101\tAvg-Loss: 0.0853\n",
            "Epoch: 19\tBatch: 201\tAvg-Loss: 0.0927\n",
            "Epoch: 19\tBatch: 301\tAvg-Loss: 0.0922\n",
            "Epoch: 19\tBatch: 401\tAvg-Loss: 0.0935\n",
            "Epoch: 19\tBatch: 501\tAvg-Loss: 0.0936\n",
            "Epoch: 19\tBatch: 601\tAvg-Loss: 0.0925\n",
            "Epoch: 19\tBatch: 701\tAvg-Loss: 0.0924\n",
            "Epoch: 19\t Training Accuracy: 96.7980\t Validation Accuracy: 92.3200\t Avg-Loss: 0.0933\n",
            "Epoch: 20\tBatch: 1\tAvg-Loss: 0.0692\n",
            "Epoch: 20\tBatch: 101\tAvg-Loss: 0.0946\n",
            "Epoch: 20\tBatch: 201\tAvg-Loss: 0.0941\n",
            "Epoch: 20\tBatch: 301\tAvg-Loss: 0.0904\n",
            "Epoch: 20\tBatch: 401\tAvg-Loss: 0.0886\n",
            "Epoch: 20\tBatch: 501\tAvg-Loss: 0.0895\n",
            "Epoch: 20\tBatch: 601\tAvg-Loss: 0.0877\n",
            "Epoch: 20\tBatch: 701\tAvg-Loss: 0.0880\n",
            "Epoch: 20\t Training Accuracy: 96.9840\t Validation Accuracy: 92.4800\t Avg-Loss: 0.0880\n",
            "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch: 21\tBatch: 1\tAvg-Loss: 0.0214\n",
            "Epoch: 21\tBatch: 101\tAvg-Loss: 0.0861\n",
            "Epoch: 21\tBatch: 201\tAvg-Loss: 0.0829\n",
            "Epoch: 21\tBatch: 301\tAvg-Loss: 0.0839\n",
            "Epoch: 21\tBatch: 401\tAvg-Loss: 0.0827\n",
            "Epoch: 21\tBatch: 501\tAvg-Loss: 0.0822\n",
            "Epoch: 21\tBatch: 601\tAvg-Loss: 0.0826\n",
            "Epoch: 21\tBatch: 701\tAvg-Loss: 0.0814\n",
            "Epoch: 21\t Training Accuracy: 97.2100\t Validation Accuracy: 92.6000\t Avg-Loss: 0.0821\n",
            "Epoch: 22\tBatch: 1\tAvg-Loss: 0.0405\n",
            "Epoch: 22\tBatch: 101\tAvg-Loss: 0.0854\n",
            "Epoch: 22\tBatch: 201\tAvg-Loss: 0.0811\n",
            "Epoch: 22\tBatch: 301\tAvg-Loss: 0.0827\n",
            "Epoch: 22\tBatch: 401\tAvg-Loss: 0.0819\n",
            "Epoch: 22\tBatch: 501\tAvg-Loss: 0.0819\n",
            "Epoch: 22\tBatch: 601\tAvg-Loss: 0.0807\n",
            "Epoch: 22\tBatch: 701\tAvg-Loss: 0.0804\n",
            "Epoch: 22\t Training Accuracy: 97.3680\t Validation Accuracy: 92.5900\t Avg-Loss: 0.0803\n",
            "Epoch: 23\tBatch: 1\tAvg-Loss: 0.0920\n",
            "Epoch: 23\tBatch: 101\tAvg-Loss: 0.0747\n",
            "Epoch: 23\tBatch: 201\tAvg-Loss: 0.0702\n",
            "Epoch: 23\tBatch: 301\tAvg-Loss: 0.0712\n",
            "Epoch: 23\tBatch: 401\tAvg-Loss: 0.0723\n",
            "Epoch: 23\tBatch: 501\tAvg-Loss: 0.0731\n",
            "Epoch: 23\tBatch: 601\tAvg-Loss: 0.0736\n",
            "Epoch: 23\tBatch: 701\tAvg-Loss: 0.0757\n",
            "Epoch: 23\t Training Accuracy: 97.4220\t Validation Accuracy: 92.5200\t Avg-Loss: 0.0764\n",
            "Epoch: 24\tBatch: 1\tAvg-Loss: 0.0258\n",
            "Epoch: 24\tBatch: 101\tAvg-Loss: 0.0698\n",
            "Epoch: 24\tBatch: 201\tAvg-Loss: 0.0766\n",
            "Epoch: 24\tBatch: 301\tAvg-Loss: 0.0760\n",
            "Epoch: 24\tBatch: 401\tAvg-Loss: 0.0774\n",
            "Epoch: 24\tBatch: 501\tAvg-Loss: 0.0765\n",
            "Epoch: 24\tBatch: 601\tAvg-Loss: 0.0776\n",
            "Epoch: 24\tBatch: 701\tAvg-Loss: 0.0774\n",
            "Epoch: 24\t Training Accuracy: 97.3600\t Validation Accuracy: 92.6700\t Avg-Loss: 0.0776\n",
            "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch: 25\tBatch: 1\tAvg-Loss: 0.1416\n",
            "Epoch: 25\tBatch: 101\tAvg-Loss: 0.0720\n",
            "Epoch: 25\tBatch: 201\tAvg-Loss: 0.0753\n",
            "Epoch: 25\tBatch: 301\tAvg-Loss: 0.0759\n",
            "Epoch: 25\tBatch: 401\tAvg-Loss: 0.0753\n",
            "Epoch: 25\tBatch: 501\tAvg-Loss: 0.0756\n",
            "Epoch: 25\tBatch: 601\tAvg-Loss: 0.0765\n",
            "Epoch: 25\tBatch: 701\tAvg-Loss: 0.0762\n",
            "Epoch: 25\t Training Accuracy: 97.4100\t Validation Accuracy: 92.5700\t Avg-Loss: 0.0761\n",
            "Epoch: 26\tBatch: 1\tAvg-Loss: 0.0775\n",
            "Epoch: 26\tBatch: 101\tAvg-Loss: 0.0784\n",
            "Epoch: 26\tBatch: 201\tAvg-Loss: 0.0803\n",
            "Epoch: 26\tBatch: 301\tAvg-Loss: 0.0790\n",
            "Epoch: 26\tBatch: 401\tAvg-Loss: 0.0768\n",
            "Epoch: 26\tBatch: 501\tAvg-Loss: 0.0763\n",
            "Epoch: 26\tBatch: 601\tAvg-Loss: 0.0753\n",
            "Epoch: 26\tBatch: 701\tAvg-Loss: 0.0764\n",
            "Epoch: 26\t Training Accuracy: 97.3740\t Validation Accuracy: 92.6600\t Avg-Loss: 0.0765\n",
            "Epoch: 27\tBatch: 1\tAvg-Loss: 0.0592\n",
            "Epoch: 27\tBatch: 101\tAvg-Loss: 0.0777\n",
            "Epoch: 27\tBatch: 201\tAvg-Loss: 0.0745\n",
            "Epoch: 27\tBatch: 301\tAvg-Loss: 0.0786\n",
            "Epoch: 27\tBatch: 401\tAvg-Loss: 0.0771\n",
            "Epoch: 27\tBatch: 501\tAvg-Loss: 0.0780\n",
            "Epoch: 27\tBatch: 601\tAvg-Loss: 0.0780\n",
            "Epoch: 27\tBatch: 701\tAvg-Loss: 0.0778\n",
            "Epoch: 27\t Training Accuracy: 97.3860\t Validation Accuracy: 92.5100\t Avg-Loss: 0.0776\n",
            "Epoch: 28\tBatch: 1\tAvg-Loss: 0.0845\n",
            "Epoch: 28\tBatch: 101\tAvg-Loss: 0.0771\n",
            "Epoch: 28\tBatch: 201\tAvg-Loss: 0.0741\n",
            "Epoch: 28\tBatch: 301\tAvg-Loss: 0.0736\n",
            "Epoch: 28\tBatch: 401\tAvg-Loss: 0.0749\n",
            "Epoch: 28\tBatch: 501\tAvg-Loss: 0.0755\n",
            "Epoch: 28\tBatch: 601\tAvg-Loss: 0.0770\n",
            "Epoch: 28\tBatch: 701\tAvg-Loss: 0.0772\n",
            "Epoch: 28\t Training Accuracy: 97.3160\t Validation Accuracy: 92.6400\t Avg-Loss: 0.0767\n",
            "Epoch    29: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch: 29\tBatch: 1\tAvg-Loss: 0.0118\n",
            "Epoch: 29\tBatch: 101\tAvg-Loss: 0.0703\n",
            "Epoch: 29\tBatch: 201\tAvg-Loss: 0.0704\n",
            "Epoch: 29\tBatch: 301\tAvg-Loss: 0.0742\n",
            "Epoch: 29\tBatch: 401\tAvg-Loss: 0.0740\n",
            "Epoch: 29\tBatch: 501\tAvg-Loss: 0.0749\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a8847f7aee12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                   \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                   nesterov)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# update momentum_buffers in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-uR1m2O2Xtm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2FEu9oownPUc",
        "outputId": "0d72d88d-8bce-4d6a-989a-debe554b1b45"
      },
      "source": [
        "# Train!\n",
        "numEpochs = 100\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(trainset)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(testloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(testset)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_d3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 2.3778\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 1.6199\n",
            "Epoch: 0\tBatch: 201\tAvg-Loss: 1.4233\n",
            "Epoch: 0\tBatch: 301\tAvg-Loss: 1.3190\n",
            "Epoch: 0\t Training Accuracy: 55.2460\t Validation Accuracy: 62.2100\t Avg-Loss: 1.2627\n",
            "Epoch: 1\tBatch: 1\tAvg-Loss: 1.0804\n",
            "Epoch: 1\tBatch: 101\tAvg-Loss: 1.1021\n",
            "Epoch: 1\tBatch: 201\tAvg-Loss: 1.1522\n",
            "Epoch: 1\tBatch: 301\tAvg-Loss: 1.1413\n",
            "Epoch: 1\t Training Accuracy: 60.9720\t Validation Accuracy: 59.6500\t Avg-Loss: 1.1151\n",
            "Epoch: 2\tBatch: 1\tAvg-Loss: 1.1830\n",
            "Epoch: 2\tBatch: 101\tAvg-Loss: 1.0432\n",
            "Epoch: 2\tBatch: 201\tAvg-Loss: 1.0140\n",
            "Epoch: 2\tBatch: 301\tAvg-Loss: 0.9927\n",
            "Epoch: 2\t Training Accuracy: 65.8860\t Validation Accuracy: 72.8300\t Avg-Loss: 0.9836\n",
            "Epoch: 3\tBatch: 1\tAvg-Loss: 0.8010\n",
            "Epoch: 3\tBatch: 101\tAvg-Loss: 0.8658\n",
            "Epoch: 3\tBatch: 201\tAvg-Loss: 0.8496\n",
            "Epoch: 3\tBatch: 301\tAvg-Loss: 0.8410\n",
            "Epoch: 3\t Training Accuracy: 70.8140\t Validation Accuracy: 77.0000\t Avg-Loss: 0.8269\n",
            "Epoch: 4\tBatch: 1\tAvg-Loss: 0.6917\n",
            "Epoch: 4\tBatch: 101\tAvg-Loss: 0.8217\n",
            "Epoch: 4\tBatch: 201\tAvg-Loss: 0.8232\n",
            "Epoch: 4\tBatch: 301\tAvg-Loss: 0.8066\n",
            "Epoch: 4\t Training Accuracy: 72.3560\t Validation Accuracy: 77.1000\t Avg-Loss: 0.7903\n",
            "Epoch: 5\tBatch: 1\tAvg-Loss: 0.6388\n",
            "Epoch: 5\tBatch: 101\tAvg-Loss: 0.7180\n",
            "Epoch: 5\tBatch: 201\tAvg-Loss: 0.7262\n",
            "Epoch: 5\tBatch: 301\tAvg-Loss: 0.7247\n",
            "Epoch: 5\t Training Accuracy: 74.4480\t Validation Accuracy: 77.6200\t Avg-Loss: 0.7300\n",
            "Epoch: 6\tBatch: 1\tAvg-Loss: 0.8624\n",
            "Epoch: 6\tBatch: 101\tAvg-Loss: 0.6827\n",
            "Epoch: 6\tBatch: 201\tAvg-Loss: 0.6759\n",
            "Epoch: 6\tBatch: 301\tAvg-Loss: 0.6781\n",
            "Epoch: 6\t Training Accuracy: 76.1260\t Validation Accuracy: 80.1900\t Avg-Loss: 0.6792\n",
            "Epoch: 7\tBatch: 1\tAvg-Loss: 0.6642\n",
            "Epoch: 7\tBatch: 101\tAvg-Loss: 0.6437\n",
            "Epoch: 7\tBatch: 201\tAvg-Loss: 0.6610\n",
            "Epoch: 7\tBatch: 301\tAvg-Loss: 0.6657\n",
            "Epoch: 7\t Training Accuracy: 76.7360\t Validation Accuracy: 79.3300\t Avg-Loss: 0.6636\n",
            "Epoch: 8\tBatch: 1\tAvg-Loss: 0.6415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-a8847f7aee12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        },
        "id": "_GRElIaZkU4u",
        "outputId": "4b66d45d-0731-4e15-9b03-adea0f10e2d0"
      },
      "source": [
        "# Train!\n",
        "numEpochs = 100\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(trainset)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(testloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(testset)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_d3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 2.3666\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 1.6057\n",
            "Epoch: 0\tBatch: 201\tAvg-Loss: 1.3843\n",
            "Epoch: 0\tBatch: 301\tAvg-Loss: 1.2689\n",
            "Epoch: 0\t Training Accuracy: 57.3980\t Validation Accuracy: 71.6800\t Avg-Loss: 1.2038\n",
            "Epoch: 1\tBatch: 1\tAvg-Loss: 1.0228\n",
            "Epoch: 1\tBatch: 101\tAvg-Loss: 0.9367\n",
            "Epoch: 1\tBatch: 201\tAvg-Loss: 0.9324\n",
            "Epoch: 1\tBatch: 301\tAvg-Loss: 0.9158\n",
            "Epoch: 1\t Training Accuracy: 68.6020\t Validation Accuracy: 76.3700\t Avg-Loss: 0.9003\n",
            "Epoch: 2\tBatch: 1\tAvg-Loss: 1.0382\n",
            "Epoch: 2\tBatch: 101\tAvg-Loss: 0.8229\n",
            "Epoch: 2\tBatch: 201\tAvg-Loss: 0.8131\n",
            "Epoch: 2\tBatch: 301\tAvg-Loss: 0.8031\n",
            "Epoch: 2\t Training Accuracy: 72.1500\t Validation Accuracy: 77.3200\t Avg-Loss: 0.7999\n",
            "Epoch: 3\tBatch: 1\tAvg-Loss: 0.6936\n",
            "Epoch: 3\tBatch: 101\tAvg-Loss: 0.7465\n",
            "Epoch: 3\tBatch: 201\tAvg-Loss: 0.7558\n",
            "Epoch: 3\tBatch: 301\tAvg-Loss: 0.7535\n",
            "Epoch: 3\t Training Accuracy: 74.2880\t Validation Accuracy: 79.0800\t Avg-Loss: 0.7465\n",
            "Epoch: 4\tBatch: 1\tAvg-Loss: 0.6616\n",
            "Epoch: 4\tBatch: 101\tAvg-Loss: 0.6991\n",
            "Epoch: 4\tBatch: 201\tAvg-Loss: 0.6835\n",
            "Epoch: 4\tBatch: 301\tAvg-Loss: 0.6814\n",
            "Epoch: 4\t Training Accuracy: 76.5160\t Validation Accuracy: 79.9300\t Avg-Loss: 0.6791\n",
            "Epoch: 5\tBatch: 1\tAvg-Loss: 0.5194\n",
            "Epoch: 5\tBatch: 101\tAvg-Loss: 0.6580\n",
            "Epoch: 5\tBatch: 201\tAvg-Loss: 0.6589\n",
            "Epoch: 5\tBatch: 301\tAvg-Loss: 0.6537\n",
            "Epoch: 5\t Training Accuracy: 77.3080\t Validation Accuracy: 80.8800\t Avg-Loss: 0.6576\n",
            "Epoch: 6\tBatch: 1\tAvg-Loss: 0.4783\n",
            "Epoch: 6\tBatch: 101\tAvg-Loss: 0.6279\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-a8847f7aee12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_LWrOU4ZAhuo",
        "outputId": "ef8071a9-7dd1-4dd6-a9ef-de0a3b441ff3"
      },
      "source": [
        "# Train!\n",
        "numEpochs = 100\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(trainset)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(testloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(testset)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 2.4260\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 1.9716\n",
            "Epoch: 0\tBatch: 201\tAvg-Loss: 1.7083\n",
            "Epoch: 0\tBatch: 301\tAvg-Loss: 1.5676\n",
            "Epoch: 0\t Validation Accuracy: 63.4900\t Avg-Loss: 1.4767\n",
            "Epoch: 1\tBatch: 1\tAvg-Loss: 1.0468\n",
            "Epoch: 1\tBatch: 101\tAvg-Loss: 1.1158\n",
            "Epoch: 1\tBatch: 201\tAvg-Loss: 1.0855\n",
            "Epoch: 1\tBatch: 301\tAvg-Loss: 1.0600\n",
            "Epoch: 1\t Validation Accuracy: 69.8500\t Avg-Loss: 1.0416\n",
            "Epoch: 2\tBatch: 1\tAvg-Loss: 0.8544\n",
            "Epoch: 2\tBatch: 101\tAvg-Loss: 0.9327\n",
            "Epoch: 2\tBatch: 201\tAvg-Loss: 0.9241\n",
            "Epoch: 2\tBatch: 301\tAvg-Loss: 0.9190\n",
            "Epoch: 2\t Validation Accuracy: 73.6200\t Avg-Loss: 0.9112\n",
            "Epoch: 3\tBatch: 1\tAvg-Loss: 0.8604\n",
            "Epoch: 3\tBatch: 101\tAvg-Loss: 0.8475\n",
            "Epoch: 3\tBatch: 201\tAvg-Loss: 0.8353\n",
            "Epoch: 3\tBatch: 301\tAvg-Loss: 0.8315\n",
            "Epoch: 3\t Validation Accuracy: 76.3800\t Avg-Loss: 0.8308\n",
            "Epoch: 4\tBatch: 1\tAvg-Loss: 0.7236\n",
            "Epoch: 4\tBatch: 101\tAvg-Loss: 0.7743\n",
            "Epoch: 4\tBatch: 201\tAvg-Loss: 0.7701\n",
            "Epoch: 4\tBatch: 301\tAvg-Loss: 0.7746\n",
            "Epoch: 4\t Validation Accuracy: 76.5000\t Avg-Loss: 0.7712\n",
            "Epoch: 5\tBatch: 1\tAvg-Loss: 0.7560\n",
            "Epoch: 5\tBatch: 101\tAvg-Loss: 0.7372\n",
            "Epoch: 5\tBatch: 201\tAvg-Loss: 0.7331\n",
            "Epoch: 5\tBatch: 301\tAvg-Loss: 0.7297\n",
            "Epoch: 5\t Validation Accuracy: 78.1300\t Avg-Loss: 0.7280\n",
            "Epoch: 6\tBatch: 1\tAvg-Loss: 0.8417\n",
            "Epoch: 6\tBatch: 101\tAvg-Loss: 0.6702\n",
            "Epoch: 6\tBatch: 201\tAvg-Loss: 0.6792\n",
            "Epoch: 6\tBatch: 301\tAvg-Loss: 0.6789\n",
            "Epoch: 6\t Validation Accuracy: 77.6500\t Avg-Loss: 0.6803\n",
            "Epoch: 7\tBatch: 1\tAvg-Loss: 0.6608\n",
            "Epoch: 7\tBatch: 101\tAvg-Loss: 0.6436\n",
            "Epoch: 7\tBatch: 201\tAvg-Loss: 0.6514\n",
            "Epoch: 7\tBatch: 301\tAvg-Loss: 0.6523\n",
            "Epoch: 7\t Validation Accuracy: 80.2300\t Avg-Loss: 0.6508\n",
            "Epoch: 8\tBatch: 1\tAvg-Loss: 0.5735\n",
            "Epoch: 8\tBatch: 101\tAvg-Loss: 0.5973\n",
            "Epoch: 8\tBatch: 201\tAvg-Loss: 0.6095\n",
            "Epoch: 8\tBatch: 301\tAvg-Loss: 0.6166\n",
            "Epoch: 8\t Validation Accuracy: 80.1600\t Avg-Loss: 0.6140\n",
            "Epoch: 9\tBatch: 1\tAvg-Loss: 0.6370\n",
            "Epoch: 9\tBatch: 101\tAvg-Loss: 0.5880\n",
            "Epoch: 9\tBatch: 201\tAvg-Loss: 0.5860\n",
            "Epoch: 9\tBatch: 301\tAvg-Loss: 0.5887\n",
            "Epoch: 9\t Validation Accuracy: 80.8000\t Avg-Loss: 0.5915\n",
            "Epoch: 10\tBatch: 1\tAvg-Loss: 0.4968\n",
            "Epoch: 10\tBatch: 101\tAvg-Loss: 0.5620\n",
            "Epoch: 10\tBatch: 201\tAvg-Loss: 0.5650\n",
            "Epoch: 10\tBatch: 301\tAvg-Loss: 0.5669\n",
            "Epoch: 10\t Validation Accuracy: 81.0000\t Avg-Loss: 0.5693\n",
            "Epoch: 11\tBatch: 1\tAvg-Loss: 0.4119\n",
            "Epoch: 11\tBatch: 101\tAvg-Loss: 0.5365\n",
            "Epoch: 11\tBatch: 201\tAvg-Loss: 0.5367\n",
            "Epoch: 11\tBatch: 301\tAvg-Loss: 0.5413\n",
            "Epoch: 11\t Validation Accuracy: 81.0600\t Avg-Loss: 0.5457\n",
            "Epoch: 12\tBatch: 1\tAvg-Loss: 0.4841\n",
            "Epoch: 12\tBatch: 101\tAvg-Loss: 0.5192\n",
            "Epoch: 12\tBatch: 201\tAvg-Loss: 0.5191\n",
            "Epoch: 12\tBatch: 301\tAvg-Loss: 0.5235\n",
            "Epoch: 12\t Validation Accuracy: 81.3400\t Avg-Loss: 0.5237\n",
            "Epoch: 13\tBatch: 1\tAvg-Loss: 0.4661\n",
            "Epoch: 13\tBatch: 101\tAvg-Loss: 0.4907\n",
            "Epoch: 13\tBatch: 201\tAvg-Loss: 0.5006\n",
            "Epoch: 13\tBatch: 301\tAvg-Loss: 0.5018\n",
            "Epoch: 13\t Validation Accuracy: 81.4000\t Avg-Loss: 0.5033\n",
            "Epoch: 14\tBatch: 1\tAvg-Loss: 0.4047\n",
            "Epoch: 14\tBatch: 101\tAvg-Loss: 0.4800\n",
            "Epoch: 14\tBatch: 201\tAvg-Loss: 0.4743\n",
            "Epoch: 14\tBatch: 301\tAvg-Loss: 0.4800\n",
            "Epoch: 14\t Validation Accuracy: 81.6500\t Avg-Loss: 0.4849\n",
            "Epoch: 15\tBatch: 1\tAvg-Loss: 0.4366\n",
            "Epoch: 15\tBatch: 101\tAvg-Loss: 0.4613\n",
            "Epoch: 15\tBatch: 201\tAvg-Loss: 0.4662\n",
            "Epoch: 15\tBatch: 301\tAvg-Loss: 0.4659\n",
            "Epoch: 15\t Validation Accuracy: 81.7900\t Avg-Loss: 0.4684\n",
            "Epoch: 16\tBatch: 1\tAvg-Loss: 0.4224\n",
            "Epoch: 16\tBatch: 101\tAvg-Loss: 0.4377\n",
            "Epoch: 16\tBatch: 201\tAvg-Loss: 0.4427\n",
            "Epoch: 16\tBatch: 301\tAvg-Loss: 0.4422\n",
            "Epoch: 16\t Validation Accuracy: 82.3500\t Avg-Loss: 0.4464\n",
            "Epoch: 17\tBatch: 1\tAvg-Loss: 0.3200\n",
            "Epoch: 17\tBatch: 101\tAvg-Loss: 0.4166\n",
            "Epoch: 17\tBatch: 201\tAvg-Loss: 0.4328\n",
            "Epoch: 17\tBatch: 301\tAvg-Loss: 0.4353\n",
            "Epoch: 17\t Validation Accuracy: 82.3600\t Avg-Loss: 0.4381\n",
            "Epoch: 18\tBatch: 1\tAvg-Loss: 0.4009\n",
            "Epoch: 18\tBatch: 101\tAvg-Loss: 0.4076\n",
            "Epoch: 18\tBatch: 201\tAvg-Loss: 0.4131\n",
            "Epoch: 18\tBatch: 301\tAvg-Loss: 0.4136\n",
            "Epoch: 18\t Validation Accuracy: 82.8200\t Avg-Loss: 0.4167\n",
            "Epoch: 19\tBatch: 1\tAvg-Loss: 0.3933\n",
            "Epoch: 19\tBatch: 101\tAvg-Loss: 0.3912\n",
            "Epoch: 19\tBatch: 201\tAvg-Loss: 0.4015\n",
            "Epoch: 19\tBatch: 301\tAvg-Loss: 0.4023\n",
            "Epoch: 19\t Validation Accuracy: 82.0500\t Avg-Loss: 0.4029\n",
            "Epoch: 20\tBatch: 1\tAvg-Loss: 0.2816\n",
            "Epoch: 20\tBatch: 101\tAvg-Loss: 0.3838\n",
            "Epoch: 20\tBatch: 201\tAvg-Loss: 0.3852\n",
            "Epoch: 20\tBatch: 301\tAvg-Loss: 0.3840\n",
            "Epoch: 20\t Validation Accuracy: 82.4600\t Avg-Loss: 0.3901\n",
            "Epoch: 21\tBatch: 1\tAvg-Loss: 0.3275\n",
            "Epoch: 21\tBatch: 101\tAvg-Loss: 0.3629\n",
            "Epoch: 21\tBatch: 201\tAvg-Loss: 0.3695\n",
            "Epoch: 21\tBatch: 301\tAvg-Loss: 0.3725\n",
            "Epoch: 21\t Validation Accuracy: 83.1000\t Avg-Loss: 0.3779\n",
            "Epoch: 22\tBatch: 1\tAvg-Loss: 0.3415\n",
            "Epoch: 22\tBatch: 101\tAvg-Loss: 0.3569\n",
            "Epoch: 22\tBatch: 201\tAvg-Loss: 0.3591\n",
            "Epoch: 22\tBatch: 301\tAvg-Loss: 0.3635\n",
            "Epoch: 22\t Validation Accuracy: 82.3000\t Avg-Loss: 0.3645\n",
            "Epoch: 23\tBatch: 1\tAvg-Loss: 0.2783\n",
            "Epoch: 23\tBatch: 101\tAvg-Loss: 0.3418\n",
            "Epoch: 23\tBatch: 201\tAvg-Loss: 0.3427\n",
            "Epoch: 23\tBatch: 301\tAvg-Loss: 0.3475\n",
            "Epoch: 23\t Validation Accuracy: 82.7000\t Avg-Loss: 0.3534\n",
            "Epoch: 24\tBatch: 1\tAvg-Loss: 0.3325\n",
            "Epoch: 24\tBatch: 101\tAvg-Loss: 0.3214\n",
            "Epoch: 24\tBatch: 201\tAvg-Loss: 0.3297\n",
            "Epoch: 24\tBatch: 301\tAvg-Loss: 0.3378\n",
            "Epoch: 24\t Validation Accuracy: 82.6800\t Avg-Loss: 0.3393\n",
            "Epoch: 25\tBatch: 1\tAvg-Loss: 0.2739\n",
            "Epoch: 25\tBatch: 101\tAvg-Loss: 0.3138\n",
            "Epoch: 25\tBatch: 201\tAvg-Loss: 0.3145\n",
            "Epoch: 25\tBatch: 301\tAvg-Loss: 0.3198\n",
            "Epoch: 25\t Validation Accuracy: 82.9100\t Avg-Loss: 0.3250\n",
            "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch: 26\tBatch: 1\tAvg-Loss: 0.2463\n",
            "Epoch: 26\tBatch: 101\tAvg-Loss: 0.3055\n",
            "Epoch: 26\tBatch: 201\tAvg-Loss: 0.2982\n",
            "Epoch: 26\tBatch: 301\tAvg-Loss: 0.2936\n",
            "Epoch: 26\t Validation Accuracy: 83.2300\t Avg-Loss: 0.2916\n",
            "Epoch: 27\tBatch: 1\tAvg-Loss: 0.2536\n",
            "Epoch: 27\tBatch: 101\tAvg-Loss: 0.2748\n",
            "Epoch: 27\tBatch: 201\tAvg-Loss: 0.2733\n",
            "Epoch: 27\tBatch: 301\tAvg-Loss: 0.2767\n",
            "Epoch: 27\t Validation Accuracy: 83.1200\t Avg-Loss: 0.2757\n",
            "Epoch: 28\tBatch: 1\tAvg-Loss: 0.2832\n",
            "Epoch: 28\tBatch: 101\tAvg-Loss: 0.2705\n",
            "Epoch: 28\tBatch: 201\tAvg-Loss: 0.2636\n",
            "Epoch: 28\tBatch: 301\tAvg-Loss: 0.2636\n",
            "Epoch: 28\t Validation Accuracy: 83.5300\t Avg-Loss: 0.2663\n",
            "Epoch: 29\tBatch: 1\tAvg-Loss: 0.2407\n",
            "Epoch: 29\tBatch: 101\tAvg-Loss: 0.2645\n",
            "Epoch: 29\tBatch: 201\tAvg-Loss: 0.2509\n",
            "Epoch: 29\tBatch: 301\tAvg-Loss: 0.2553\n",
            "Epoch: 29\t Validation Accuracy: 83.5300\t Avg-Loss: 0.2556\n",
            "Epoch: 30\tBatch: 1\tAvg-Loss: 0.2076\n",
            "Epoch: 30\tBatch: 101\tAvg-Loss: 0.2509\n",
            "Epoch: 30\tBatch: 201\tAvg-Loss: 0.2503\n",
            "Epoch: 30\tBatch: 301\tAvg-Loss: 0.2517\n",
            "Epoch: 30\t Validation Accuracy: 83.6300\t Avg-Loss: 0.2521\n",
            "Epoch: 31\tBatch: 1\tAvg-Loss: 0.2065\n",
            "Epoch: 31\tBatch: 101\tAvg-Loss: 0.2518\n",
            "Epoch: 31\tBatch: 201\tAvg-Loss: 0.2533\n",
            "Epoch: 31\tBatch: 301\tAvg-Loss: 0.2511\n",
            "Epoch: 31\t Validation Accuracy: 83.5600\t Avg-Loss: 0.2540\n",
            "Epoch: 32\tBatch: 1\tAvg-Loss: 0.2126\n",
            "Epoch: 32\tBatch: 101\tAvg-Loss: 0.2370\n",
            "Epoch: 32\tBatch: 201\tAvg-Loss: 0.2413\n",
            "Epoch: 32\tBatch: 301\tAvg-Loss: 0.2434\n",
            "Epoch: 32\t Validation Accuracy: 83.6600\t Avg-Loss: 0.2446\n",
            "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch: 33\tBatch: 1\tAvg-Loss: 0.2078\n",
            "Epoch: 33\tBatch: 101\tAvg-Loss: 0.2385\n",
            "Epoch: 33\tBatch: 201\tAvg-Loss: 0.2425\n",
            "Epoch: 33\tBatch: 301\tAvg-Loss: 0.2432\n",
            "Epoch: 33\t Validation Accuracy: 83.7700\t Avg-Loss: 0.2418\n",
            "Epoch: 34\tBatch: 1\tAvg-Loss: 0.2928\n",
            "Epoch: 34\tBatch: 101\tAvg-Loss: 0.2378\n",
            "Epoch: 34\tBatch: 201\tAvg-Loss: 0.2383\n",
            "Epoch: 34\tBatch: 301\tAvg-Loss: 0.2388\n",
            "Epoch: 34\t Validation Accuracy: 83.7400\t Avg-Loss: 0.2385\n",
            "Epoch: 35\tBatch: 1\tAvg-Loss: 0.1955\n",
            "Epoch: 35\tBatch: 101\tAvg-Loss: 0.2388\n",
            "Epoch: 35\tBatch: 201\tAvg-Loss: 0.2411\n",
            "Epoch: 35\tBatch: 301\tAvg-Loss: 0.2400\n",
            "Epoch: 35\t Validation Accuracy: 83.4400\t Avg-Loss: 0.2397\n",
            "Epoch: 36\tBatch: 1\tAvg-Loss: 0.2369\n",
            "Epoch: 36\tBatch: 101\tAvg-Loss: 0.2365\n",
            "Epoch: 36\tBatch: 201\tAvg-Loss: 0.2358\n",
            "Epoch: 36\tBatch: 301\tAvg-Loss: 0.2377\n",
            "Epoch: 36\t Validation Accuracy: 83.8300\t Avg-Loss: 0.2378\n",
            "Epoch: 37\tBatch: 1\tAvg-Loss: 0.2084\n",
            "Epoch: 37\tBatch: 101\tAvg-Loss: 0.2397\n",
            "Epoch: 37\tBatch: 201\tAvg-Loss: 0.2425\n",
            "Epoch: 37\tBatch: 301\tAvg-Loss: 0.2403\n",
            "Epoch: 37\t Validation Accuracy: 83.7500\t Avg-Loss: 0.2409\n",
            "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch: 38\tBatch: 1\tAvg-Loss: 0.2431\n",
            "Epoch: 38\tBatch: 101\tAvg-Loss: 0.2399\n",
            "Epoch: 38\tBatch: 201\tAvg-Loss: 0.2402\n",
            "Epoch: 38\tBatch: 301\tAvg-Loss: 0.2387\n",
            "Epoch: 38\t Validation Accuracy: 83.6200\t Avg-Loss: 0.2405\n",
            "Epoch: 39\tBatch: 1\tAvg-Loss: 0.2798\n",
            "Epoch: 39\tBatch: 101\tAvg-Loss: 0.2413\n",
            "Epoch: 39\tBatch: 201\tAvg-Loss: 0.2383\n",
            "Epoch: 39\tBatch: 301\tAvg-Loss: 0.2391\n",
            "Epoch: 39\t Validation Accuracy: 83.6000\t Avg-Loss: 0.2378\n",
            "Epoch: 40\tBatch: 1\tAvg-Loss: 0.2498\n",
            "Epoch: 40\tBatch: 101\tAvg-Loss: 0.2335\n",
            "Epoch: 40\tBatch: 201\tAvg-Loss: 0.2385\n",
            "Epoch: 40\tBatch: 301\tAvg-Loss: 0.2371\n",
            "Epoch: 40\t Validation Accuracy: 83.8700\t Avg-Loss: 0.2367\n",
            "Epoch: 41\tBatch: 1\tAvg-Loss: 0.2020\n",
            "Epoch: 41\tBatch: 101\tAvg-Loss: 0.2392\n",
            "Epoch: 41\tBatch: 201\tAvg-Loss: 0.2376\n",
            "Epoch: 41\tBatch: 301\tAvg-Loss: 0.2344\n",
            "Epoch: 41\t Validation Accuracy: 83.5700\t Avg-Loss: 0.2363\n",
            "Epoch    42: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch: 42\tBatch: 1\tAvg-Loss: 0.1850\n",
            "Epoch: 42\tBatch: 101\tAvg-Loss: 0.2354\n",
            "Epoch: 42\tBatch: 201\tAvg-Loss: 0.2368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-041f2078694c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                   \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                   nesterov)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# update momentum_buffers in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DhGxleDVinqS",
        "outputId": "bad751ef-d1db-4390-df59-60284969bdde"
      },
      "source": [
        "# Train!\n",
        "numEpochs = 100\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "\n",
        "        correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, train_loss/(batch_num+1)))\n",
        "\n",
        "    train_accuracy = correct / len(trainset)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for batch_num1, (x, y) in enumerate(testloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "    val_accuracy = num_correct / len(testset)\n",
        "    my_acc.append(val_accuracy)\n",
        "    my_loss.append(train_loss/(batch_num+1))\n",
        "    print('Epoch: {}\\t Training Accuracy: {:.4f}\\t Validation Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, train_accuracy*100, val_accuracy * 100, train_loss/(batch_num+1)))\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    #torch.save(network.state_dict(),'/content/drive/MyDrive/DL_CMU/HW2_P2/ResNet_Plateau_d3/Net_'+str(epoch)+'_'+str(val_accuracy)+'_checkpoint.t7')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tBatch: 1\tAvg-Loss: 0.2056\n",
            "Epoch: 0\tBatch: 101\tAvg-Loss: 0.2359\n",
            "Epoch: 0\tBatch: 201\tAvg-Loss: 0.2357\n",
            "Epoch: 0\tBatch: 301\tAvg-Loss: 0.2367\n",
            "Epoch: 0\t Training Accuracy: 91.8280\t Validation Accuracy: 83.7600\t Avg-Loss: 0.2370\n",
            "Epoch: 1\tBatch: 1\tAvg-Loss: 0.2385\n",
            "Epoch: 1\tBatch: 101\tAvg-Loss: 0.2445\n",
            "Epoch: 1\tBatch: 201\tAvg-Loss: 0.2455\n",
            "Epoch: 1\tBatch: 301\tAvg-Loss: 0.2415\n",
            "Epoch: 1\t Training Accuracy: 91.5680\t Validation Accuracy: 83.5100\t Avg-Loss: 0.2396\n",
            "Epoch: 2\tBatch: 1\tAvg-Loss: 0.2614\n",
            "Epoch: 2\tBatch: 101\tAvg-Loss: 0.2340\n",
            "Epoch: 2\tBatch: 201\tAvg-Loss: 0.2400\n",
            "Epoch: 2\tBatch: 301\tAvg-Loss: 0.2385\n",
            "Epoch: 2\t Training Accuracy: 91.8520\t Validation Accuracy: 83.7900\t Avg-Loss: 0.2362\n",
            "Epoch: 3\tBatch: 1\tAvg-Loss: 0.2465\n",
            "Epoch: 3\tBatch: 101\tAvg-Loss: 0.2334\n",
            "Epoch: 3\tBatch: 201\tAvg-Loss: 0.2387\n",
            "Epoch: 3\tBatch: 301\tAvg-Loss: 0.2404\n",
            "Epoch: 3\t Training Accuracy: 91.7700\t Validation Accuracy: 83.5500\t Avg-Loss: 0.2400\n",
            "Epoch    46: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Epoch: 4\tBatch: 1\tAvg-Loss: 0.1531\n",
            "Epoch: 4\tBatch: 101\tAvg-Loss: 0.2320\n",
            "Epoch: 4\tBatch: 201\tAvg-Loss: 0.2384\n",
            "Epoch: 4\tBatch: 301\tAvg-Loss: 0.2402\n",
            "Epoch: 4\t Training Accuracy: 91.6540\t Validation Accuracy: 83.5200\t Avg-Loss: 0.2415\n",
            "Epoch: 5\tBatch: 1\tAvg-Loss: 0.2260\n",
            "Epoch: 5\tBatch: 101\tAvg-Loss: 0.2386\n",
            "Epoch: 5\tBatch: 201\tAvg-Loss: 0.2402\n",
            "Epoch: 5\tBatch: 301\tAvg-Loss: 0.2419\n",
            "Epoch: 5\t Training Accuracy: 91.5260\t Validation Accuracy: 83.4800\t Avg-Loss: 0.2423\n",
            "Epoch: 6\tBatch: 1\tAvg-Loss: 0.2502\n",
            "Epoch: 6\tBatch: 101\tAvg-Loss: 0.2442\n",
            "Epoch: 6\tBatch: 201\tAvg-Loss: 0.2453\n",
            "Epoch: 6\tBatch: 301\tAvg-Loss: 0.2437\n",
            "Epoch: 6\t Training Accuracy: 91.6280\t Validation Accuracy: 83.5900\t Avg-Loss: 0.2432\n",
            "Epoch: 7\tBatch: 1\tAvg-Loss: 0.2156\n",
            "Epoch: 7\tBatch: 101\tAvg-Loss: 0.2335\n",
            "Epoch: 7\tBatch: 201\tAvg-Loss: 0.2360\n",
            "Epoch: 7\tBatch: 301\tAvg-Loss: 0.2400\n",
            "Epoch: 7\t Training Accuracy: 91.7120\t Validation Accuracy: 83.7100\t Avg-Loss: 0.2389\n",
            "Epoch: 8\tBatch: 1\tAvg-Loss: 0.2523\n",
            "Epoch: 8\tBatch: 101\tAvg-Loss: 0.2396\n",
            "Epoch: 8\tBatch: 201\tAvg-Loss: 0.2365\n",
            "Epoch: 8\tBatch: 301\tAvg-Loss: 0.2388\n",
            "Epoch: 8\t Training Accuracy: 91.6880\t Validation Accuracy: 83.5500\t Avg-Loss: 0.2399\n",
            "Epoch: 9\tBatch: 1\tAvg-Loss: 0.2756\n",
            "Epoch: 9\tBatch: 101\tAvg-Loss: 0.2359\n",
            "Epoch: 9\tBatch: 201\tAvg-Loss: 0.2371\n",
            "Epoch: 9\tBatch: 301\tAvg-Loss: 0.2375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-95b14175b751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                   \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                   nesterov)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# update momentum_buffers in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93uRamNXIKEB",
        "outputId": "432c0631-c16f-4b8f-b200-9f249cc7cc7e"
      },
      "source": [
        "# Validate\n",
        "model.eval()\n",
        "num_correct = 0\n",
        "for batch_num1, (x, y) in enumerate(trainloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    outputs = model(x)\n",
        "\n",
        "    num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "val_accuracy = num_correct / len(testset)\n",
        "my_acc.append(val_accuracy)\n",
        "my_loss.append(train_loss/(batch_num+1))\n",
        "print('Epoch: {}\\t Train Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, val_accuracy * 100, train_loss/(batch_num+1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3\t Train Accuracy: 50.0000\t Avg-Loss: 2.2719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR62tH31G9Gf",
        "outputId": "41c89c23-8ace-42ee-8624-d7bfcd2b2386"
      },
      "source": [
        "# Validate\n",
        "model.eval()\n",
        "num_correct = 0\n",
        "for batch_num1, (x, y) in enumerate(trainloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    outputs = model(x)\n",
        "\n",
        "    num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "\n",
        "val_accuracy = num_correct / len(testset)\n",
        "my_acc.append(val_accuracy)\n",
        "my_loss.append(train_loss/(batch_num+1))\n",
        "print('Epoch: {}\\t Train Accuracy: {:.4f}\\t Avg-Loss: {:.4f}'.format(epoch, val_accuracy * 100, train_loss/(batch_num+1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 11\t Train Accuracy: 50.0000\t Avg-Loss: 2.2609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrVSBUQCBM6x"
      },
      "source": [
        "# Train!\n",
        "numEpochs = 40\n",
        "for epoch in range(numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    L_total = 0\n",
        "    val_total_loss = 0.0\n",
        "    for batch_num, (x, y, x_len, y_len) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        out, out_lens = model(x, x_len)\n",
        "\n",
        "        loss = criterion(out, y, out_lens, y_len)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_num % 50 == 0:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, total_loss/(batch_num+1)))   \n",
        "\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    for batch_num1, (x, y, x_len, y_len) in enumerate(test_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        out, out_lens = model(x, x_len)\n",
        "        loss = criterion(out, y, out_lens, y_len)\n",
        "        val_total_loss += loss.item()\n",
        "        L_total += test_metric(out, out_lens, y, y_len)\n",
        "\n",
        "    avg_loss, L_avg_dis, val_avg_loss = total_loss/(batch_num+1), L_total/(batch_num1+1), val_total_loss/(batch_num1+1)\n",
        "    print('Epoch: {}\\tAvg-Loss: {:.4f}\\tVal-Avg-Loss: {:.4f}\\tAvg-Levenstein-dis: {:.4f}'.format(epoch, avg_loss, val_avg_loss, L_avg_dis))\n",
        "    torch.save(model.state_dict(),'/content/drive/MyDrive/DL_CMU/HW3P2/Conv_3/Net_'+str(epoch)+'_'+str(L_avg_dis)+'_checkpoint.t7')\n",
        "    scheduler.step(val_avg_loss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}